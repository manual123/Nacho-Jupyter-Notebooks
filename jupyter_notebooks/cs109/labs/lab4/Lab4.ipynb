{
  "metadata": {
    "name": ""
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "source": [
            "Scikit-Learn, Regression, and PCA, and still more regression."
          ], 
          "cell_type": "heading", 
          "metadata": {}, 
          "level": 2
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 1, 
          "input": [
            "%matplotlib inline\n", 
            "from collections import defaultdict\n", 
            "import json\n", 
            "\n", 
            "import numpy as np\n", 
            "import scipy as sp\n", 
            "import matplotlib.pyplot as plt\n", 
            "import pandas as pd\n", 
            "\n", 
            "from matplotlib import rcParams\n", 
            "import matplotlib.cm as cm\n", 
            "import matplotlib as mpl\n", 
            "\n", 
            "#colorbrewer2 Dark2 qualitative color table\n", 
            "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n", 
            "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n", 
            "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n", 
            "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n", 
            "                (0.4, 0.6509803921568628, 0.11764705882352941),\n", 
            "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n", 
            "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n", 
            "\n", 
            "rcParams['figure.figsize'] = (10, 6)\n", 
            "rcParams['figure.dpi'] = 150\n", 
            "rcParams['axes.color_cycle'] = dark2_colors\n", 
            "rcParams['lines.linewidth'] = 2\n", 
            "rcParams['axes.facecolor'] = 'white'\n", 
            "rcParams['font.size'] = 14\n", 
            "rcParams['patch.edgecolor'] = 'white'\n", 
            "rcParams['patch.facecolor'] = dark2_colors[0]\n", 
            "rcParams['font.family'] = 'StixGeneral'\n", 
            "\n", 
            "\n", 
            "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n", 
            "    \"\"\"\n", 
            "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n", 
            "    \n", 
            "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n", 
            "    \"\"\"\n", 
            "    ax = axes or plt.gca()\n", 
            "    ax.spines['top'].set_visible(top)\n", 
            "    ax.spines['right'].set_visible(right)\n", 
            "    ax.spines['left'].set_visible(left)\n", 
            "    ax.spines['bottom'].set_visible(bottom)\n", 
            "    \n", 
            "    #turn off all ticks\n", 
            "    ax.yaxis.set_ticks_position('none')\n", 
            "    ax.xaxis.set_ticks_position('none')\n", 
            "    \n", 
            "    #now re-enable visibles\n", 
            "    if top:\n", 
            "        ax.xaxis.tick_top()\n", 
            "    if bottom:\n", 
            "        ax.xaxis.tick_bottom()\n", 
            "    if left:\n", 
            "        ax.yaxis.tick_left()\n", 
            "    if right:\n", 
            "        ax.yaxis.tick_right()\n", 
            "        \n", 
            "pd.set_option('display.width', 500)\n", 
            "pd.set_option('display.max_columns', 100)"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 2, 
          "input": [
            "states_abbrev_dict = {\n", 
            "        'AK': 'Alaska',\n", 
            "        'AL': 'Alabama',\n", 
            "        'AR': 'Arkansas',\n", 
            "        'AS': 'American Samoa',\n", 
            "        'AZ': 'Arizona',\n", 
            "        'CA': 'California',\n", 
            "        'CO': 'Colorado',\n", 
            "        'CT': 'Connecticut',\n", 
            "        'DC': 'District of Columbia',\n", 
            "        'DE': 'Delaware',\n", 
            "        'FL': 'Florida',\n", 
            "        'GA': 'Georgia',\n", 
            "        'GU': 'Guam',\n", 
            "        'HI': 'Hawaii',\n", 
            "        'IA': 'Iowa',\n", 
            "        'ID': 'Idaho',\n", 
            "        'IL': 'Illinois',\n", 
            "        'IN': 'Indiana',\n", 
            "        'KS': 'Kansas',\n", 
            "        'KY': 'Kentucky',\n", 
            "        'LA': 'Louisiana',\n", 
            "        'MA': 'Massachusetts',\n", 
            "        'MD': 'Maryland',\n", 
            "        'ME': 'Maine',\n", 
            "        'MI': 'Michigan',\n", 
            "        'MN': 'Minnesota',\n", 
            "        'MO': 'Missouri',\n", 
            "        'MP': 'Northern Mariana Islands',\n", 
            "        'MS': 'Mississippi',\n", 
            "        'MT': 'Montana',\n", 
            "        'NA': 'National',\n", 
            "        'NC': 'North Carolina',\n", 
            "        'ND': 'North Dakota',\n", 
            "        'NE': 'Nebraska',\n", 
            "        'NH': 'New Hampshire',\n", 
            "        'NJ': 'New Jersey',\n", 
            "        'NM': 'New Mexico',\n", 
            "        'NV': 'Nevada',\n", 
            "        'NY': 'New York',\n", 
            "        'OH': 'Ohio',\n", 
            "        'OK': 'Oklahoma',\n", 
            "        'OR': 'Oregon',\n", 
            "        'PA': 'Pennsylvania',\n", 
            "        'PR': 'Puerto Rico',\n", 
            "        'RI': 'Rhode Island',\n", 
            "        'SC': 'South Carolina',\n", 
            "        'SD': 'South Dakota',\n", 
            "        'TN': 'Tennessee',\n", 
            "        'TX': 'Texas',\n", 
            "        'UT': 'Utah',\n", 
            "        'VA': 'Virginia',\n", 
            "        'VI': 'Virgin Islands',\n", 
            "        'VT': 'Vermont',\n", 
            "        'WA': 'Washington',\n", 
            "        'WI': 'Wisconsin',\n", 
            "        'WV': 'West Virginia',\n", 
            "        'WY': 'Wyoming'\n", 
            "}\n", 
            "abbrev_states_dict = {v: k for k, v in states_abbrev_dict.items()}"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "###First, a bit about scikit-learn\n", 
            "\n", 
            "The structure of scikit-learn:\n", 
            "\n", 
            "Some of the following text is taken from the scikit-learn API paper: http://arxiv.org/pdf/1309.0238v1.pdf\n", 
            "\n", 
            ">All objects within scikit-learn share a uniform common basic API consisting of\n", 
            "three complementary interfaces: an estimator interface for building and \ufb01tting\n", 
            "models, a predictor interface for making predictions and a transformer interface\n", 
            "for converting data.\n", 
            "\n", 
            ">The estimator interface is at the core of the library. It de\ufb01nes instantiation\n", 
            "mechanisms of objects and exposes a ***fit*** method for learning a model from\n", 
            "training data. All supervised and unsupervised learning algorithms (e.g., for\n", 
            "classi\ufb01cation, regression or clustering) are o\ufb00ered as objects implementing this\n", 
            "interface. Machine learning tasks like feature extraction, feature selection or\n", 
            "dimensionality reduction are also provided as estimators.\n", 
            "\n"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "An example along these lines:\n", 
            "\n", 
            "    clf = LogisticRegression()\n", 
            "    clf.fit(X_train, y_train)\n", 
            "    \n", 
            "If one changes classifiers, say, to a Random Forest classifier, one would simply replace `LogisticRegression()` in the snippet above by `RandomForestClassifier()`.\n", 
            "\n", 
            ">The **predictor** interface extends the notion of an estimator by adding a predict\n", 
            "method that takes an array `X_test` and produces predictions for `X_test`, based on\n", 
            "the learned parameters of the estimator. In the case of\n", 
            "supervised learning estimators, this method typically returns the predicted labels or values computed by the model. Some unsupervised learning estimators may also implement the predict interface, such as k-means, where the predicted values are the cluster labels.\n", 
            "\n", 
            "    clf.predict(X_test)"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            ">Since it is common to modify or \ufb01lter data before feeding it to a learning algorithm, some estimators in the library implement a **transformer** interface which de\ufb01nes a transform method. It takes as input some new data `X_test` and yields\n", 
            "as output a transformed version of `X_test`. Preprocessing, feature selection,\n", 
            "feature extraction and dimensionality reduction algorithms are all provided as\n", 
            "transformers within the library.\n", 
            "\n", 
            "This is usually done via the `fit_transform` method. For example, to do a PCA:\n", 
            "\n", 
            "    pca = RandomizedPCA(n_components=2)\n", 
            "    train_x = pca.fit_transform(train_x)\n", 
            "    test_x = pca.transform(test_x)\n", 
            "    \n", 
            "The training set here is \"fit\" to find the PC components, and then then transformed. Since `pca.fit()` by itself changes the `pca` object, if we want to transform other data using the same transformation we simply call `transform` subsequently."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "Finally, for now, there is the concept of a meta-estimator, which behaves quite similarly to standard estimators, but allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\n", 
            "\n", 
            "    from sklearn.multiclass import OneVsOneClassifier\n", 
            "    clf=OneVsOneClassifier(LogisticRegression())\n", 
            "\n", 
            ">In scikit-learn, model selection is supported in two distinct meta-estimators,\n", 
            "GridSearchCV and RandomizedSearchCV. They take as input an estimator (basic\n", 
            "or composite), whose hyper-parameters must be optimized, and a set of hyperparameter settings to search through.\n", 
            "\n", 
            "We shall use this latter functionality soon (and you have seen an example of it in HW2).\n", 
            "\n", 
            "From: http://nbviewer.ipython.org/urls/raw.github.com/jakevdp/sklearn_scipy2013/master/rendered_notebooks/02.1_representation_of_data.ipynb\n", 
            "\n", 
            ">Most machine learning algorithms implemented in scikit-learn expect data to be stored in a two-dimensional array or matrix. The arrays can be either numpy arrays, or in some cases scipy.sparse matrices. The size of the array is expected to be [n_samples, n_features]\n", 
            "\n", 
            "To get a grip on how to do machine learning with scikit-learn, it is worth working through the entire set of notebooks at:\n", 
            "https://github.com/jakevdp/sklearn_pycon2013 . These go relatively fast, are fun to read.\n", 
            "The repository at https://github.com/jakevdp/sklearn_scipy2013 has more advanced notebooks. The `rendered_notebooks` folder here is useful with worked-out examples."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "### PART 1: Linear Regression with a touch of PCA.\n", 
            "\n", 
            "We'll see an example of the concepts mentioned above by considering a linear regression problem. Let us load the census data set."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 3, 
          "input": [
            "census_data = pd.read_csv(\"./data/census_demographics.csv\")\n", 
            "census_data.head()"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Clean the data set, and have it indexed by the state abbrev."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 4, 
          "input": [
            "def capitalize(s):\n", 
            "    s = s.title()\n", 
            "    s = s.replace(\"Of\", \"of\")\n", 
            "    return s\n", 
            "census_data[\"State\"] = census_data.state.map(capitalize)\n", 
            "del census_data[\"state\"]\n", 
            "census_data['State']=census_data['State'].replace(abbrev_states_dict)\n", 
            "census_data.set_index(\"State\", inplace=True)\n", 
            "census_data.head()"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We use a SPLOM to visualize some columns of this dataset. In Panda's the SPLOM is a one-liner."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 5, 
          "input": [
            "smaller_frame=census_data[['educ_coll', 'average_income', 'per_vote']]\n", 
            "from pandas.tools.plotting import scatter_matrix\n", 
            "axeslist=scatter_matrix(smaller_frame, alpha=0.8, figsize=(12, 12), diagonal=\"kde\")\n", 
            "for ax in axeslist.flatten():\n", 
            "    ax.grid(False)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Notice how `average_income` seems to have a strong correlation with `educ_coll`. Lets try and regress the former against the latter. One might expect that the average income is higher in states which have \"better\" education systems and send more students to college. First lets confirm our intuition by seeing the co-relations."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 6, 
          "input": [
            "smaller_frame.corr()"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We carry out the regression, first standardizing our variables. This is strictly not necessary, but we are doing it as we wish to play around with PCA. Since `scikit-learn` wants a `n_sample` rows times `n_features` matrix, we need to reshape the `x` variable. We store both an `_vec` variable, which is easier to plot with, as well as the reshaped variable. "
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 7, 
          "input": [
            "from sklearn.linear_model import LinearRegression\n", 
            "X_HD=smaller_frame[['educ_coll', 'average_income']].values\n", 
            "X_HDn=(X_HD - X_HD.mean(axis=0))/X_HD.std(axis=0)\n", 
            "educ_coll_std_vec=X_HDn[:,0]\n", 
            "educ_coll_std=educ_coll_std_vec.reshape(-1,1)\n", 
            "average_income_std_vec=X_HDn[:,1]\n", 
            "average_income_std=average_income_std_vec.reshape(-1,1)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We split the data into a training set and a testing set. By default, 25% of the data is reserved for testing. This is the first of multiple ways that we will see to do this."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 8, 
          "input": [
            "from sklearn.cross_validation import train_test_split\n", 
            "X_train, X_test, y_train, y_test = train_test_split(educ_coll_std, average_income_std_vec)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We use the training set for the fit, and find what our predictions ought to be on both the training and test set."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 9, 
          "input": [
            "clf1 = LinearRegression()\n", 
            "clf1.fit(X_train, y_train)\n", 
            "predicted_train = clf1.predict(X_train)\n", 
            "predicted_test = clf1.predict(X_test)\n", 
            "trains=X_train.reshape(1,-1).flatten()\n", 
            "tests=X_test.reshape(1,-1).flatten()\n", 
            "print clf1.coef_, clf1.intercept_"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We plot the scatter against the fit for both training and test data."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 10, 
          "input": [
            "plt.scatter(educ_coll_std_vec, average_income_std_vec,c='r')\n", 
            "plt.plot(trains, predicted_train, c='g', alpha=0.5)\n", 
            "plt.plot(tests, predicted_test, c='g', alpha=0.2)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We then look at the residuals, again on both sets."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 11, 
          "input": [
            "plt.scatter(predicted_test, predicted_test- y_test, c='g', s=40)\n", 
            "plt.scatter(predicted_train, predicted_train- y_train, c='b', s=40, alpha=0.5)\n", 
            "plt.plot([0.4,2],[0,0])"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We ask scikit-learn to spit out the $R^2$. If you'd like R-style detailed information about your regression, use `statsmodels` instead. "
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 12, 
          "input": [
            "clf1.score(X_train, y_train), clf1.score(X_test, y_test)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "#### Doing a PCA on the data"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "Lets take the standarddized data and do a 2-D PCA on it. Here we do not seek to accomplish a dimensional reduction, but to understand the variance structure of the data."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 13, 
          "input": [
            "from sklearn.decomposition import PCA\n", 
            "pca = PCA(n_components=2)\n", 
            "X = pca.fit_transform(X_HDn)\n", 
            "print pca.explained_variance_ratio_"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 14, 
          "input": [
            "plt.scatter(X[:, 0], X[:, 1])"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Since the first component is so latge, lets only keep it, and then reconstruct the original data from only this component, setting the others to 0."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 15, 
          "input": [
            "pca1 = PCA(n_components=1) # only keep one dimension!\n", 
            "X_E = pca1.fit_transform(X_HDn)\n", 
            "X_reconstructed = pca1.inverse_transform(X_E)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We plot the reconstructed education(x) and income(y) from the first principal component, in blue."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 16, 
          "input": [
            "plt.scatter(X_reconstructed[:,0], X_reconstructed[:,1],c='b', s=35, alpha=0.7)\n", 
            "plt.scatter(educ_coll_std_vec, average_income_std_vec, s=40, c='r', alpha=0.6)\n", 
            "plt.plot(trains, predicted_train, c='g', alpha=0.3)\n", 
            "plt.plot(tests, predicted_test, c='g', alpha=0.3)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We see that the principal component line is steeper as we saw in class. The principle behind the projections is captured below in this plot from stack overflow that HP showed in class.\n", 
            "\n", 
            "![](files/data/pcavsfit.png)"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            ">YOUR TURN NOW"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "Regress the variables in the other direction: ie the `educ_coll` against the `average_income`. This might feel strange to you but remember regression is not about CAUSALITY, only about CORRELATION."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 17, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Create a similar scatterplot and fit line for this regression."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 18, 
          "input": [
            "plt.plot(average_income_std_vec, clf2.predict(average_income_std))\n", 
            "plt.scatter(average_income_std_vec, educ_coll_std_vec)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "### Part 2: Logistic Regression\n", 
            "\n", 
            "From http://www.edwardtufte.com/tufte/ebooks, in \"Visual and Statistical Thinking: \n", 
            "Displays of Evidence for Making Decisions\":\n", 
            "\n", 
            ">On January 28, 1986, the space shuttle Challenger exploded and seven astronauts died because two rubber O-rings leaked. These rings had lost their resiliency because the shuttle was launched on a very cold day. Ambient temperatures were in the low 30s and the O-rings themselves were much colder, less than 20F.\n", 
            "\n", 
            ">One day before the flight, the predicted temperature for the launch was 26F to 29F. Concerned that the rings would not seal at such a cold temperature, the engineers who designed the rocket opposed launching Challenger the next day.\n", 
            "\n", 
            "But they did not make their case persuasively, and were over-ruled by NASA."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 19, 
          "input": [
            "from IPython.display import Image as Im\n", 
            "from IPython.display import display\n", 
            "Im('./data/shuttle.png')"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "The image above shows the leak, where the O-ring failed.\n", 
            "\n", 
            "We have here data on previous failures of the O-rings at various temperatures."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 20, 
          "input": [
            "data=np.array([[float(j) for j in e.strip().split()] for e in open(\"./data/chall.txt\")])\n", 
            "data"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Lets plot this data"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 21, 
          "input": [
            "temps, pfail = data[:,0], data[:,1]\n", 
            "plt.scatter(temps, pfail)\n", 
            "axes=plt.gca()\n", 
            "axes.grid(False)\n", 
            "remove_border(axes)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "1 represents failure. This graph has a classic sigmoid shape, so one might expect logistic regression to work. Furthermore, we do want to find the probability of failure and make predictions from there.\n", 
            "\n", 
            "Logistic regression is carried out in the same way as linear. However, there is the \"pesky\" matter of setting the regularization co-efficient `C`. We havent still covered this in class, so what we do today is just a preview, but its nonetheless important to see.\n", 
            "\n", 
            "The default `C` in sklearn is 1.The meaning of `C` is: the larger the `C`, the lesser the regularization. The smaller the `C` the higher the regularization.\n", 
            "\n", 
            "What does regularization do? Larger regularizations penalize the values of regression coefficients. Smaller ones let the co-efficients range widely. Thus, larger `C` let the regression coefficients range widely. Scikit-learn bakes in two penalties: a l2 penalty which penalizes the sum of the squares of the coefficients, and a l1 penalty which penalizes the sum of the absolute values.\n", 
            "\n", 
            "The reason for doing this is that is there are many co-variates we use for our prediction: we want to make sure we can get away with the simplest model that describes our data, even if that might increase the bias side of the bias-variance tradeoff a bit.\n", 
            "\n", 
            "Remember here, though, that we have just two co-efficents: an intercept, and the outside temperature. So we do not expect to need regularization much. Indeed lets set `C=1000`."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 22, 
          "input": [
            "from sklearn.linear_model import LogisticRegression\n", 
            "reg=1000.\n", 
            "clf4 = LogisticRegression(C=reg)\n", 
            "clf4.fit(temps.reshape(-1,1), pfail)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Lets make predictions, get the associated probabilities, and plot them. We create a new grid of temperatures to evaluate our predictions at. Note that we do not do a test/train split: we have only 23 data points, but need to shut down the launch if there is any doubt. (One wishes...)"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 23, 
          "input": [
            "tempsnew=np.linspace(20., 90., 15)\n", 
            "probs = clf4.predict_proba(tempsnew.reshape(-1,1))[:, 1]\n", 
            "predicts = clf4.predict(tempsnew.reshape(-1,1))"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 24, 
          "input": [
            "plt.scatter(temps, pfail)\n", 
            "axes=plt.gca()\n", 
            "axes.grid(False)\n", 
            "remove_border(axes)\n", 
            "plt.plot(tempsnew, probs, marker='s')\n", 
            "plt.scatter(tempsnew, predicts, marker='s', color=\"green\")"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We use pandas `crosstab` to write a table of prediction vs failure on the \"training\" set. As one might expect, the mislabellings come at the higher temperatures."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 25, 
          "input": [
            "pd.crosstab(pfail, clf4.predict(temps.reshape(-1,1)), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            ">YOUR TURN NOW\n", 
            "\n", 
            "Carry out a Logistic Regression with scikit-learn's default value for `C`. Make a plot similar to the scatterplot above, and carry out the cross-tabulation. What happens?"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 26, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 27, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "#### Logistic Regression with cross-validation\n", 
            "\n", 
            "We now actually go ahead and to the train/test split. Not once but multiple times, on a grid search, for different values of `C`. For each `C`, we:\n", 
            "\n", 
            "1. create `n_folds` folds. Since the data size is 23 here, and we have 5 folds, we roughly split the data into 5 folds of 4-5 values each, randomly. \n", 
            "2. We then train on 4 of these folds, test on the 5th\n", 
            "3. We average the results of all such combinations\n", 
            "4. We move on to the next value of `C`, and find the optimal value that minimizes mean square error.\n", 
            "5. We finally use that value to make the final fit.\n", 
            "\n", 
            "Notice the structure of the `GridSearchCV` estimator in `cv_optimize`."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 28, 
          "input": [
            "from sklearn.linear_model import LogisticRegression\n", 
            "def fit_logistic(X_train, y_train, reg=0.0001, penalty=\"l2\"):\n", 
            "    clf = LogisticRegression(C=reg, penalty=penalty)\n", 
            "    clf.fit(X_train, y_train)\n", 
            "    return clf\n", 
            "\n", 
            "from sklearn.grid_search import GridSearchCV\n", 
            "\n", 
            "def cv_optimize(X_train, y_train, paramslist, penalty=\"l2\", n_folds=10):\n", 
            "    clf = LogisticRegression(penalty=penalty)\n", 
            "    parameters = {\"C\": paramslist}\n", 
            "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n", 
            "    gs.fit(X_train, y_train)\n", 
            "    return gs.best_params_, gs.best_score_\n", 
            "\n", 
            "def cv_and_fit(X_train, y_train, paramslist, penalty=\"l2\", n_folds=5):\n", 
            "    bp, bs = cv_optimize(X_train, y_train, paramslist, penalty=penalty, n_folds=n_folds)\n", 
            "    print \"BP,BS\", bp, bs\n", 
            "    clf = fit_logistic(X_train, y_train, penalty=penalty, reg=bp['C'])\n", 
            "    return clf"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 29, 
          "input": [
            "clf=cv_and_fit(temps.reshape(-1,1), pfail, np.logspace(-4, 3, num=100))\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 30, 
          "input": [
            "pd.crosstab(pfail, clf.predict(temps.reshape(-1,1)), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We plot our results, this time also marking in red the predictions on the \"training\" set."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 31, 
          "input": [
            "plt.scatter(temps, pfail, s=40)\n", 
            "axes=plt.gca()\n", 
            "axes.grid(False)\n", 
            "remove_border(axes)\n", 
            "probs=clf.predict_proba(tempsnew.reshape(-1,1))[:,1]\n", 
            "predicts=clf.predict(tempsnew.reshape(-1,1))\n", 
            "plt.plot(tempsnew, probs, marker='s')\n", 
            "plt.scatter(tempsnew, predicts, marker='D', color=\"green\", s=80, alpha=0.4)\n", 
            "train_probs=clf.predict_proba(temps.reshape(-1,1))[:,1]\n", 
            "plt.scatter(temps, train_probs, marker='s', c='r', alpha=0.5, s=40)\n", 
            "train_predicts=clf.predict(temps.reshape(-1,1))\n", 
            "plt.scatter(temps, train_predicts, marker='s', c='r', alpha=0.2, s=80)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "The failures in prediction are, exactly where you might have expected them to be, as before."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 32, 
          "input": [
            "zip(temps,pfail, clf.predict(temps.reshape(-1,1)))"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We note that the true story was even worse than our data made it out to be! We did not take the severity of the incidents into account. How could we have incorporated this severity into our analysis? (these images are taken from Tufte's booklet)."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 33, 
          "input": [
            "Im('./data/chall-table.png')"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "![](files/data/chall-damage.png)"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "### Part 3: PCA"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 34, 
          "input": [
            "from PIL import Image\n"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "You are an ATM and have to distinguish between cash and check. Its based on a check/drivers license separator at the yhat blog (http://blog.yhathq.com/posts/image-classification-in-Python.html), and a fair bit of code is obtained from there. This problem is a bit more interesting as there is more structure in the images. \n", 
            "\n", 
            "We standardize the size of the images:"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 35, 
          "input": [
            "#setup a standard image size; this will distort some images but will get everything into the same shape\n", 
            "STANDARD_SIZE = (322, 137)\n", 
            "def img_to_matrix(filename, verbose=False):\n", 
            "    \"\"\"\n", 
            "    takes a filename and turns it into a numpy array of RGB pixels\n", 
            "    \"\"\"\n", 
            "    img = Image.open(filename)\n", 
            "    if verbose==True:\n", 
            "        print \"changing size from %s to %s\" % (str(img.size), str(STANDARD_SIZE))\n", 
            "    img = img.resize(STANDARD_SIZE)\n", 
            "    img = list(img.getdata())\n", 
            "    img = map(list, img)\n", 
            "    img = np.array(img)\n", 
            "    return img\n", 
            "\n", 
            "def flatten_image(img):\n", 
            "    \"\"\"\n", 
            "    takes in an (m, n) numpy array and flattens it \n", 
            "    into an array of shape (1, m * n)\n", 
            "    \"\"\"\n", 
            "    s = img.shape[0] * img.shape[1]\n", 
            "    img_wide = img.reshape(1, s)\n", 
            "    return img_wide[0]"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 36, 
          "input": [
            "import os\n", 
            "checks_dir = \"./data/images/images/checks/\"\n", 
            "dollars_dir = \"./data/images/images/dollars/\"\n", 
            "def images(img_dir):\n", 
            "    return [img_dir+f for f in os.listdir(img_dir)]\n", 
            "checks=images(checks_dir)\n", 
            "dollars=images(dollars_dir)\n", 
            "images=checks+dollars\n", 
            "labels = [\"check\" for i in range(len(checks))] + [\"dollar\" for i in range(len(dollars))]\n", 
            "len(labels), len(images)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Lets see what some of these images look like:"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 37, 
          "input": [
            "for i in range(3):\n", 
            "    display(Im(checks[i]))\n", 
            "for i in range(3):\n", 
            "    display(Im(dollars[i]))"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "What features might you use to istinguish the notes from the checks?"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "Here is an example of transforming an image into its R channel."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 38, 
          "input": [
            "i0=images[20]\n", 
            "display(Im(i0))\n", 
            "i0m=img_to_matrix(i0)\n", 
            "print i0m.shape\n", 
            "plt.imshow(i0m[:,1].reshape(137,322))"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We do this for every image, flattening each image into 3 channels of 44114 pixels, for a total of 132342 features per image!"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 39, 
          "input": [
            "data = []\n", 
            "for image in images:\n", 
            "    img = img_to_matrix(image)\n", 
            "    img = flatten_image(img)\n", 
            "    data.append(img)\n", 
            "\n", 
            "data = np.array(data)\n", 
            "data.shape"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 40, 
          "input": [
            "y = np.where(np.array(labels)==\"check\", 1, 0)\n", 
            "y.shape"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We now carryout a 20D PCA, which captures 73% of the variance."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 41, 
          "input": [
            "def do_pca(d,n):\n", 
            "    pca = PCA(n_components=n)\n", 
            "    X = pca.fit_transform(d)\n", 
            "    print pca.explained_variance_ratio_\n", 
            "    return X, pca"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 42, 
          "input": [
            "X20, pca20=do_pca(data,20)"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 43, 
          "input": [
            "np.sum(pca20.explained_variance_ratio_)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Justfor kicks, because we can plot it, we'll do the 2D PCA"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 44, 
          "input": [
            "X2, pca2=do_pca(data,2)"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 45, 
          "input": [
            "df = pd.DataFrame({\"x\": X2[:, 0], \"y\": X2[:, 1], \"label\":np.where(y==1, \"check\", \"dollar\")})\n", 
            "colors = [\"red\", \"yellow\"]\n", 
            "for label, color in zip(df['label'].unique(), colors):\n", 
            "    mask = df['label']==label\n", 
            "    plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n", 
            "plt.legend()"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "**Whoa**! What do we have here. A quick visual shows that 2Dims may be enough to allow for linear separation of checks from dollars, with 42% of the variance accounted for. It would be usefull for face recognition, but all we want to do is to split images into two classes, so its not actually that surpring. \n", 
            "\n", 
            "(For a notebook on face recognition, see:\n", 
            "\n", 
            "http://nbviewer.ipython.org/urls/raw.github.com/jakevdp/sklearn_scipy2013/master/rendered_notebooks/05.1_application_to_face_recognition.ipynb.)"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            "We provide some code to reconstruct, from the principal components, the images corresponding to them."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 46, 
          "input": [
            "def normit(a):\n", 
            "    a=(a - a.min())/(a.max() -a.min())\n", 
            "    a=a*256\n", 
            "    return np.round(a)\n", 
            "\n", 
            "def getRGB(o):\n", 
            "    size=322*137*3\n", 
            "    r=o[0:size:3]\n", 
            "    g=o[1:size:3]\n", 
            "    b=o[2:size:3]\n", 
            "    r=normit(r)\n", 
            "    g=normit(g)\n", 
            "    b=normit(b)\n", 
            "    return r,g,b\n", 
            "\n", 
            "def getNC(pc, j):\n", 
            "    return getRGB(pc.components_[j])\n", 
            "\n", 
            "def getMean(pc):\n", 
            "    m=pc.mean_\n", 
            "    return getRGB(m)\n", 
            "\n", 
            "def display_from_RGB(r, g, b):\n", 
            "    rgbArray = np.zeros((137,322,3), 'uint8')\n", 
            "    rgbArray[..., 0] = r.reshape(137,322)\n", 
            "    rgbArray[..., 1] = g.reshape(137,322)\n", 
            "    rgbArray[..., 2] = b.reshape(137,322)\n", 
            "    img = Image.fromarray(rgbArray)\n", 
            "    plt.imshow(np.asarray(img))\n", 
            "    ax=plt.gca()\n", 
            "    ax.set_xticks([])\n", 
            "    ax.set_yticks([])\n", 
            "    return ax\n", 
            "\n", 
            "def display_component(pc, j):\n", 
            "    r,g,b = getNC(pc,j)\n", 
            "    return display_from_RGB(r,g,b)\n", 
            "    \n"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "And use these to see the first two PC's. It looks like the contrast difference between the presidential head and the surroundings is the main key to doing the classifying. The second PC seems to capture general darkness."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 47, 
          "input": [
            "display_component(pca2,0)"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 48, 
          "input": [
            "display_component(pca2,1)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            ">YOUR TURN NOW\n", 
            "\n", 
            "Do a 5 dimensional PCA, get the variance explanation, and display the components."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 49, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 50, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 51, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 52, 
          "input": [
            "display_from_RGB(*getMean(pca5))"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "#### Using a logistic clasifier\n", 
            "\n", 
            "We provide our usual code adapted from the scikit-learn web site to show classification boundaries."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 53, 
          "input": [
            "from matplotlib.colors import ListedColormap\n", 
            "def points_plot(Xtr, Xte, ytr, yte, clf):\n", 
            "    X=np.concatenate((Xtr, Xte))\n", 
            "    h = .02\n", 
            "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", 
            "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", 
            "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n", 
            "                         np.linspace(y_min, y_max, 50))\n", 
            "\n", 
            "    # just plot the dataset first\n", 
            "    cm = plt.cm.RdBu\n", 
            "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", 
            "    f,ax = plt.subplots()\n", 
            "    # Plot the training points\n", 
            "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr, cmap=cm_bright)\n", 
            "    # and testing points\n", 
            "    ax.scatter(Xte[:, 0], Xte[:, 1], c=yte, cmap=cm_bright, marker=\"s\", s=50, alpha=0.9)\n", 
            "    ax.set_xlim(xx.min(), xx.max())\n", 
            "    ax.set_ylim(yy.min(), yy.max())\n", 
            "    ax.set_xticks(())\n", 
            "    ax.set_yticks(())\n", 
            "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
            "    Z = Z.reshape(xx.shape)\n", 
            "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.4)\n", 
            "    cs2 = ax.contour(xx, yy, Z, cmap=cm, alpha=.4)\n", 
            "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14)\n", 
            "    return ax"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Here we show a way of doing the train-test breakdown ourselves!"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 54, 
          "input": [
            "is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n", 
            "train_x, train_y = data[is_train], y[is_train]\n", 
            "test_x, test_y = data[is_train==False], y[is_train==False]"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We *fit (find PC's) and transform* the training data, and then use the PC's to transform the test data."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 55, 
          "input": [
            "pca = PCA(n_components=2)\n", 
            "train_x = pca.fit_transform(train_x)\n", 
            "test_x = pca.transform(test_x)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "We then do a cross-validated logistic regression. Note the large amount of the regularization. Why do you think this is the case?"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 56, 
          "input": [
            "logreg = cv_and_fit(train_x, train_y, np.logspace(-4, 3, num=100))\n", 
            "pd.crosstab(test_y, logreg.predict(test_x), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 57, 
          "input": [
            "logreg.coef_, logreg.intercept_"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 58, 
          "input": [
            "points_plot(train_x, test_x, train_y, test_y, logreg)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Lets try a \"L1\" penalty instead of \"L2\". this is strictly not a correct thing to do since PCA and L2 regularization are both rotationally invariant. However, lets see what happen to the co-efficients."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 59, 
          "input": [
            "logreg_l1=cv_and_fit(train_x, train_y, np.logspace(-4, 3, num=100), penalty=\"l1\")\n", 
            "pd.crosstab(test_y, logreg_l1.predict(test_x), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 60, 
          "input": [
            "print logreg_l1.coef_, logreg_l1.intercept_"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 61, 
          "input": [
            "points_plot(train_x, test_x, train_y, test_y, logreg_l1)"
          ], 
          "metadata": {}
        }, 
        {
          "source": [
            "Notice \"L1\" regularization supresses the internet and reduces the importance of the second dimension. If one wants to minimize non-zero coefficients, one uses \"L1\" regularization."
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "source": [
            ">YOUR TURN NOW\n", 
            "\n", 
            "Carry out a 5 dimensional PCA and then a logistic rgression in both \"l2\" and \"l1\" modes. Create crosstabs and print co-efficents for both. What do you find?"
          ], 
          "cell_type": "markdown", 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 62, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 63, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 64, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 65, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }, 
        {
          "cell_type": "code", 
          "language": "python", 
          "outputs": [], 
          "collapsed": false, 
          "prompt_number": 66, 
          "input": [
            "#your code here\n"
          ], 
          "metadata": {}
        }
      ], 
      "metadata": {}
    }
  ]
}