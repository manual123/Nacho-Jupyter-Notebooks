{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Cheat  Sheet\n",
    "This notebook provides a summary of the most important and useful pandas commands.\n",
    "\n",
    "# Resources\n",
    "\n",
    "* [Master Data Analysis with Python](https://online.dunderdata.com/courses/master-data-analysis-with-python-volume-1-foundations-of-data-exploration)\n",
    "    * Book by Ted Petrou - 1000 Pages, 400 Exercises\n",
    "* [Complete Master Data Analysis with Python Bundle](https://online.dunderdata.com/bundles/complete-master-data-analysis-with-python-bundle)\n",
    "    * Exercise Python and Master Data Analysis with Python and all videos\n",
    "* [Pandas Cookbook](https://www.amazon.com/Pandas-Cookbook-Ted-Petrou/dp/1784393878)\n",
    "    * Book from Ted Petrou. Detailed recipes that show how to complete specific tasks with real-world data.\n",
    "* [Official Pandas Documentation](http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* [Tagged pandas questions on stackoverflow](https://stackoverflow.com/questions/tagged/pandas)\n",
    "\n",
    "\n",
    "# Making the most of a Jupyter Notebook\n",
    "* Jupyter Notebooks are composed of cells\n",
    "* There are two main **types** of cells \n",
    "    * **Code** cells - understand python code\n",
    "    * **Markdown** cells - understand markdown (a simple plain text formatting language)\n",
    "* Each cell has two separate **modes**\n",
    "    * **Edit** mode - The keys you press will work similarly as they do in a text editor\n",
    "        * Cell is outlined in green\n",
    "        * Flashing cursor inside the cell\n",
    "        * Pencil icon appears in the upper right hand corner of the notebook\n",
    "    * **Command** mode - The keys have special meaning and do not print to the screen\n",
    "        * Cell is outlined in blue\n",
    "        * No flashing cursor\n",
    "        * No pencil icon\n",
    "* Pressing **ESC** puts you in command mode\n",
    "* Pressing **ENTER** puts you in edit mode\n",
    "* In command mode:\n",
    "    * `A` - Add a new cell above\n",
    "    * `B` - Add a new cell below\n",
    "    * `DD` - Delete a cell\n",
    "    * `M` - Change cell type to markdown\n",
    "    * `Y` - Change cell type to code\n",
    "    * `H` - Get all keyboard shortcuts for both modes\n",
    "* Executing code cells:\n",
    "    * **Shift + enter** - execute and move to next cell in command cell OR if last cell create new code cell in edit mode\n",
    "    * **Ctrl + enter** - execute and stay in current cell in command mode\n",
    "    * **Alt/option + enter** - execute and insert new cell in edit mode\n",
    "* Keep your hands on the keyboard. Do not use your mouse to switch from edit to command mode\n",
    "\n",
    "## Help in the Notebook\n",
    "* Press **Shift + Tab + Tab** at the end of a function/method to reveal the docstring as a popup window\n",
    "* Place a `?` at the end of a function/method and execute the cell to reveal the docstring as a window at the bottom of the notebook\n",
    "* Place `??` at the end of a function/method and execute the cell to reveal the source code\n",
    "\n",
    "# Five step process for Doing Data Science in the Notebook\n",
    "[See the blog post for more](https://medium.com/dunder-data/the-five-step-process-for-data-exploration-in-a-jupyter-notebook-92fe818b5a62)\n",
    "1. Write and execute a single line of code to explore your data. Usually you are doing something to a DataFrame or a Series\n",
    "1. Verify that this line of code works by inspecting the output\n",
    "1. Assign the result to a variable\n",
    "1. Within the same cell, in a second line, output the head of the DataFrame or Series\n",
    "1. Continue to the next cell. Do not add more lines of code to the cell\n",
    "\n",
    "\n",
    "# Pandas\n",
    "* Name derived from panel + data\n",
    "* Used for two-dimensional, 'tabular' data analysis \n",
    "* Tabular data has two dimensions, rows and columns. A table.\n",
    "* Relies heavily on numpy to store data and do calculations.\n",
    "* The **DataFrame** is the primary container of data. It is two-dimensional and contains heterogeneous data (data of different types)\n",
    "* The **Series** is another container of data and is one-dimensional. A Series is essentially a single column of data from a DataFrame\n",
    "\n",
    "## Data Types\n",
    "Every column of a Pandas DataFrame has a particular **data type**. The data type is very important and informs us that each value within a particular column is of that data type.\n",
    "\n",
    "### Most Common Data Types\n",
    "There are many possible data types with the following being more common:\n",
    "\n",
    "* Boolean\n",
    "* Integer\n",
    "* Float\n",
    "* Object (can be any Python object but is usually strings)\n",
    "* Datetime\n",
    "* Timedelta\n",
    "* Period\n",
    "* Categorical\n",
    "\n",
    "## Missing Value Representation\n",
    "Pandas uses three representations for missing values: `NaN`, `None`, `NaT`\n",
    "* NaN - not a number, found only in float or object columns\n",
    "* None - Python object **`None`** - found only in object columns\n",
    "* NaT - not a time, found only in Datetime, Timedelta, and Period columns\n",
    "* No missing values for integer or booleans (an unfortunate Pandas limitation)\n",
    "\n",
    "## DataFrame\n",
    "\n",
    "![][2]\n",
    "* 2 dimensional, \"tabular\" data. rows and columns. \n",
    "* Think of a DataFrame as a collection of columns. Pandas thinks columnwise\n",
    "* Three main components - **index**, **columns** and **data (values)**\n",
    "* The index labels the rows and the column names label the columns\n",
    "* Uses an Index object for both the rows and the columns\n",
    "* The row Index is simply called the index. The column index is called the columns.\n",
    "* Access Index with **`df.index`**, the columns with **`df.columns`**, and the values with **`df.values`**\n",
    "* Most common way to create a DataFrame is with **`pd.read_csv('file_name.csv')`**\n",
    "\n",
    "## Series\n",
    "\n",
    "![][1]\n",
    "\n",
    "* One dimensional object\n",
    "* Two components to a Series - the **index** and the **data (values)**\n",
    "* Get the values: **`s.values`** - returns a NumPy array\n",
    "* Get the index: **`s.index`** - return a Pandas Index object - default index is a **`RangeIndex`**\n",
    "* Use **`head/tail`** methods to shorten long output\n",
    "\n",
    "\n",
    "### Common DataFrame Attributes\n",
    "\n",
    "* **`df.index`**\n",
    "* **`df.columns`**\n",
    "* **`df.values`**\n",
    "* **`df.shape`** - tuple of (rows, columns)\n",
    "* **`df.size`** - Total elements in DataFrame - rows x columns\n",
    "* **`df.dtypes`** - Returns each data type of a column as a Series\n",
    "\n",
    "### Common Series attributes\n",
    "\n",
    "* **`s.index`**\n",
    "* **`s.values`**\n",
    "* **`s.size`** - can also find number of Series elements with **`len(s)`**\n",
    "* **`s.dtype`**\n",
    "\n",
    "### Setting a Meaningful index on a DataFrame\n",
    "* It is never necessary to set an index with DataFrame\n",
    "* All data analysis is possible without setting an index\n",
    "* The default index for a DataFrame is the range of integers 0 to n-1. This is called a **`RangeIndex`**\n",
    "* If you do set an index, choose one that is both unique and descriptive. Uniqueness is not enforced\n",
    "* Set the index on read with the `index_col` parameter. **`pd.read_csv('file_name.csv', index_col='title')`**\n",
    "* Set the index after read with `df.set_index('title')`\n",
    "* Turn the index into the first column of a DataFrame with `df.reset_index()`\n",
    "\n",
    "## Selecting subsets of DataFrames\n",
    "\n",
    "* The indexing operator is has specific rules for a DataFrame. It selects either a single as a Series or multiple columns as a DataFrame\n",
    "* Use a single column name (usually a string) to select one column as a Series - **`df['col1']`**\n",
    "* **`df.col1`** also selects a single column, but do NOT use. It doesn't work for columns with spaces\n",
    "* Use a list of column names to select multiple columns as a DataFrame. **`df[['col1', 'col2', 'col3']]`**  Notice the inner list\n",
    "*  **`df[['col']]`** selects a one column DataFrame\n",
    "\n",
    "#### Simultaneous row and column selection with `loc`\n",
    "* The **`loc`** indexer has completely different rules than just the indexing operator.\n",
    "* Use **`loc`** to simultaneously select rows and columns from a DataFrame - `df.loc[rows, cols]` where `rows` and `cols` can be one of the following three:\n",
    "    * A single label\n",
    "    * A list of labels\n",
    "    * A slice of labels\n",
    "* **`loc`** only works with index and column **labels** NOT by integer location\n",
    "* The comma separates the row selection from the column selection\n",
    "\n",
    "#### `loc` examples\n",
    "* **`df.loc[['index1', 'index5'], ['col3', 'col1', 'col7']]`** selects two rows and three columns with lists for each row and column selection\n",
    "* **`df.loc['index1':'index10', ['col3', 'col7']]`** Uses slice notation for the rows and lists for the columns\n",
    "* **`df.loc['index1', 'col5]`** selects a single scalar value\n",
    "* Notice that the column selection is optional and not present for the following examples. In these cases, all the columns are selected\n",
    "* **`df.loc['index1']`** selects a single row as a Series. Notice that the columns are optional and not present here.\n",
    "* **`df.loc[['index1', 'index2']]`** selects multiple rows as a DataFrame\n",
    "* **`df.loc['index1':'index100':5]`** - slice notation selects multiple rows as a DataFrame\n",
    "\n",
    "#### Simultaneous row and column selection with `iloc`\n",
    "* **`.iloc`** works analogously as `loc` but uses integer location instead\n",
    "* ** Integer location** refers to the position of the index or column label along its respective axis. \n",
    "* It follow the pattern `df.iloc[rows, cols]` where `rows` and `cols` can be one of the following three:\n",
    "    * A single integer\n",
    "    * A list of integers\n",
    "    * A slice of integers\n",
    "\n",
    "#### `iloc` examples\n",
    "* **`df.iloc[[2, 6], [0, 4, 2]]`** selects two rows and three columns with lists for each row and column selection\n",
    "* **`df.iloc[5:10, [3, 7]]`** Uses slice notation for the rows and lists for the columns\n",
    "* **`df.iloc[1, 5]`** Uses a single integer for both row and column selection to select a single scalar value\n",
    "* Notice that the column selection is optional and not present for the following examples. In these cases, all the columns are selected\n",
    "* **`df.iloc[2]`** selects a single row as a Series.\n",
    "* **`df.iloc[[2, 5]]`** selects multiple rows as a DataFrame\n",
    "* **`df.iloc[10:200:5]`** - slice notation selects multiple rows as a DataFrame\n",
    "\n",
    "## Selecting subsets of Series \n",
    "\n",
    "* It is possible to use just the indexing operator to make selections from a Series, but I advise against it as it is ambiguous whether you are using it to select with integers or labels. Instead use **`loc`** or **`iloc`**.\n",
    "* Both the **`loc`** and **`iloc`** indexers work analogously as the do with a DataFrame, but only take one selection as there are no columns. \n",
    "* **`s.loc['label1']`** - scalar that selects a single item in Series\n",
    "* **`s.loc[['label1', 'label5']]`** - use a list to select disjoint items\n",
    "* **`s.loc['start':'stop':step]`** - use a slice to select from start to stop inclusive\n",
    "* **`s.iloc[integer1]`** - scalar that selects a single item in Series\n",
    "* **`s.iloc[[integer1, integer2]]`** - use a list to select disjoint items\n",
    "* **`s.iloc[start:stop:step]`** - use a slice to select from start to stop inclusive\n",
    "* **`.ix`** is deprecated. Do NOT use it.\n",
    "* **`s[label]`** works for both integer and label location. It is ambiguous. Avoid if possible.\n",
    "* **`automatic alignment of index`** - be careful when operating with two Series at the same time. They will join on the index first, creating a cartesian product and then complete the calculation.\n",
    "\n",
    "## Boolean Indexing for DataFrames\n",
    "* Boolean indexing (a.k.a Boolean selection) is the process of filtering data based on its actual values\n",
    "* Examples of boolean indexing: Find all the employees with salaray over 100k, find all the flights leaving from Houston with a distance over 500 miles\n",
    "* Boolean indexing works by passing in a Series, or other one-dimensional sequence of booleans to the brackets or **`loc`** indexer. Only values that are True remain.\n",
    "* Must create a Series (or sequence) of booleans first. Sometimes save this to a variable named **`filt`**\n",
    "* Usually the boolean Series is created by using comparison operators with columns in the DataFrame\n",
    "* Example filter: `filt = df['col1'] > 5`\n",
    "* Pass this filter into the brackets to complete the selection: `df[filt]`\n",
    "* Boolean indexing does not with `.iloc`\n",
    "\n",
    "### Multiple Condition Boolean Series\n",
    "* Create multiple conditions by using `&`, `|` operators for the logical **and** and **or**.\n",
    "* Must wrap each condition in parentheses when on the same line\n",
    "* Example filter: `filt = ((df['col1'] > 5) | (df['col2'] < -2)) & (df['col3'] % 2 == 0)`\n",
    "* Invert a Boolean Series with the `~` operator\n",
    "\n",
    "#### Simultaneous boolean selection and column selection\n",
    "* **`df[filt]`** selects all the columns for the rows that are true\n",
    "* Use **`loc`** to simultaneously do boolean selection on the rows while selecting particular columns\n",
    "* Example: **`df.loc[filt, [col1, col2]]`** does boolean selection on the rows while simultaneously selecting columns with a list.\n",
    "\n",
    "#### Methods that create Boolean Series\n",
    "* Use `isin` to do multiple equality comparisons. `filt = df['col1'].isin([val1, val2, val3])`\n",
    "* Use `between` as a shortcut to check whether each value in a Series is between two values. `filt = df['col1'].between(50, 100)`\n",
    "* Use `isna` to check whether each value is missing or not\n",
    "\n",
    "## Boolean Indexing on a Series\n",
    "\n",
    "* Works similarly to boolean indexing for DataFrames\n",
    "* Create the filter and pass it back into the brackets\n",
    "* Boolean Series example: **`filt = ((s > 5) | (s < -2)) & (s % 2 == 0)`**\n",
    "*  **`s[filt]`**  does the boolean selection\n",
    "\n",
    "## Operations on a numeric Series\n",
    "\n",
    "### Series  Arithmetic Operations\n",
    "\n",
    "* **`+, -, *, /, //, ** `** - all operate on every value of the Series\n",
    "* **`s + 5`** adds 5 to every value in the Series\n",
    "* * Operations are **vectorized**\n",
    "* Vectorization means no explicit writing of a for loop\n",
    "\n",
    "### Series Comparison Operations\n",
    "* **`<, >, <=, >=, ==, !=`** - applies condition to each value in Series - returns boolean Series\n",
    "* **`s > 5`** - compares every value in Series to 5 and returns a Series of booleans\n",
    "* Comparison against a missing always returns False\n",
    "* Operations are vectorized\n",
    "\n",
    "## Series Descriptive Statistical Methods\n",
    "\n",
    "### Aggregation methods\n",
    "An aggregation is defined as a function that returns a **single** value.\n",
    "\n",
    "* **`s.sum`**\n",
    "* **`s.min`**\n",
    "* **`s.max`**\n",
    "* **`s.median`**\n",
    "* **`s.mean`**\n",
    "* **`s.count`** - counts non-missing values\n",
    "* **`s.std`**\n",
    "* **`s.var`**\n",
    "* **`s.quantile(q=.5)`** - percentile of distribution\n",
    "* **`s.describe`** - returns most of the above aggregations in one Series\n",
    "\n",
    "### Non-Aggregation methods\n",
    "A non-aggregating method does not return a single value and most often returns a Series the same length as the original.\n",
    "* **`s.abs`** - takes absolute value\n",
    "* **`s.round`** - round to the nearest given decimal\n",
    "* **`s.cummin`** - cumulative minimum\n",
    "* **`s.cummax`** - cumulative maximum\n",
    "* **`s.cumsum`** - cumulative sum\n",
    "* **`s.rank`** - rank values in a variety of different ways\n",
    "* **`s.diff`** - difference between one element and another\n",
    "* **`s.pct_change`** - percent change from one element to another\n",
    "\n",
    "### Missing Value methods\n",
    "* **`s.isna`** - Returns a Series of booleans based on whether each value is missing or not. **`isull`** is an alias\n",
    "* **`s.notna`** - Exact opposite of **`isna`**\n",
    "* **`s.fillna`** - fills missing values in a variety of ways\n",
    "* **`s.dropna`** - Drops the missing values from the Series\n",
    "\n",
    "### More Series methods\n",
    "* **`s.idxmin`**, **`s.idxmax`** - returns the index label of the minimum/maximum value\n",
    "* **`s.unique`** - returns a NumPy array of unique values\n",
    "* **`s.nunique`** - returns the number of unique values\n",
    "* **`s.drop_duplicates`** - returns a Series of the first unique values by default\n",
    "* **`s.sort_values`** - Sorts from least to greatest by default. Use **`ascending=False`** to reverse\n",
    "* **`s.sort_index`** - Sorts index of Series\n",
    "* **`s.sample`** - Randomly samples Series\n",
    "* **`s.replace`** - replaces values in a Series\n",
    "\n",
    "### Series `value_counts` - very important method\n",
    "* **`value_counts`** - returns the sorted frequency of each unique value in a Series. Use **`normalize=True`** to return proportions\n",
    "\n",
    "## Operations on a Series of strings with the `str` accessor\n",
    "* Only available to Series that have string data (object data type)\n",
    "* Many Methods overlap with Python strings\n",
    "* Popular str methods include: `contains, count, extract, split, strip`\n",
    "* Learn regular expressions for more powerful searching and extracting\n",
    "\n",
    "## Operations on a Series of datetimes with the `dt` accessor\n",
    "* Available to Series with datetime data\n",
    "* Mostly simple attributes that retrieve particular information about the data type with attributes such as year, hour, month, weekday, etc...\n",
    "* Popular methods include `round`, `floor`, `ceil` which require offset alias strings (see below)\n",
    "* Also available for timedelta and period columns\n",
    "* timedelta and period columns have different but similar attributes and methods\n",
    "\n",
    "\n",
    "## DataFrame  Arithmetic Operations\n",
    "* **`+, -, *, /, //, `** - all apply single value to all values of DataFrame\n",
    "* **`df + 5`** adds 5 to every value in the Series\n",
    "* Error will occur if operation does not work with every single data type (i.e. adding 5 to a string column)\n",
    "* Operations are vectorized\n",
    "* Use **`select_dtypes`** to select all columns with the given type - Use strings 'int', 'float', 'bool', 'object', 'datetime', 'timedelta', 'category'\n",
    "* Use string 'number' to select both int and float\n",
    "\n",
    "## DataFrame Comparison Operations\n",
    "* **`<, >, <=, >=, ==, !=`** - applies condition to each value in DataFrame - returns DataFrame of all booleans\n",
    "* **`df > 5`** - compares every value in DataFrame to 5.\n",
    "* Operations are vectorized\n",
    "\n",
    "## Basics of DataFrame Methods \n",
    "* DataFrames and Series share about 90% of their methods\n",
    "* DataFrame methods differ than those from Series because they (usually) have an `axis` parameter that controls the **direction of the operation**.\n",
    "* The direction of the operation will either be vertical or horizontal\n",
    "* **`axis`** equal to **`index`** (or **`0`**) will perform the operation vertically (i.e. summing all the values in each column). \n",
    "* **`axis`** equal to **`columns`** (or **`1`**) will perform the action horizontally. \n",
    "* The default is almost always **`axis=0`** - meaning Pandas **thinks column-wise**. By default, operations happen to each column independently.\n",
    "* Use the string names for the axes ('index', and 'columns') instead of the integers 0 and 1 as the string names are more explicit.\n",
    "* **`info`** method returns some metadata from the DataFrame\n",
    "* All the descriptive statistical methods are the same as they are with Series and by default operate down each column.\n",
    "* **`describe`** method has the **`include`** parameter which accepts a string or list of strings for data types that you would like to find summary statistics for\n",
    "* **`rename`** - pass a dictionary of old,new key-value pairs to **`columns`** attribute to rename columns\n",
    "* **`drop`** - pass string or list of strings to **`columns`** parameter to drop columns\n",
    "\n",
    "### Specifics on certain DataFrame methods\n",
    "* **`sort_values`** - must pass a string or list of strings to **`by`** parameter to sort columns. Pass **`ascending`** list of booleans that correspond with **`by`** columns to control direction of sort.\n",
    "* **`drop_duplicates`** and **`dropna`** have a **`subset`** parameter. Pass columns to them to it to limit the functionality to just those columns.\n",
    "\n",
    "## Methods not available to DataFrames\n",
    "* The `str, dt, cat` accessors are only available to Series\n",
    "* `value_counts`, `unique` are only available to Series\n",
    "\n",
    "## Methods not available to Series\n",
    "* `set_index`, `info`, `select_dtypes`, `melt`, `pivot_table`\n",
    "\n",
    "### Reading in Data\n",
    "* Many datasets will be stored in CSV's. Read them in by passing the file location to **`pd.read_csv`**\n",
    "* Set the index column on read with parameter **`index_col`**\n",
    "* Set the **`parse_dates`** parameter to a list of the Datetime column names to convert them on read\n",
    "* Set the index column after read with **`df.set_index('column name')`**\n",
    "* A good choice for index is a column that uniquely identifies each row. Indexes can have duplicates\n",
    "\n",
    "### Automatic alignment of the Index\n",
    "* Pandas has the surprising and implicit feature of automatically aligning the index and columns when operating with two Series/DataFrames at the same time\n",
    "* For example adding two Series - `s1 + s2` or two DataFrames `df1 + df2`\n",
    "* Both the index and columns align first before any operation is done.\n",
    "* They align by forming an outer join. For index values that repeat in each, a Cartesian product is produced\n",
    "* Index and Column labels unique to either Series/DataFrame will remain in the result with a missing value\n",
    "\n",
    "\n",
    "## Grouping\n",
    "Also, known as split apply combine\n",
    "\n",
    "* Split - Splits your data into independent groups\n",
    "* Apply - Apply a function to each of your groups\n",
    "* Combine - Put the results of the apply function back together\n",
    "\n",
    "![][3]\n",
    "\n",
    "The most common type of function to apply to each group is one that **aggregates**. This pattern will always have three components:\n",
    "\n",
    "* **grouping columns** - Each unique combination forms a group\n",
    "* **aggregating columns** - Values of these columns are going to be aggregated into a single number\n",
    "* **aggregating functions** - Determines how the aggregation will happen - **`sum, mean, median`**, etc...\n",
    "\n",
    "A popular syntax for grouping a single column, aggregating a single column, and applying a single aggregation function is:\n",
    "\n",
    "```\n",
    ">>> df.groupby('<grouping column>').agg({'<aggregating column>':'<aggregating function>'})\n",
    "```\n",
    "\n",
    "* Use a list if you would like to use more than one grouping column.\n",
    "* To have additional aggregating columns, add them as new key-value pairs to the dictionary inside of **`agg`**\n",
    "* Use a list as the value in the dictionary to use more than one aggregating function\n",
    "* There are many built-in aggregating functions. Use their string name. Some examples are `min, max, sum, mean, count, size, std` and more.\n",
    "* By default, grouping columns get put in the index. Call the **`reset_index`** method after to make them columns.\n",
    "* Shortcut for getting the size of each group: `df.groupby('<grouping column>').size()`\n",
    "\n",
    "### Advanced Grouping\n",
    "There are several alternate syntaxes for groupby\n",
    "* If all aggregating columns will use the same aggregating functions:\n",
    "```\n",
    ">>> df.groupby('<grouping column>')['<aggregating columns>'].agg([<aggregating columns>])\n",
    "```\n",
    "* If all aggregating columns will use exactly one aggregating function, use it as a method:\n",
    "```\n",
    ">>> df.groupby('<grouping column>')['<aggregating columns>'].sum()\n",
    "```\n",
    "* If you'd like to aggregate all the non-grouping columns with the same function\n",
    "```\n",
    ">>> df.groupby('<grouping column>').sum()\n",
    "```\n",
    "* You can write your own custom aggregation function if it does not exist in Pandas.\n",
    "* Custom aggregation functions have poor performance. Avoid if possible.\n",
    "* Use the `filter` groupby method to filter out groups as a whole. You must write a custom function that returns a single boolean for each group. The original data is filtered and the same number of columns are returned\n",
    "* Use the `transform` method to return either a single value of a sequence the same length as the group. Good if you want to preserve the shape of the original DataFrame\n",
    "\n",
    "### Pivot Table\n",
    "* The **`pivot_table`** method is very similar to a **`groupby`** aggregation, but will pivot one of the grouping columns. We can use the same terminology as we do with groupby\n",
    "```\n",
    ">>> df.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n",
    "```\n",
    "* `pd.crosstab` is useful to normalize counts across more than one variable. Otherwise it is not needed and adds no value over `pivot_table`. \n",
    "* It is nearly identical to `df.pivot_table` but it is a **function** therefore you need to pass it Series and not strings. Set normalize to either 'all', 'index', or 'columns'\n",
    "```\n",
    ">>> pd.crosstab(index=df['grouping column 1'], columns=df['grouping column 2'], normalize='index') \n",
    "```\n",
    "## Time Series\n",
    "\n",
    "### Datetimes (Timestamps)\n",
    "* A date is only year, month, day\n",
    "* A time is only hour, minute, second, part of second\n",
    "* A datetime is a date and a time combined\n",
    "* Pandas has a **`Timestamp`** type but it is just a powerful datetime object. It is a specific moment in time.\n",
    "* In pandas when we talk about Timestamps and Datetimes, we are talking about the same thing. Confusing!\n",
    "* Create Timestamps with the **`to_datetime`** function. It converts a wide variety of strings. Also converts number to units after Unix epoch of Jan 1, 1970.\n",
    "\n",
    "### Timedeltas\n",
    "* Pandas has the **`Timedelta`** data type for amounts of time. e.g. 5 hours 36 minutes and 10 seconds\n",
    "* The **`to_timedelta`** function converts strings and numbers to Timedeltas\n",
    "* Subtracting two Timestamps creates a Timedelta\n",
    "* Both Timestamps and Timedeltas have the same (and more) attributes and methods that the Series **`dt`** accessor has\n",
    "\n",
    "## Offset Alias Strings\n",
    "* Used to round, group, select, and roll by amounts of time\n",
    "\n",
    "![][4]\n",
    "\n",
    "### Time Series Data\n",
    "* A time series is a sequence of data collected over time, often with time increments equally spaced and unique\n",
    "* Set the index to the datetime column when you have time series data to make operations easier. This formally creates a DatetimeIndex\n",
    "\n",
    "#### Selecting with Time Series\n",
    "* Use partial date matching to make selections when you have a DatetimeIndex\n",
    "    * `df['2014-12-5']` selects all data from Dec 5, 2014\n",
    "    * `df['2014-12']` selects all data from Dec 2014\n",
    "    * `df['2014']` selects all data from 2014\n",
    "    * `df['2014-12-5':'2015-2-12]` selects all data from Dec 5, 2014 to Feb 2, 2015 inclusive\n",
    "    * `df['2014-12':'2015]` selects all data from Dec 1, 2014 to Dec 31, 2015\n",
    "    \n",
    "### Sampling Time Series\n",
    "* Use `asfreq` to upsample/downsample with **offset alias** strings\n",
    "* Use the **`resample`** method to group by a period of time. Use the **`on`** parameter to specify the date column if its not in the index.\n",
    "* **`resample`** is very similar to **`groupby`**. Chain the **`agg`** method to aggregate.\n",
    "* Use offset aliases to specify the date grouping increment.\n",
    "```\n",
    "df.resample('M').agg({'col1':'sum'})\n",
    "```\n",
    "* **Anchored offset aliases** can shift the date group range. **W-FRI** anchors the week to end on Friday\n",
    "* A DatetimeIndex makes it easier to select subsets of data. `df['2014:5']` selects all rows in May, 2014.\n",
    "* Calculate moving window statistics with the **`rolling`** method. This also works just like `groupby`\n",
    "* The window can be either a date period (use offset aliases) or a fixed size window (use integer)\n",
    "* Both **`resample`** and **`rolling`** have simpler syntax with Series that have a DatetimeIndex. `s.resample('M').sum()` and `s.rolling('5D').sum()`\n",
    "* Group by date and another column in two ways:\n",
    "    * Independently: `df.groupby('col1').resample('M').agg({'col2':'sum'})`\n",
    "    * Together - must use `pd.Grouper` - think of it as just a dictionary holding data\n",
    "        * `tg = pd.Grouper(freq='M')`\n",
    "        * `df.groupby([tg, 'col1']).agg({'col2':'sum'})`\n",
    "\n",
    "## Tidy Data\n",
    "* Tidy data is a structure of data that makes data analysis easy\n",
    "* Tidy data is defined as:\n",
    "    * Each variable forms a column\n",
    "    * Each observation forms a row\n",
    "    * Each type of observational unit forms a table\n",
    "* All other datasets are \"messy\"\n",
    "* Messy data does not mean it is difficult to read\n",
    "* Tidy data is simply a structure that makes it easy to do most other kinds of data analysis\n",
    "* Identify variables and then transform your DataFrame\n",
    "\n",
    "### Melting\n",
    "* Used to make column names into variable values. Also called 'unpivoting' or 'stacking'\n",
    "* The **`melt`** method has two main parameters\n",
    "    * **`id_vars`** - Columns that you wish to keep vertical\n",
    "    * **`value_vars`** - Columns that you wish to melt. Column names will all go in a single column. Values will go in another column.\n",
    "* Not necessary to explicitly assign `value_vars`. By default, any columns not given to `id_vars` will be melted.\n",
    "\n",
    "### Pivoting\n",
    "* The **`pivot`** method takes three parameters\n",
    "    * **`index`** - column to keep vertical\n",
    "    * **`columns`** - column name whose values will become new column names\n",
    "    * **`values`** - column name whose values will be tiled over the intersection of the index and columns\n",
    "\n",
    "### Common Messy Datasets\n",
    "1. Column names are values, not variable names.\n",
    "1. Multiple variables are stored in one column.\n",
    "1. Variables are stored in both rows and columns.\n",
    "1. Multiple types of observational units are stored in the same table.\n",
    "1. A single observational unit is stored in multiple tables\n",
    "\n",
    "### Data Normalization\n",
    "* A process to reduce data redundancy and increase data integrity\n",
    "* If data is unnecessarily repeated, separate into own table\n",
    "* Add primary key to uniquely identify each row\n",
    "* Dimension tables hold non-event information that is more static, such as a store name and address\n",
    "* Fact tables hold event and transaction information such as a sale within a store\n",
    "\n",
    "## Regular Expressions\n",
    "\n",
    "* Used to find patterns withing text\n",
    "* Miniature programming language\n",
    "* Two different types of characters - literal and metacharacters (a.k.a special characters)\n",
    "* Literal characters represent themselves\n",
    "* Special or metacharacters represent something entirely different\n",
    "* Primarily usage of regex is to either match a particular string or extract a substring\n",
    "* Many Pandas `str` accessor methods accept regular expressions\n",
    "* You will often use **`contains`** and **`extract`** when doing regex work\n",
    "* Use raw Python strings when writing regex. Raw strings have 'r' prepended to them.\n",
    "\n",
    "### Metacharacter Summary\n",
    "* All metacharacters - `. ^ $ * + ? { } [ ] \\ | ( )`\n",
    "\n",
    "#### The dot - `.`\n",
    "* `.` - Matches any character except line breaks\n",
    "\n",
    "#### Anchors - `^, $`\n",
    "* `^` - Anchors next characters to beginning\n",
    "    * `^My` matches strings that begin with 'My'\n",
    "* `$` - Anchors previous characters to end\n",
    "    * `Movie$` matches that strings that end with 'Movie'\n",
    "\n",
    "#### Quantifiers - `*, +, ?, {}`\n",
    "* `*` - Matches 0 or more occurrences of previous character\n",
    "* `+` - Matches 1 or more occurrences of previous character\n",
    "* `?` - Matches 0 or 1 occurrences of previous character\n",
    "* `{m}` - Matches exactly m of the previous character, \n",
    "* `{m,}` - Matches m or more of the previous character \n",
    "* `{,n}` - Matches up to n of the previous character \n",
    "* `{m,n}` - Matches between m and n repeats of the previous character\n",
    "\n",
    "#### Character Sets\n",
    "* `[]` - A character set to match one out of many characters. `[aeiou]` matches a single vowel\n",
    "* `[a-z]`, `[A-Z]`, `[0-9]` - Character sets for lowercase, uppercase, and digits\n",
    "* `[^abc]` - Use caret at beginning of bracket to match anything but these characters\n",
    "* `\\` - backslash changes meaning of next character\n",
    "* `\\s` - whitespace - single space, tab, new-line\n",
    "* `\\S` - non-whitespace\n",
    "* `\\w` - word character - lower/uppercase, digits, and underscore\n",
    "* `\\W` - non-word-character\n",
    "* `\\d` - digits\n",
    "* `\\D` - non-digits\n",
    "* `\\b` - word boundary - matches empty string between words, that is between `\\w` and `\\W`\n",
    "* `\\B` - non-word boundary\n",
    "* `\\.` - Escapes all special characters such as literal dot here. `\\*` matches the literal asterisk\n",
    "\n",
    "\n",
    "#### Or Clause\n",
    "* `|` - The pipe metacharacter represents the **or** clause. Matches when either left or right set of characters match. `cat|dog` matches either 'cat' or 'dog'\n",
    "\n",
    "#### Grouping\n",
    "* `()` - Groups together parts of regex like mathematical parentheses to achieve different operator precedence\n",
    "* `()` - Also represents capture groups for extracting text.\n",
    "* `(?:)` - A non-capturing group. The group won't be captured or be able to be referenced. Use this when you just want to group but don't want to capture. Example: `(?:His|Her) shoe` matches both 'His shoe' and 'Her shoe'\n",
    "* `(?=)` - Positive lookahead - Example: `Batman(?=.*Robin)(?=.*Joker)` Matches string that have Batman followed by both Robin and Joker somewhere after them. No characters are consumed.\n",
    "* `(?!)` - Negative lookahead - Example: `Beauty and the (?!.*Beast)` Matches strings that have 'Beauty and the' followed by anything other than 'Beast'\n",
    "* `(?<=)` - Positive look-behind - Example: `(?<=bird)house` Matches strings the word 'house', but only those that are immediately preceded by 'bird'\n",
    "* `(?!)` - Negative look-behind - Example: `(?<!bird)house` Matches strings the word 'house' except those that are immediately preceded by 'bird'\n",
    "\n",
    "## Joining\n",
    "* Use the function `pd.concat` to stack multiple DataFrames on top of each other or side-by-side. Pandas automatically aligns on the index.\n",
    "* Use sqlalchemy to make a connection to a SQL database\n",
    "* The `merge` method does sql-style joins.\n",
    "\n",
    "## Visualization\n",
    "\n",
    "* For any numerical Series: `s.plot()` will produce a line plot by default with index as x-axis and values as y-axis.\n",
    "* Use parameter `kind=` to change plot type to `hist, bar, barh, kde, pie, box, density, area`\n",
    "* `linestyle` (ls) - Pass a string of one of the following ['--', '-.', '-', ':']\n",
    "* `color` (c) - Can take a string of a named color, a string of the hexadecimal characters or a rgb tuple with each number between 0 and 1. [Check out this really good stackoverflow post to see the colors](http://stackoverflow.com/questions/22408237/named-colors-in-matplotlib)\n",
    "* `linewidth` (lw) - controls thickness of line. Default is 1\n",
    "* `alpha` - controls opacity with a number between 0 and 1\n",
    "* `figsize` - a tuple used to control the size of the plot. (width, height) \n",
    "* `legend` - boolean to control legend\n",
    "* Change plotting template with `plt.style.use('ggplot')`. See all templates with `plt.style.available`\n",
    "\n",
    "[1]: 01.%20Selecting%20Subsets%20of%20Data/images/series_components.png\n",
    "[2]: 01.%20Selecting%20Subsets%20of%20Data/images/df_components.png\n",
    "[3]: 03.%20Grouping/images/split-apply-combine.png\n",
    "[4]: 04.%20Time%20Series/images/offsetalias.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
