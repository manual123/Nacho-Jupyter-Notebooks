{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Messy Datasets\n",
    "\n",
    "The previous notebooks focused on one particular type of messy dataset. A dataset where the column names are actually variable values and not variable names. This was illustrated with the dataset on arrival delay. The `melt` method will quickly tidy these basic datasets. But, often is the case that datasets take more manipulation to make them tidy. This notebook covers several more common messy datasets.\n",
    "\n",
    "## Most common messy data problems\n",
    "\n",
    "1. Column names are variable values, not variable names.\n",
    "1. Multiple variables are stored in one column.\n",
    "1. Variables are stored in both rows and columns.\n",
    "1. Multiple types of observational units are stored in the same table.\n",
    "1. A single observational unit is stored in multiple tables\n",
    "\n",
    "The first type of messy data was covered in the previous notebook. This notebook will cover the next three examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple variables are stored in one column\n",
    "\n",
    "A tidy data set requires that values of a single variable are stored in one column.\n",
    "\n",
    "### Column names appear as values in a column\n",
    "\n",
    "Take a look at the dataset below. Notice how the `Value` column has both numeric and string data types and the `Info` column contains variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data={'State': ['Texas', 'Arizona', 'Florida'] * 3,\n",
    "                        'Info': ['Age'] * 3 + ['Salary'] * 3 + ['Hair Color'] * 3, \n",
    "                        'Value': [10, 15, 20, 3, 4, 5, 'Brown', 'Pink','Red']},\n",
    "                 columns=['State', 'Info', 'Value'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fix\n",
    "This dataset has three variables in a single column. You can think of it as 'overly melted'. Pivoting it with the **`pivot`** method will make it tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot(index='State', columns='Info', values='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy = df.pivot(index='State', columns='Info', values='Value').reset_index()\n",
    "df_tidy = df_tidy.rename_axis(None, axis='columns')\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data types\n",
    "Whenever we have a mix of variables in a single column, you might also have a mix of data types. It's important to check the data types after reshaping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data types\n",
    "Both `Age` and `Salary` should be integers but instead are objects. We need to change their data types. We have seen this previously with the function `pd.to_numeric`, but you can also see use the `astype` method. They both do nearly the same thing, except `pd.numeric` gives you more options (which were needed in a previous notebook). We will use each here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy['Age'] = df_tidy['Age'].astype('int')\n",
    "df_tidy['Salary'] = pd.to_numeric(df_tidy['Salary'])\n",
    "df_tidy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two or more values are stored in the same cell\n",
    "Two or more values of the same variable or different variable can be stored in the same cell in a DataFrame. You will need to extract the desired quantities which might necessitate regular expressions. Let's take a look at a dataset with multiple variables stored in a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = pd.DataFrame({'City':['Houston', 'Dallas', 'Austin'], \n",
    "                   'Geolocation':['(29.7604° N, 95.3698° W)', \n",
    "                                  '32.7767° N, 96.7970° W', \n",
    "                                  '30.2672° N, 97.7431° W']})\n",
    "geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Variables\n",
    "The first step in tidying data is identifying the variables. The `Geolocation` column has quite a lot of information packed into it. We will parse it into 4 separate variables.\n",
    "\n",
    "* latitude \n",
    "* latitude direction\n",
    "* longitude\n",
    "* longitude direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information with regular expressions using the `str` accessor\n",
    "\n",
    "The `extract` string method takes a regular expression with **capture groups** and returns each captured group as a new column.\n",
    "\n",
    "Our regular expression has 4 capture groups. One for each variable. \n",
    "\n",
    "```\n",
    "([0-9.]+).*?([NS]).*?([0-9.]+).*?([EW])\n",
    "```\n",
    "\n",
    "### My explanation\n",
    "\n",
    "* `([0-9.]+)` - This is a capture group that matches one or more of the digits 0-9 and the literal `.`\n",
    "* `.*?([NS])` - Matches any number of characters in a non-greedy fashion before capturing N or S.\n",
    "* The pattern then repeats to capture the longitude and direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_extract = geo['Geolocation'].str.extract(r'([0-9.]+).*?([NS]).*?([0-9.]+).*?([EW])')\n",
    "geo_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names\n",
    "\n",
    "pandas defaults the column names of the resulting DataFrame to integers. We would like these new columns appended to our original DataFrame.\n",
    "\n",
    "### Creating multiple new column names\n",
    "\n",
    "It is possible to create several new columns in our original DataFrame by simply assigning the above resulting DataFrame to a selection of new column names as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo[['latitude', 'latitude direction', 'longitude', 'longitude direction']] = geo_extract\n",
    "geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the original column\n",
    "We can remove the **`Geolocation`** column as we have finished processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = geo.drop(columns='Geolocation')\n",
    "geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and change Data Types\n",
    "Both latitude and longitude are clearly supposed to be numeric (floats) but since they were extracted from a string, remain as strings. Let's change them to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo['latitude'] = geo['latitude'].astype('float')\n",
    "geo['longitude'] = geo['longitude'].astype('float')\n",
    "geo.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have tidied this dataset which had multiple values stored in a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables are stored in both rows and columns\n",
    "A more difficult situation occurs when variables are stored down a column and across the column names. Pivoting and melting may have to be used together to make it tidy. Let's take a look at the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp = pd.read_csv('../data/tidy/temp_flow_pressure.csv')\n",
    "tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Variables\n",
    "Identifying variables in this dataset is not as straightforward as it is in others. There are variable values stored across multiple rows and variable values stored as column names.\n",
    "\n",
    "We can use the following as variables:\n",
    "\n",
    "* Group\n",
    "* Pressure\n",
    "* Temperature\n",
    "* Flow\n",
    "* Years\n",
    "\n",
    "The years are column names and the pressure, temperature, and flow are values in the property column. The Group column is the only one in the correct place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt the years\n",
    "Tidying this particular dataset must happen in multiple stages. We won't be able to tidy each variable at the same time. We will begin by melting the year column names into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_melt = tfp.melt(id_vars=['Group', 'Property'], \n",
    "                    value_vars=['2012', '2013', '2014', '2015', '2016'],\n",
    "                    var_name='Year')\n",
    "tfp_melt.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to pivot Property\n",
    "We now need to pivot the **Property** column so that the values become column names, and keep Group and Year as columns. The values will come from the `value` column.\n",
    "\n",
    "### Problem! `pivot` only works with a single column as the index\n",
    "\n",
    "If we try and pivot by passing a list of values to the `index` parameter, we get an error. Pandas actually thinks we are using the `['Group', 'Year']` not as column names but as values to pivot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_melt.pivot(index=['Group', 'Year'], columns='Property', values='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Must use `pivot_table`\n",
    "\n",
    "The `pivot_table` method does allow us to keep multiple columns in the index. Multiple aggregation functions produce the same result as there is only one value to aggregate per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_tidy = tfp_melt.pivot_table(index=['Group', 'Year'], columns='Property', \n",
    "                                values='value', aggfunc='max')\n",
    "tfp_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that there is one value per intersection\n",
    "Let's verify that there is one value per intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_melt.pivot_table(index=['Group', 'Year'], columns='Property', \n",
    "                     values='value', aggfunc='size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_tidy = tfp_tidy.reset_index()\n",
    "tfp_tidy = tfp_tidy.rename_axis(None, axis='columns')\n",
    "tfp_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_tidy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Year to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_tidy['Year'] = tfp_tidy['Year'].astype('int')\n",
    "tfp_tidy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to produce tidy data\n",
    "There won't be an exact set of procedures that will always result in a tidy dataset. This guideline may help you turn messy data into tidy data.\n",
    "\n",
    "1. Identify each variable\n",
    "1. Look for variable values masquerading as column names\n",
    "1. Look for column names masquerading as variable values\n",
    "1. Examine the 5 types of common messy data sets to see which one your dataset most closely resembles\n",
    "1. You will likely need to use `melt`, `pivot`, and `pivot_table`\n",
    "1. You might need to separate different variables into their own DataFrame to make for easier tidying\n",
    "1. Parse string data with the `str` accessor with the help of regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "<span  style=\"color:green; font-size:16px\">Make the `tidy/country_hour_price.csv` dataset tidy by putting all the hour columns into a single column.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "<span  style=\"color:green; font-size:16px\">If the resulting DataFrame from Exercise 1 has the strings 'HOUR1' and 'HOUR2' as values in the hour column, then extract just the numerical part of the strings and reassign the result to the hour column.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "<span  style=\"color:green; font-size:16px\">Tidy the `tidy/flights_status.csv` dataset.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "<span  style=\"color:green; font-size:16px\">Tidy the `tidy/metrics.csv` dataset.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
