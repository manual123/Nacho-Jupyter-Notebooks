{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classifying Warranty Claims with Symptom Class Names Using Machine Learning</h2><br><br>\n",
    "by Daniel J. Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Honda Market Quality (MQ), we are responsible for identifying vehicle quality and safety problems.  The primary source of market or field information is warranty claims data.  This data represents the voice of our customers.  The data contains several attributes such as part number, part cost, days to failure, miles to failure, customer's complaint, etc.  Over the years, Honda has accumulated several millions of warranty claims.  In order to efficiently identify market problems, methods have to be employed to \"classify\" or group like or similar claims together so that analysis can be made to efficiently find trends, track problems, and ensure problems are fixed or counter-measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, warranty claims data is classified using several, hard-coded algorithms, requiring extensive maintenance.  The jobs that our IT runs to complete the classification take several hours overnight.  Due to recent advancements and accessibility of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) (ML) methodologies, I believe MQ and our Honda IT professionals should investigate how ML can be used to improve the warranty claims classification process and extend its usage to other applicable areas of business.  Furthermore, I strongly believe MQ need to develop in-house capability and knowledge in machine learning.  Unfortunately at MQ, we do not have associates that are knowledgeable in ML or have limited knowledge, this includes me.  But we can change that and hopefully we can discover benefits of applying machine learning to enhance MQ's business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is an attempt at a proof-of-concept of how machine learning can be used to classify warranty claims without hard-coded algorithms and is not meant to be representative of a \"production\" application.  The programming language used to employ the machine learning algorithm is Python using the [scikit-learn](http://scikit-learn.org/stable/) machine learning library.  This document is a [Jupyter](http://jupyter.org/) web notebook which allows me to document my process so that perhaps others can duplicate or understand my process as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to confidentiality, raw data will not be available.  Instead source of sample data was from an Excel file which I then \"copy/pasted\" from my computer's \"clipboard\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation: Data Cleansing and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source data had dollar sign and comma in the part cost amounts.  So we need to remove them and ensure the part cost is a numeric (float) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['PART_COST_USD'] = df['PART_COST_USD'].str.replace('$','').str.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['PART_COST_USD'] = df['PART_COST_USD'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm data type of the source data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAIL_SHORT_PARTNO        object\n",
       "PART_COST_USD           float64\n",
       "DAYS_TO_FAIL_MINZERO      int64\n",
       "MILES_TO_FAIL             int64\n",
       "TEXT_CLUSTER_FAMILY      object\n",
       "SYMP_CLASS_NM            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the first 5 rows of the training data set we will use.  The features columns that we will use are the first 5 columns and the target or label data will be the last column (\"SYMPTOM_CLASS_NM\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we want to label or classify future claims based on part #, part cost, DTF, MTF, and symptom text cluster family to their appropriate symptom class name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's view our sample data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAIL_SHORT_PARTNO</th>\n",
       "      <th>PART_COST_USD</th>\n",
       "      <th>DAYS_TO_FAIL_MINZERO</th>\n",
       "      <th>MILES_TO_FAIL</th>\n",
       "      <th>TEXT_CLUSTER_FAMILY</th>\n",
       "      <th>SYMP_CLASS_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00030</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NOISE/VIBRATION</td>\n",
       "      <td>BRAKE JUDDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01469</td>\n",
       "      <td>458.91</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>FUNCTION ISSUE</td>\n",
       "      <td>BRAKE PEDAL SOFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01469</td>\n",
       "      <td>455.32</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>FUNCTION ISSUE</td>\n",
       "      <td>MASTER CYLINDER/BOOSTER/POWER ASSY/FUNCTION ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01611</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>COSMETIC ISSUE</td>\n",
       "      <td>SIDE PANEL / FENDER/FENDER (FRONT)/COSMETIC ISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04110</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0</td>\n",
       "      <td>20887</td>\n",
       "      <td>FUNCTION ISSUE</td>\n",
       "      <td>BULBS (INTERIOR)/04110/FUNCTION ISSUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FAIL_SHORT_PARTNO  PART_COST_USD  DAYS_TO_FAIL_MINZERO  MILES_TO_FAIL  \\\n",
       "0             00030           0.00                     0              2   \n",
       "1             01469         458.91                     0             11   \n",
       "2             01469         455.32                     0             10   \n",
       "3             01611           0.00                     0              5   \n",
       "4             04110           2.62                     0          20887   \n",
       "\n",
       "  TEXT_CLUSTER_FAMILY                                      SYMP_CLASS_NM  \n",
       "0     NOISE/VIBRATION                                       BRAKE JUDDER  \n",
       "1      FUNCTION ISSUE                                   BRAKE PEDAL SOFT  \n",
       "2      FUNCTION ISSUE  MASTER CYLINDER/BOOSTER/POWER ASSY/FUNCTION ISSUE  \n",
       "3      COSMETIC ISSUE  SIDE PANEL / FENDER/FENDER (FRONT)/COSMETIC ISSUE  \n",
       "4      FUNCTION ISSUE              BULBS (INTERIOR)/04110/FUNCTION ISSUE  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using machine learning libraries, most require that the input data do not contain text/string data.  Since part #s can contain string values, I had to resort to creating a Python dictionary that maps a part5 (first 5 digits of a part number) to a unique number.  I had to do the same for symptom text cluster family and symptom class name since they are also text/string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Python list containing unique part #s and their corresponding numeric value\n",
    "part5_unique = sorted(df.FAIL_SHORT_PARTNO.unique().tolist())\n",
    "part5_index = [n for n in range(len(part5_unique))]\n",
    "\n",
    "# Now create Python dictionaries that map part5 to integer value and vice versa\n",
    "part5_to_int_mapper = dict(zip(part5_unique, part5_index))\n",
    "int_to_part5_mapper = dict(zip(part5_index, part5_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Python list containing unique text cluster values and their corresponding numeric value\n",
    "symptoms_unique = sorted(df.TEXT_CLUSTER_FAMILY.unique().tolist())\n",
    "symptoms_index = [n for n in range(len(symptoms_unique))]\n",
    "\n",
    "# Now create Python dictionaries that map symptom text to integer value and vice versa\n",
    "symptom_to_int_mapper = dict(zip(symptoms_unique, symptoms_index))\n",
    "int_to_symptom_mapper = dict(zip(symptoms_index, symptoms_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Python list containing unique symptom class values and their corresponding numeric value\n",
    "symp_class_unique = sorted(df.SYMP_CLASS_NM.unique().tolist())\n",
    "symp_class_index = [n for n in range(len(symp_class_unique))]\n",
    "\n",
    "# Now create Python dictionaries that map symptom class to integer value and vice versa\n",
    "symp_class_to_int_mapper = dict(zip(symp_class_unique, symp_class_index))\n",
    "int_to_symp_class_mapper = dict(zip(symp_class_index, symp_class_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, here is what my **symptom_to_int_mapper** looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COSMETIC ISSUE': 0,\n",
       " 'FUNCTION ISSUE': 1,\n",
       " 'LEAK': 2,\n",
       " 'NOISE/VIBRATION': 3,\n",
       " 'NOT APPL': 4,\n",
       " 'ODOR': 5,\n",
       " 'WARNING LIGHT ON': 6}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symptom_to_int_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above maps \"COSMETIC ISSUE\" to the numeric value of 0, \"FUNCTION ISSUE\" to 1, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is in action.  You just provide a symptom text cluster, it returns the numeric value for it.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symptom_to_int_mapper['FUNCTION ISSUE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go the other direction with my int_to_symptom_mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FUNCTION ISSUE'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_symptom_mapper[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to be able to re-use these data structures for un-classified data later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data Structure Persistence using Python's pickle library: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save Python data structures to disk/file\n",
    "pickle.dump(part5_to_int_mapper, open(r'D:\\jupyter\\machine_learning\\part5_to_int_mapper.sk','wb'))\n",
    "pickle.dump(symptom_to_int_mapper, open(r'D:\\jupyter\\machine_learning\\symptom_to_int_mapper.sk','wb'))\n",
    "pickle.dump(symp_class_to_int_mapper, open(r'D:\\jupyter\\machine_learning\\symp_class_to_int_mapper.sk','wb'))\n",
    "pickle.dump(int_to_symp_class_mapper, open(r'D:\\jupyter\\machine_learning\\int_to_symp_class_mapper.sk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**-For the sake of simplicity, I resorted to saving the mappings using Python's pickle object serialization library.  In a production environment, it would be more suitable to use a relational database to store the mappings in a table instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use our Python dictionaries to map text value to their respective integer value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below creates a new column called \"PART5\" which is the numeric representation of the \"FAIL_SHORT_PARTNO\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['PART5'] = df.FAIL_SHORT_PARTNO.map(part5_to_int_mapper).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAIL_SHORT_PARTNO        object\n",
       "PART_COST_USD           float64\n",
       "DAYS_TO_FAIL_MINZERO      int64\n",
       "MILES_TO_FAIL             int64\n",
       "TEXT_CLUSTER_FAMILY      object\n",
       "SYMP_CLASS_NM            object\n",
       "PART5                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do the same for text cluster and symptom class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['TEXT_CLUSTER'] = df.TEXT_CLUSTER_FAMILY.map(symptom_to_int_mapper)\n",
    "df['SYMP_CLASS'] = df.SYMP_CLASS_NM.map(symp_class_to_int_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is list of final data columns and their respective data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAIL_SHORT_PARTNO        object\n",
       "PART_COST_USD           float64\n",
       "DAYS_TO_FAIL_MINZERO      int64\n",
       "MILES_TO_FAIL             int64\n",
       "TEXT_CLUSTER_FAMILY      object\n",
       "SYMP_CLASS_NM            object\n",
       "PART5                     int64\n",
       "TEXT_CLUSTER              int64\n",
       "SYMP_CLASS                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to create our features input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features data will consist of: part5, part cost, DTF, MTF, and symptom text cluster (all represented with numeric values thanks to my mappings made earlier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = df[['PART5','PART_COST_USD','DAYS_TO_FAIL_MINZERO','MILES_TO_FAIL','TEXT_CLUSTER']].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our features data does not contain text/string data.  Let's look at the first 10 rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 2.0, 3.0],\n",
       " [6.0, 458.91, 0.0, 11.0, 1.0],\n",
       " [6.0, 455.32, 0.0, 10.0, 1.0],\n",
       " [7.0, 0.0, 0.0, 5.0, 0.0],\n",
       " [11.0, 2.62, 0.0, 20887.0, 1.0],\n",
       " [11.0, 4.37, 0.0, 11849.0, 1.0],\n",
       " [12.0, 3.04, 0.0, 3.0, 6.0],\n",
       " [13.0, 0.0, 0.0, 5.0, 4.0],\n",
       " [19.0, 0.0, 0.0, 11.0, 0.0],\n",
       " [19.0, 0.0, 0.0, 14.0, 0.0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create our target/label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = df.SYMP_CLASS.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[161, 162, 1708, 2280, 248, 248, 1247, 2343, 2293, 2293]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use scikit-learn's NaiveBayes Gaussian classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ML classification algorithms to choose from.  Since I do not know the internal implementation of each and every algorithm, I had to resort to trial-and-error in finding the algorithm which gave me the best results, and quite frankly, I am not knowledgeable on how to perform proper model validation.  The model that gave me best results using large data set was the Naive Bayes classification algorithm.  With smaller data sets, decision tree or random forest algorithm gave me good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training data and target/label data.  Again, the training data is the part #, part cost, DTF, MTF, and symptom text cluster.  The label data is the symptom class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict just one record.**  Here I will feed the prediction model a transmission part # (06200), an arbitrary part cost ($2000), arbitrary DTF (0), MTF (5), and symptom text cluster (\"WARNING LIGHT ON\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRANSMISSION (GENERAL)/FULL ASSY CORE/WARNING LIGHT ON'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criteria = [part5, part cost, dtf, mtf, symptom]\n",
    "criteria = [part5_to_int_mapper['06200'], 2000, 0, 5, symptom_to_int_mapper['WARNING LIGHT ON']]\n",
    "int_to_symp_class_mapper[clf.predict([criteria])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this to the source data, this is correct and makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting 10 records:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 2.0, 3.0],\n",
       " [6.0, 458.91, 0.0, 11.0, 1.0],\n",
       " [6.0, 455.32, 0.0, 10.0, 1.0],\n",
       " [7.0, 0.0, 0.0, 5.0, 0.0],\n",
       " [11.0, 2.62, 0.0, 20887.0, 1.0],\n",
       " [11.0, 4.37, 0.0, 11849.0, 1.0],\n",
       " [12.0, 3.04, 0.0, 3.0, 6.0],\n",
       " [13.0, 0.0, 0.0, 5.0, 4.0],\n",
       " [19.0, 0.0, 0.0, 11.0, 0.0],\n",
       " [19.0, 0.0, 0.0, 14.0, 0.0]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = features[:10]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSA LIGHT ON (NO PARTS REPLACED) 2.0\n",
      "MASTER CYLINDER/BOOSTER/POWER ASSY/FUNCTION ISSUE 11.0\n",
      "MASTER CYLINDER/BOOSTER/POWER ASSY/FUNCTION ISSUE 10.0\n",
      "RELAY/FUSE/FUSE (15A)/COSMETIC ISSUE 5.0\n",
      "BULBS (INTERIOR)/04110/FUNCTION ISSUE 20887.0\n",
      "BULBS (INTERIOR)/04110/FUNCTION ISSUE 11849.0\n",
      "HARNESS/KIT, AUDIO PIN/WARNING LIGHT ON 3.0\n",
      "SRS/CONNECTOR/FUNCTION ISSUE 5.0\n",
      "SIDE PANEL / FENDER/SIDE PANEL (RIGHT SIDE)/COSMETIC ISSUE 11.0\n",
      "SIDE PANEL / FENDER/SIDE PANEL (RIGHT SIDE)/COSMETIC ISSUE 14.0\n"
     ]
    }
   ],
   "source": [
    "for item in test_data:\n",
    "    print(int_to_symp_class_mapper[clf.predict([item])[0]], item[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the above output to the source data, all but 1 was not classified as the original source.  This is pretty good considering generally symptom class name assignments are not completely accurate anyways AND were not expected to be.  Warranty data is inherently dirty data and is reflected in the not-so-perfect symptom class names.  So the performance of my ML classification attempt looks very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Using the Machine Learning Model to Classify Future Claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the model so that we can re-use it without having to retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(clf1, open(r'D:\\jupyter\\machine_learning\\nbayes.sk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-use the Model and Load Helper Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "clf2 = pickle.load(open(r'D:\\jupyter\\machine_learning\\nbayes.sk','rb'))\n",
    "\n",
    "# Load helper data structures that we made earlier\n",
    "part5_to_int_mapper = pickle.load(open(r'D:\\jupyter\\machine_learning\\part5_to_int_mapper.sk', 'rb'))\n",
    "symptom_to_int_mapper = pickle.load(open(r'D:\\jupyter\\machine_learning\\symptom_to_int_mapper.sk', 'rb'))\n",
    "int_to_symp_class_mapper = pickle.load(open(r'D:\\jupyter\\machine_learning\\int_to_symp_class_mapper.sk', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in a production environment, it is probably best to load the mappings from a relational database instead of using Python's pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a single observation using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEAT BELTS/REAR/COSMETIC ISSUE'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criteria = [part5, part cost, dtf, mtf, symptom]\n",
    "criteria = [part5_to_int_mapper['04823'], 0, 0, 207, symptom_to_int_mapper['COSMETIC ISSUE']]\n",
    "int_to_symp_class_mapper[clf2.predict([criteria])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of symptom class names, we can classify warranty claims with other different types of classification labels so this classification example can be extended for any other classification we can come up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not tested this model extensively with other larger test data, but so far I have been impressed with the model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small-scale example shows that a machine learning classification algorithm was able to classify warranty claims without hard-coded algorithms.  It was \"trained\" solely from the training data consisting of just some of the attributes of the warranty claims data."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
