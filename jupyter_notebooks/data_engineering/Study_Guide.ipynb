{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Data Engineering Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idempotence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put, this is the idea that running a task in your dataflow with the same input more than once will have the same effect as running it exactly once.\n",
    "\n",
    "Why would you want this? Say a later stage of your ETL process breaks so you have to rerun the whole thing and earlier stages that did run successfully are run again. Or the pipeline is triggered twice with the same input for some reason. If the jobs are idempotent you won't get nasty side effects, such as primary key errors throwing errors the second time around, or ending up with duplicated entries.\n",
    "\n",
    "Practically, this could mean using upserts instead inserts, or checking to see if the stage has been applied on an input already, so that if it has, it won't run again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directed Acyclic Graphs. This is the idea that you structure your dataflows as a series of tasks, each of which has one concern. On top of that, your task scheduler is aware of dependencies between them. So a later stage of the pipeline will only kick off once all of the input dependencies for that task are met. This graph of tasks is directed because the dependency relation is one way, and acyclic because the there are no cycles in the graph of dependencies. It's a useful abstraction to be able to map out your tasks in terms of, for example: A relies on B, which itself relies of C & D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relating to both of the above, a really useful idea is that of serializing your intermediate steps. By that I mean saving the results of each stage of your pipeline. You can use it enforce idempotence. ie. when processing an input, you can see if there already exists an output for that input, ala memoization. In those cases, you can shortcut to just returning the precomputed result, rather than reprocessing the input file. This is also faster, and allows for efficient rerunning of your pipeline if a later stage breaks. Finally, you can also inspect the contents of intermediate results to check the health of your pipeline and look for processing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL \"as code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an idea that Maxime Beauchemin writes about in his [article](https://www.freecodecamp.org/news/the-rise-of-the-data-engineer-91be18f1e603/), The Rise of the Data Engineer. He argues that the logic required to build modern data pipelines is too complex for drag-and-drop ETL tools, and that complexity is best represented in code.\n",
    "\n",
    "Another advantage of ETL/Infrastructure as code, however, is ability to version control it, audit it and deploy it automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 types of data models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conceptual data model (business stakeholders and data architects) -\"What\"\n",
    "- Logical data model (data architects) - \"How\"\n",
    "- Physical data model (DBA) - \"How\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**<br>\n",
    "- [10-minute guide to data modeling](https://www.credera.com/blog/technology-solutions/data-modeling-explained-in-10-minutes-or-less/)\n",
    "- [Data Modeling 101](http://www.agiledata.org/essays/dataModeling101.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization versus De-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization: breaking a table down into more tables to reduce redundancy and maintain data integrity\n",
    "- De-Normalization: not breaking a table into smaller tables, consolidating to fewer tables, risk for data redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:<br>\n",
    "- [What is normalization?](https://www.guru99.com/database-normalization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 3 Normal Forms or 3NF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1st Normal Form: Remove duplicate data and break data down to granular or more atomic level\n",
    "- 2nd Normal Form: All columns in a table must depend on the primary key column\n",
    "- 3rd Normal Form: A column should not be duplicated or exists in multiple tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are higher normal forms, but achieving 3NF is \"good enough\" for most real world cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not in 3NF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Poor_Design.png \"Bad Design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In 3NF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Better_Design.png \"Bad Design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star vs Snowflake Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [Star vs Snowflake Schema](https://www.vertabelo.com/blog/technical-articles/data-warehouse-modeling-star-schema-vs-snowflake-schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Schema Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Star_Schema_Summary.png \"Star Schema Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Schema Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Star_Schema.png \"Star Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Schema Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Snow_Flake_Summary.png \"Snow Flake Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Schema Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Snow_Flake_Schema.png \"Snow Flake Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Designing a traditional relational database](https://www.youtube.com/watch?v=I_rxqSJAj6U)\n",
    "- [Data Warehouse Design](https://www.youtube.com/watch?v=--OJpdPeH80)\n",
    "- [ETL Design](https://youtu.be/sLhInuwdwcc)\n",
    "- [Star Schema vs Snowflake](https://youtu.be/Qq4yhhAk9fc)\n",
    "- [OLAP vs OLTP](https://youtu.be/AiZWeSUjylU)\n",
    "- [Slowly Changing Dimensions](https://youtu.be/1FZ7et0pN4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectural Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Architectural Considerations](https://medium.com/ssense-tech/principled-data-engineering-part-i-architectural-overview-6d4bdf89b657)\n",
    "- Data Mart\n",
    "- Database\n",
    "- Data Warehouse\n",
    "- Data Lake\n",
    "    - **Raw Data:** A “raw data” bucket which contains the raw,dumps untransformed, and lossless incoming data from all our sources such as event streams from microservices, transactional database snapshots, log dumps, and data from third party sources via FTP or API calls. Data here can be in various formats such as CSV, JSON, flat text files etc., and may not have defined schemas. This bucket plays the key role of guaranteeing no data loss and replayability of our pipelines. In other words, if our pipelines downstream change or fail, our raw data store guarantees that all the original source data remains intact and available for re-processing.\n",
    "    - **Interim Data:** An “interim data” bucket which imports the aforementioned raw data and performs the most minimal transformations required to homogenize its structure, impose schemas, and allow cataloging. Recommend using a format that enforces a schema like parquet. This eliminates a lot of dangerous ambiguities of schema-less data, which can lead to data loss and poor governance. The minimal transformations performed at this step also allow for some critical type management such as homogenizing date formats and number types (decimals, floats, doubles, etc.), and handling null values.\n",
    "    - **Business Data:** A “business data” bucket which presents transformed datasets to end-users. Data here conforms to semantically meaningful naming conventions and each dataset corresponds to a specific business need. Furthermore, the datasets here have more refined schemas that make sense to our end-users — the consumers of this data.\n",
    "- Pipeline\n",
    "    - Desired characteristics:\n",
    "        - Replayable or reproducible\n",
    "        - Has idempotency: applied several times without changing the result beyond the initial application to prevent duplicate or corrupt data\n",
    "    - Types of pipelines:\n",
    "        - ETL: In an ETL pipeline, data is extracted from a source, transformed to the required shape, and inserted into the target. The advantage of ETL is that data enters your system in the shape you want it to be in, and can easily be modeled for analytics. This works exceedingly well when the data is coming from consistent and trusted sources, but can quickly become too brittle in cases where there is a possibility of a schema change, or the data cannot be re-extracted. Imagine a scenario wherein data is extracted from a third-party API, transformed and then loaded into a data warehouse. Some time later the process needs to be rerun, but the source data is no longer available due to the third-party aggregating its data after a certain amount of time (to reduce storage fees). In other words, your system does not offer immutable data.\n",
    "        - ELT: ELT addresses this by extracting and loading the raw data immediately into storage. From there, you can rerun the transformation portion of the pipelines to your heart’s content. The drawback here is that the raw data can potentially be schemaless and unstructured. Managing this raw data becomes the main difficulty in this configuration, and in an era of ever increasing compliance, proper cataloging is critical.\n",
    "        - Pipeline \"as code\" (use of programming languages)\n",
    "        - GUI pipeline tools (Informatica, Talend, Pentaho, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Data Governance](https://medium.com/ssense-tech/principled-data-engineering-part-ii-data-governance-30297abb2446)\n",
    "- Metadata Management\n",
    "    - Data Cataloging: the \"what\" of your data: table names, column names, data type, etc\n",
    "    - Data Lineage: the \"where\" of your data: what are the sources of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLAP - Online Analytical Processing System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical of STAR Schema\n",
    "- De-normalized (dimension and fact design)\n",
    "- Faster analysis and search by combining tables\n",
    "- Requires more data storage due to redundancy\n",
    "- Less complex join complexity compared to OLTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLTP - Online Transaction Processing System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical of Snow Flake Schema\n",
    "- Normalized (1st to 3rd normal form)\n",
    "- Faster inserts, updates, deletes, and improved data quality due to reduced redundancy\n",
    "- Requires less space due to reduction of redundancy\n",
    "- Query performance slower compared to OLAP or STAR Schema due to more tables to scan\n",
    "- Join complexity increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Ecosystem - \"Old\" Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 3 Main Layers**\n",
    "- **HDFS:** Hadoop Distributed File System (storage layer) \n",
    "- **Yarn:** Yet Another Resource Negotiator: How to get all these computers to work together in an \n",
    "  orderly fashion (orchestration layer)\n",
    "- **MapReduce:** How to do stuff with the distributed data in a cluster (execution layer / Java code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Storage layer:** RDD, Apache Arrow\n",
    "- **Query / ETL layer:** Spark SQL, Spark Dataframe, Apache Beam, Apache Pig, Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Web Services (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Storage layer:** S3 buckets\n",
    "- **Query / ETL layer:** Athena (SQL syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ETL tools\n",
    "    - AWS Glue\n",
    "    - Apache Beam\n",
    "    - Apache Spark\n",
    "    - Dask\n",
    "    - Apache Airflow (Python)\n",
    "    - Prefect (Python)\n",
    "    - Papermill (Python)\n",
    "    - SAS\n",
    "    - IBM Informatica\n",
    "    - Talend\n",
    "    - Pentaho\n",
    "- SQL-like tools for Big Data\n",
    "    - AWS Athena / PyAthena\n",
    "    - Apache Spark SQL\n",
    "    - Apache Hive\n",
    "- Dataframe-like tools for Big Data\n",
    "    - Spark dataframe\n",
    "    - [Koalas](https://github.com/databricks/koalas)\n",
    "    - Dask dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data or Storage Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema-less formats\n",
    "    - HDFS\n",
    "    - S3\n",
    "    - RDD\n",
    "Schema formats\n",
    "    - parquet (column oriented, on-disc)\n",
    "    - avro (row oriented, on-disc)\n",
    "    - orc (column oriented, on-disc)\n",
    "    - arrow (column oriented, main memory store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Learning Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Top 10 blogs about transitioning to data engineering](https://blog.insightdatascience.com/top-10-blog-posts-to-help-you-transition-to-data-engineering-1db2312ecdaf)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
