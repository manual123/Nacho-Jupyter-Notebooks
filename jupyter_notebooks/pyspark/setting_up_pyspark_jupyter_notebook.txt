Instructions for setting up a PySpark Jupyter Notebook Environment (Linux)

Install Java >= version 8
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install openjdk-8-jdk
From command line: java -version

Install Spark from https://spark.apache.org/downloads.html
In your ~/.bashrc:
export SPARK_HOME="/home/<your_username>/path_to_spark-2.x.x-bin-hadoop2.7/"
test via at command line: bin/pyspark

Install hadoop from https://hadoop.apache.org/releases.html, download "binary"
In your ~/.bashrc:
export HADOOP_HOME="/home/<your_username>/hadoop-2.x.x"
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
test via at command line: bin/pyspark
See if you still have warning: NativeCodeLoader: Unable to load native-hadoop library for your platform...

# Launch master server
Need your ip # so do ifconfig and this will be referred as your "master ip>
Then start master server:
spark_installation_folder>./sbin/start-master.sh -h <your_master_ip_>
Spark server will be launched and you can then visit localhost:8080 or
<your_master_ip>:8080 in your browser
Make note of the "URL".  This will be referred as your "master URL"
To stop the master server:
spark_installation_folder>./sbin/stop-master.sh

# Connecting your "slaves"
spark_installation_folder>./sbin/start-slave.sh <your_master_url>
Go to your browser and refresh page to see your slave worker
To add more workers, just execute the same command on other computers
To stop the slave workers:
spark_installation_folder>./sbin/stop-slave.sh

# References:
https://thegurus.tech/posts/2019/06/how-to-spark-cluster/
https://thegurus.tech/posts/2019/05/hadoop-python/
For windows: https://medium.com/@naomi.fridman/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f
or https://towardsdatascience.com/installing-apache-pyspark-on-windows-10-f5f0c506bea1
