{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ericxiao251/spark-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pytables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt update<br>\n",
    "sudo apt install default-jdk<br>\n",
    "update-alternatives --list java  # the last line is location of your java runtime<br>\n",
    "vim ~/.profile   then add: export JAVA_HOME=<path_of_java_runtime_but_exclude_/bin/java>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Spark setup with findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local spark\n",
    "findspark.init('/home/pybokeh/envs/py3.7.2/lib/python3.7/site-packages/pyspark/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting PySpark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a PySpark shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('example_app').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas -> spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First integration is about how to move data from pandas library, which is Python standard library to perform in-memory data manipulation, to Spark. First, let’s load a pandas DataFrame. This one is about Air Quality in Madrid (just to satisfy your curiosity, but not important with regards to moving data from one place to another one). You can download it [here](https://www.kaggle.com/decide-soluciones/air-quality-madrid). Make sure you install pytables to read hdf5 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEN</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CO</th>\n",
       "      <th>EBE</th>\n",
       "      <th>NMHC</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO_2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>O_3</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM25</th>\n",
       "      <th>SO_2</th>\n",
       "      <th>TCH</th>\n",
       "      <th>TOL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-07-01 01:00:00</th>\n",
       "      <td>30.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.91</td>\n",
       "      <td>42.639999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>381.299988</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>9.010000</td>\n",
       "      <td>158.899994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.509998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.050003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-01 02:00:00</th>\n",
       "      <td>29.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.59</td>\n",
       "      <td>50.360001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.500000</td>\n",
       "      <td>409.200012</td>\n",
       "      <td>23.820000</td>\n",
       "      <td>104.800003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.950001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.900002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-01 03:00:00</th>\n",
       "      <td>4.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>25.570000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.400002</td>\n",
       "      <td>143.399994</td>\n",
       "      <td>31.059999</td>\n",
       "      <td>48.470001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.270000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-01 04:00:00</th>\n",
       "      <td>4.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>22.629999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.199997</td>\n",
       "      <td>149.300003</td>\n",
       "      <td>23.780001</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-01 05:00:00</th>\n",
       "      <td>2.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>11.920000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.900002</td>\n",
       "      <td>124.800003</td>\n",
       "      <td>29.530001</td>\n",
       "      <td>49.689999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       BEN  CH4    CO        EBE  NMHC  NO        NO_2  \\\n",
       "date                                                                     \n",
       "2001-07-01 01:00:00  30.65  NaN  6.91  42.639999   NaN NaN  381.299988   \n",
       "2001-07-01 02:00:00  29.59  NaN  2.59  50.360001   NaN NaN  209.500000   \n",
       "2001-07-01 03:00:00   4.69  NaN  0.76  25.570000   NaN NaN  116.400002   \n",
       "2001-07-01 04:00:00   4.46  NaN  0.74  22.629999   NaN NaN  116.199997   \n",
       "2001-07-01 05:00:00   2.18  NaN  0.57  11.920000   NaN NaN  100.900002   \n",
       "\n",
       "                             NOx        O_3        PM10  PM25       SO_2  TCH  \\\n",
       "date                                                                            \n",
       "2001-07-01 01:00:00  1017.000000   9.010000  158.899994   NaN  47.509998  NaN   \n",
       "2001-07-01 02:00:00   409.200012  23.820000  104.800003   NaN  20.950001  NaN   \n",
       "2001-07-01 03:00:00   143.399994  31.059999   48.470001   NaN  11.270000  NaN   \n",
       "2001-07-01 04:00:00   149.300003  23.780001   47.500000   NaN  10.100000  NaN   \n",
       "2001-07-01 05:00:00   124.800003  29.530001   49.689999   NaN   7.680000  NaN   \n",
       "\n",
       "                           TOL  \n",
       "date                            \n",
       "2001-07-01 01:00:00  76.050003  \n",
       "2001-07-01 02:00:00  84.900002  \n",
       "2001-07-01 03:00:00  20.980000  \n",
       "2001-07-01 04:00:00  14.770000  \n",
       "2001-07-01 05:00:00   8.970000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality_df = pd.read_hdf('/home/pybokeh/Downloads/air-quality-madrid/madrid.h5', key='28079008')\n",
    "air_quality_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make some changes to this DataFrame, like resetting datetime index to not lose information when loading into Spark. Datetime will also be transformed to string as Spark has some issues working with dates (related to system locale, timezones, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df.reset_index(inplace=True)\n",
    "air_quality_df['date'] = air_quality_df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply load from pandas to Spark with ```createDataFrame```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once DataFrame is loaded into Spark (as ```air_quality_sdf``` here), can be manipulated easily using PySpark methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('BEN', 'double'),\n",
       " ('CH4', 'double'),\n",
       " ('CO', 'double'),\n",
       " ('EBE', 'double'),\n",
       " ('NMHC', 'double'),\n",
       " ('NO', 'double'),\n",
       " ('NO_2', 'double'),\n",
       " ('NOx', 'double'),\n",
       " ('O_3', 'double'),\n",
       " ('PM10', 'double'),\n",
       " ('PM25', 'double'),\n",
       " ('SO_2', 'double'),\n",
       " ('TCH', 'double'),\n",
       " ('TOL', 'double')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality_sdf = spark.createDataFrame(air_quality_df)\n",
    "air_quality_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               date|               NOx|\n",
      "+-------------------+------------------+\n",
      "|2001-07-01 01:00:00|            1017.0|\n",
      "|2001-07-01 02:00:00|409.20001220703125|\n",
      "|2001-07-01 03:00:00|143.39999389648438|\n",
      "|2001-07-01 04:00:00| 149.3000030517578|\n",
      "|2001-07-01 05:00:00|124.80000305175781|\n",
      "+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "air_quality_sdf.select('date', 'NOx').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas -> spark -> hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To persist a Spark DataFrame into HDFS, where it can be queried using default Hadoop SQL engine (Hive), one straightforward strategy (not the only one) is to create a temporal view from that DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_sdf.createOrReplaceTempView(\"air_quality_sdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the temporal view is created, it can be used from Spark SQL engine to create a real table using create table as select. Before creating this table, I will create a new database called ```analytics``` to store it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping database...\n",
      "creating database...\n",
      "dropping table...\n",
      "creating table...\n"
     ]
    }
   ],
   "source": [
    "sql_drop_table = \"\"\"\n",
    "drop table if exists analytics.pandas_spark_hive\n",
    "\"\"\"\n",
    "\n",
    "sql_drop_database = \"\"\"\n",
    "drop database if exists analytics cascade\n",
    "\"\"\"\n",
    "\n",
    "sql_create_database = \"\"\"\n",
    "create database if not exists analytics\n",
    "location '/home/pybokeh/temp/cloudera/analytics/'\n",
    "\"\"\"\n",
    "\n",
    "sql_create_table = \"\"\"\n",
    "create table if not exists analytics.pandas_spark_hive\n",
    "using parquet\n",
    "as select to_timestamp(date) as date_parsed, *\n",
    "from air_quality_sdf\n",
    "\"\"\"\n",
    "\n",
    "print(\"dropping database...\")\n",
    "result_drop_db = spark.sql(sql_drop_database)\n",
    "\n",
    "print(\"creating database...\")\n",
    "result_create_db = spark.sql(sql_create_database)\n",
    "\n",
    "print(\"dropping table...\")\n",
    "result_droptable = spark.sql(sql_drop_table)\n",
    "\n",
    "print(\"creating table...\")\n",
    "result_create_table = spark.sql(sql_create_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can check results using Spark SQL engine, for example to select ozone pollutant concentration over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|        date_parsed|               O_3|\n",
      "+-------------------+------------------+\n",
      "|2001-07-01 01:00:00| 9.010000228881836|\n",
      "|2001-07-01 02:00:00| 23.81999969482422|\n",
      "|2001-07-01 03:00:00|31.059999465942383|\n",
      "|2001-07-01 04:00:00|23.780000686645508|\n",
      "|2001-07-01 05:00:00|29.530000686645508|\n",
      "+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from analytics.pandas_spark_hive\").select(\"date_parsed\", \"O_3\").show(5)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
