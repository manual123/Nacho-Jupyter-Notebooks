{
 "metadata": {
  "name": "",
  "signature": "sha256:e586c7ce52656e5b15e6a5af32250e16c3a08411e7da2c1afb89acb747e3d8d6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scoring using the Vector Space Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously we discussed tf-idf as a way to calculate how relevant a search term is given a set of indexed documents. When having multiple terms, we used *overlap score measure* consisting in the sum of the *tf-idf* for each term in the given input. A more general and flexible way of scoring multi-term searches is using the **vector space model**.    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the vector space model, each document in the corpus is represented by a vector. The search input terms are also represented by a vector. Scoring search results consists then in vector operations between the documents vectors and the search terms vector.  \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But what is each of these vectors made of? Basically they define term frequencies. That is, each dimension in a vector represents the term frequency for a given term. Then, a document is represented in a multi-dimensional space by a vector of the frequencies of each of the words in hte corpus. Equally, a search input is represented by a vector in the same space but using the input terms.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Python implementation  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following is a Python representation if this concept.   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "class IrIndex:\n",
      "    \"\"\"An in-memory inverted index\"\"\"\n",
      "\n",
      "    pattern = re.compile(\"^\\s+|\\s*,*\\s*|\\s+$\")\n",
      "\n",
      "    def __init__(self):\n",
      "        self.index = {}\n",
      "        self.documents = []\n",
      "        self.vectors = []\n",
      "\n",
      "    def index_document(self, document):\n",
      "        # split\n",
      "        terms = [word for word in self.pattern.split(document)]\n",
      "        # add to documents\n",
      "        self.documents.append(document)\n",
      "        document_pos = len(self.documents) - 1\n",
      "        # add posts to index, while creating document vector\n",
      "        vector = {}\n",
      "        for term in terms:\n",
      "            if term not in self.index:\n",
      "                self.index[term] = []\n",
      "            self.index[term].append(document_pos)\n",
      "            if term not in vector:\n",
      "                vector[term] = 1\n",
      "            else:\n",
      "                vector[term] += 1\n",
      "        # add the vector\n",
      "        self.vectors.append(vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the same `IrIdenx` class and a boolean search schema. The difference is when calculating scores. We don't store a precalculated `tf` structure anymore but operate vectors directly.  \n",
      "\n",
      "In terms of complexity, when indexing we have moved from:  \n",
      "* Recalculate every `tf` entry when indexing a new document. This involves lookup + sum for each term in the document.    \n",
      "\n",
      "To:  \n",
      "* Indexing stage: store a new document as a vectors of `tf` values. Here we save the recalculation of `tf` entries.    \n",
      "\n",
      "So as we can see, the indexing stage is simpler (and more scalable) when using the vector space model. This scalability gain when indexing is not to be overlooked. In an index with hundreds of thousands or even millions of terms, indexing a new large document and recalculating term frequencies at a global scale can be costly. We could calculate term frequencies when searching instead, but then using the vector space model makes even more sense.  \n",
      "\n",
      "Next the search and scoring part.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import array, dot\n",
      "from math import log\n",
      "\n",
      "def create_tfidf_list(self, *args):\n",
      "        if len(args) == 1:\n",
      "            res = [tf for tf in args[0].itervalues()]\n",
      "        elif len(args) == 2:\n",
      "            res = []\n",
      "            for term in args[0].iterkeys():\n",
      "                if term in args[1]:\n",
      "                    idf = log(float(len(self.documents)) / float(len(self.index[term])))\n",
      "                    res.append(args[1][term] * idf)\n",
      "                else:\n",
      "                    res.append(0)\n",
      "        return res\n",
      "\n",
      "def create_tf_dictionary(self, terms):\n",
      "    res = {}\n",
      "    for term in self.pattern.split(terms):\n",
      "        if term not in res:\n",
      "            res[term] = terms.count(term)\n",
      "    return res\n",
      "\n",
      "def vector_space_search(self, terms):\n",
      "    res = []\n",
      "    hits = {}\n",
      "    # create a numeric vector from terms\n",
      "    terms_tf_dictionary = self.create_tf_dictionary(terms)\n",
      "    terms_tfidf_list = self.create_tfidf_list(terms_tf_dictionary)\n",
      "    # create a numeric vector for each hitting document\n",
      "    hitting_terms = [term for term in self.pattern.split(terms) if term in self.index]\n",
      "    for term in hitting_terms:  # for each term having at least on hit...\n",
      "        for post in self.index[term]:  # for each document create the numeric vector\n",
      "            if post not in hits:\n",
      "                tfidf_list = self.create_tfidf_list(terms_tf_dictionary, self.vectors[post])\n",
      "                hits[post] = tfidf_list\n",
      "    # do the dot products\n",
      "    for post in hits.iterkeys():\n",
      "        score = dot(array(terms_tfidf_list), array(hits[post]))\n",
      "        res.append((score, self.documents[post]))\n",
      "    return res\n",
      "\n",
      "\n",
      "IrIndex.create_tf_dictionary = create_tf_dictionary\n",
      "IrIndex.create_tfidf_list = create_tfidf_list\n",
      "IrIndex.vector_space_search = vector_space_search"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At search stage, we have moved from:  \n",
      "\n",
      "* Calculate the `idf` and access the `tf` lookup table for each search term and document hit. Sum the resulting `tf-idf` values for each document hit. This is done using two for loops, one of them including another nested internal for loop.    \n",
      "\n",
      "To:  \n",
      "* Access the `index` lookup table for any of the search terms and perform dot-product with the resulting vectors. The later is an overhead introduced by this approach in the search stage.    \n",
      "\n",
      "The new search stage has introduced vector dot products where there were just sums (although using nested lopps) when the vector space model was not used. However the data structures and their usage has been simplified. Note that we build the vectors from pre-calculated dictionaries. Doing so we can determine the dimensions form the search query vector.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Other benefits of the Vector Space Model  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But what other benefits come with the vector space model? These are some of them: \n",
      "* Treating queries as vectors allows us simplifying data structures and calculations. Where we used two dictionaries and loops, now we use a single dictionary and linear algebra.  \n",
      "* The compact and easy to operate vector representation leaves the door open to different weighting and transformation schemas that were difficult to apply before (or at least the result were not so clean).  \n",
      "* Vectors can be the input of additional Information Retrieval and Machine Learning techniques including supervised (e.g. classification) and unsupervised (e.g. clustering, frequent pattern mining).  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Examples  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us now recall our sample wine-related mini-corpus in order to see if we get similar results using the new Vector Space Model. Remember that results are given unsorted. Just pay attention to the scores.    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = IrIndex()\n",
      "index.index_document(\"Bruno Clair Chambertin Clos de Beze 2001, Bourgogne, France\")\n",
      "index.index_document(\"Bruno Clair Chambertin Clos de Beze 2005, Bourgogne, France\")\n",
      "index.index_document(\"Bruno Clair Clos Saint Jaques 2001, Bourgogne, France\")\n",
      "index.index_document(\"Bruno Clair Clos Saint Jaques 2002, Bourgogne, France\")\n",
      "index.index_document(\"Bruno Clair Clos Saint Jaques 2005, Bourgogne, France\")\n",
      "index.index_document(\"Coche-Dury Bourgogne Chardonay 2005, Bourgogne, France\")\n",
      "index.index_document(\"Chateau Margaux 1982, Bordeaux, France\")\n",
      "index.index_document(\"Chateau Margaux 1996, Bordeaux, France\")\n",
      "index.index_document(\"Chateau Latour 1982, Bordeaux, France\")\n",
      "index.index_document(\"Domaine Raveneau Le Clos 2001, Bourgogne, France\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.vector_space_search(\"hello\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.vector_space_search(\"Bordeaux\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[(1.2039728043259361, 'Chateau Latour 1982, Bordeaux, France'),\n",
        " (1.2039728043259361, 'Chateau Margaux 1982, Bordeaux, France'),\n",
        " (1.2039728043259361, 'Chateau Margaux 1996, Bordeaux, France')]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.vector_space_search(\"Margaux\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[(1.6094379124341003, 'Chateau Margaux 1982, Bordeaux, France'),\n",
        " (1.6094379124341003, 'Chateau Margaux 1996, Bordeaux, France')]"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.vector_space_search(\"Bourgogne\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "[(0.22314355131420976,\n",
        "  'Bruno Clair Chambertin Clos de Beze 2001, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Chambertin Clos de Beze 2005, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Clos Saint Jaques 2001, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Clos Saint Jaques 2002, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Clos Saint Jaques 2005, Bourgogne, France'),\n",
        " (0.44628710262841953,\n",
        "  'Coche-Dury Bourgogne Chardonay 2005, Bourgogne, France'),\n",
        " (0.22314355131420976, 'Domaine Raveneau Le Clos 2001, Bourgogne, France')]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.vector_space_search(\"hello Bordeaux\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "[(1.2039728043259361, 'Chateau Latour 1982, Bordeaux, France'),\n",
        " (1.2039728043259361, 'Chateau Margaux 1982, Bordeaux, France'),\n",
        " (1.2039728043259361, 'Chateau Margaux 1996, Bordeaux, France')]"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.vector_space_search(\"Bourgogne Bordeaux\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "[(0.22314355131420976,\n",
        "  'Bruno Clair Chambertin Clos de Beze 2001, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Chambertin Clos de Beze 2005, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Clos Saint Jaques 2001, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Clos Saint Jaques 2002, Bourgogne, France'),\n",
        " (0.22314355131420976,\n",
        "  'Bruno Clair Clos Saint Jaques 2005, Bourgogne, France'),\n",
        " (0.44628710262841953,\n",
        "  'Coche-Dury Bourgogne Chardonay 2005, Bourgogne, France'),\n",
        " (1.2039728043259361, 'Chateau Margaux 1982, Bordeaux, France'),\n",
        " (1.2039728043259361, 'Chateau Margaux 1996, Bordeaux, France'),\n",
        " (1.2039728043259361, 'Chateau Latour 1982, Bordeaux, France'),\n",
        " (0.22314355131420976, 'Domaine Raveneau Le Clos 2001, Bourgogne, France')]"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.vector_space_search(\"Margaux Bordeaux\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[(1.2039728043259361, 'Chateau Latour 1982, Bordeaux, France'),\n",
        " (2.8134107167600364, 'Chateau Margaux 1982, Bordeaux, France'),\n",
        " (2.8134107167600364, 'Chateau Margaux 1996, Bordeaux, France')]"
       ]
      }
     ],
     "prompt_number": 36
    }
   ],
   "metadata": {}
  }
 ]
}