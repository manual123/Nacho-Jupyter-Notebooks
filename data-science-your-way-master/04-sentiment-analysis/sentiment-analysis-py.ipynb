{
 "metadata": {
  "name": "",
  "signature": "sha256:2c13f9e76ba1bb3e89c76c1c63578de7e2c4ba6301c1a7a01a1c92666b071e7d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting and Preparing data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although later on we will use `sklearn.feature_extraction.text.CountVectorizer` to create a bag-of-words set of features, and this library directly accepts a file name, we need to pass instead a secuence of documents since our training file contains not just text but also sentiment tags (that we need to strip out).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "\n",
      "# define URLs\n",
      "test_data_url = \"https://kaggle2.blob.core.windows.net/competitions-data/inclass/2558/testdata.txt?sv=2012-02-12&se=2015-08-06T10%3A32%3A23Z&sr=b&sp=r&sig=a8lqVKO0%2FLjN4hMrFo71sPcnMzltKk1HN8m7OPolArw%3D\"\n",
      "train_data_url = \"https://kaggle2.blob.core.windows.net/competitions-data/inclass/2558/training.txt?sv=2012-02-12&se=2015-08-06T10%3A34%3A08Z&sr=b&sp=r&sig=meGjVzfSsvayeJiDdKY9S6C9ep7qW8v74M6XzON0YQk%3D\"\n",
      "\n",
      "# define local file names\n",
      "test_data_file_name = 'test_data.csv'\n",
      "train_data_file_name = 'train_data.csv'\n",
      "\n",
      "# download files using urlib\n",
      "test_data_f = urllib.urlretrieve(test_data_url, test_data_file_name)\n",
      "train_data_f = urllib.urlretrieve(train_data_url, train_data_file_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our files downloaded locally, we can load them into data frames for processing.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "test_data_df = pd.read_csv(test_data_file_name, header=None, delimiter=\"\\t\", quoting=3)\n",
      "test_data_df.columns = [\"Text\"]\n",
      "train_data_df = pd.read_csv(train_data_file_name, header=None, delimiter=\"\\t\", quoting=3)\n",
      "train_data_df.columns = [\"Sentiment\",\"Text\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "(7086, 2)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data_df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "(33052, 1)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, `header=0` indicates that the first line of the file contains column names, `delimiter=\\t` indicates that the fields are separated by tabs, and `quoting=3` tells Python to ignore doubled quotes, otherwise you may encounter errors trying to read the file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check the first few lines of the train data.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Sentiment</th>\n",
        "      <th>Text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td>           The Da Vinci Code book is just awesome.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> this was the first clive cussler i've ever rea...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td>                  i liked the Da Vinci Code a lot.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td>                  i liked the Da Vinci Code a lot.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 1</td>\n",
        "      <td> I liked the Da Vinci Code but it ultimatly did...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "   Sentiment                                               Text\n",
        "0          1            The Da Vinci Code book is just awesome.\n",
        "1          1  this was the first clive cussler i've ever rea...\n",
        "2          1                   i liked the Da Vinci Code a lot.\n",
        "3          1                   i liked the Da Vinci Code a lot.\n",
        "4          1  I liked the Da Vinci Code but it ultimatly did..."
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> \" I don't care what anyone says, I like Hillar...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>                 have an awesome time at purdue!..</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> Yep, I'm still in London, which is pretty awes...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> Have to say, I hate Paris Hilton's behavior bu...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>                           i will love the lakers.</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "                                                Text\n",
        "0  \" I don't care what anyone says, I like Hillar...\n",
        "1                  have an awesome time at purdue!..\n",
        "2  Yep, I'm still in London, which is pretty awes...\n",
        "3  Have to say, I hate Paris Hilton's behavior bu...\n",
        "4                            i will love the lakers."
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's count how many labels do we have for each sentiment class.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data_df.Sentiment.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "1    3995\n",
        "0    3091\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, let's calculate the average number of words per sentence. We could do the following using a list comprehension with the number of words per sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np \n",
      "\n",
      "np.mean([len(s.split(\" \")) for s in train_data_df.Text])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "10.886819079875812"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparing a *corpus*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The class [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in the wonderful `scikit learn` Python library converts a collection of text documents to a matrix of token counts. This is just what we need to implement later on our bag-of-words linear classifier.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we need to init the vectorizer. We need to remove puntuations, lowercase, remove stop words, and stem words. All these steps can be directly performed by `CountVectorizer` if we pass the right parameter values. We can do as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re, nltk\n",
      "from sklearn.feature_extraction.text import CountVectorizer        \n",
      "from nltk.stem.porter import PorterStemmer\n",
      "\n",
      "#######\n",
      "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
      "stemmer = PorterStemmer()\n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "\n",
      "def tokenize(text):\n",
      "    # remove non letters\n",
      "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
      "    # tokenize\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    # stem\n",
      "    stems = stem_tokens(tokens, stemmer)\n",
      "    return stems\n",
      "######## \n",
      "\n",
      "vectorizer = CountVectorizer(\n",
      "    analyzer = 'word',\n",
      "    tokenizer = tokenize,\n",
      "    lowercase = True,\n",
      "    stop_words = 'english',\n",
      "    max_features = 85\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The method `fit_transform` does two functions: First, it fits the model and learns the vocabulary; second, it transforms our corpus data into feature vectors. The input to `fit_transform` should be a list of strings, so we concatenate train and test data as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_data_features = vectorizer.fit_transform(train_data_df.Text.tolist() + test_data_df.Text.tolist())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Numpy arrays are easy to work with, so convert the result to an array."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_data_features_nd = corpus_data_features.toarray()\n",
      "corpus_data_features_nd.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "(40138, 85)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Take a look at the words in the vocabulary\n",
      "vocab = vectorizer.get_feature_names()\n",
      "print vocab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'aaa', u'amaz', u'angelina', u'awesom', u'beauti', u'becaus', u'boston', u'brokeback', u'citi', u'code', u'cool', u'cruis', u'd', u'da', u'drive', u'francisco', u'friend', u'fuck', u'geico', u'good', u'got', u'great', u'ha', u'harri', u'harvard', u'hate', u'hi', u'hilton', u'honda', u'imposs', u'joli', u'just', u'know', u'laker', u'left', u'like', u'littl', u'london', u'look', u'lot', u'love', u'm', u'macbook', u'make', u'miss', u'mission', u'mit', u'mountain', u'movi', u'need', u'new', u'oh', u'onli', u'pari', u'peopl', u'person', u'potter', u'purdu', u'realli', u'right', u'rock', u's', u'said', u'san', u'say', u'seattl', u'shanghai', u'stori', u'stupid', u'suck', u't', u'thi', u'thing', u'think', u'time', u'tom', u'toyota', u'ucla', u've', u'vinci', u'wa', u'want', u'way', u'whi', u'work']\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also print the counts of each word in the vocabulary as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sum up the counts of each vocabulary word\n",
      "dist = np.sum(corpus_data_features_nd, axis=0)\n",
      "\n",
      "# For each, print the vocabulary word and the number of times it \n",
      "# appears in the training set\n",
      "for tag, count in zip(vocab, dist):\n",
      "    print count, tag"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1179 aaa\n",
        "485 amaz\n",
        "1765 angelina\n",
        "3170 awesom\n",
        "2146 beauti\n",
        "1694 becaus\n",
        "2190 boston\n",
        "2000 brokeback\n",
        "423 citi\n",
        "2003 code\n",
        "481 cool\n",
        "2031 cruis\n",
        "439 d\n",
        "2087 da\n",
        "433 drive\n",
        "1926 francisco\n",
        "477 friend\n",
        "452 fuck\n",
        "1085 geico\n",
        "773 good\n",
        "571 got\n",
        "1178 great\n",
        "776 ha\n",
        "2094 harri\n",
        "2103 harvard\n",
        "4492 hate\n",
        "794 hi\n",
        "2086 hilton\n",
        "2192 honda\n",
        "1098 imposs\n",
        "1764 joli\n",
        "1054 just\n",
        "896 know\n",
        "2019 laker\n",
        "425 left\n",
        "4080 like\n",
        "507 littl\n",
        "2233 london\n",
        "811 look\n",
        "421 lot\n",
        "10334 love\n",
        "1568 m\n",
        "1059 macbook\n",
        "631 make\n",
        "1098 miss\n",
        "1101 mission\n",
        "1340 mit\n",
        "2081 mountain\n",
        "1207 movi\n",
        "1220 need\n",
        "459 new\n",
        "551 oh\n",
        "674 onli\n",
        "2094 pari\n",
        "1018 peopl\n",
        "454 person\n",
        "2093 potter\n",
        "1167 purdu\n",
        "2126 realli\n",
        "661 right\n",
        "475 rock\n",
        "3914 s\n",
        "495 said\n",
        "2038 san\n",
        "627 say\n",
        "2019 seattl\n",
        "1189 shanghai\n",
        "467 stori\n",
        "2886 stupid\n",
        "4614 suck\n",
        "1455 t\n",
        "1705 thi\n",
        "662 thing\n",
        "1524 think\n",
        "781 time\n",
        "2117 tom\n",
        "2028 toyota\n",
        "2008 ucla\n",
        "774 ve\n",
        "2001 vinci\n",
        "3703 wa\n",
        "1656 want\n",
        "932 way\n",
        "547 whi\n",
        "512 work\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A bag-of-words linear classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to perform logistic regression in Python we use [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). But first let's split our training data in order to get an evaluation set. We will use [sklearn.cross_validation.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "# remember that corpus_data_features_nd contains all of our original train and test data, so we need to exclude\n",
      "# the unlabeled test entries\n",
      "X_train, X_test, y_train, y_test  = train_test_split(\n",
      "    corpus_data_features_nd[0:len(train_data_df)], \n",
      "    train_data_df.Sentiment,\n",
      "    train_size=0.85, \n",
      "    random_state=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we are ready to train our classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_model = LogisticRegression()\n",
      "log_model = log_model.fit(X=X_train, y=y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we use the classifier to label our evaluation set. We can use either `predict` for classes or `predict_proba` for probabilities.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = log_model.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a function for classification called [sklearn.metrics.classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) which calculates several types of (predictive) scores on a classification model. Check also [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics). In this case we want to check our classifier accuracy.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "\n",
      "print(classification_report(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.98      0.99      0.98       467\n",
        "          1       0.99      0.98      0.99       596\n",
        "\n",
        "avg / total       0.98      0.98      0.98      1063\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can re-train our model with all the training data and use it for sentiment classification with the original (unlabeled) test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train classifier\n",
      "log_model = LogisticRegression()\n",
      "log_model = log_model.fit(X=corpus_data_features_nd[0:len(train_data_df)], y=train_data_df.Sentiment)\n",
      "\n",
      "# get predictions\n",
      "test_pred = log_model.predict(corpus_data_features_nd[len(train_data_df):])\n",
      "\n",
      "# sample some of them\n",
      "import random\n",
      "spl = random.sample(xrange(len(test_pred)), 15)\n",
      "\n",
      "# print text and labels\n",
      "for text, sentiment in zip(test_data_df.Text[spl], test_pred[spl]):\n",
      "    print sentiment, text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 I love paris hilton...\n",
        "1 i love seattle..\n",
        "1 I love those GEICO commercials especially the one with Mini-Me doing that little rap dance.; )..\n",
        "1 However you look at it, I've been here almost two weeks now and so far I absolutely love UCLA...\n",
        "0 By the time I left the Hospital, I had no time to drive to Boston-which sucked because 1)\n",
        "1 Before I left Missouri, I thought London was going to be so good and cool and fun and a really great experience and I was really excited.\n",
        "0 I know you're way too smart and way too cool to let stupid UCLA get to you...\n",
        "0 PARIS HILTON SUCKS!\n",
        "1 Geico was really great.\n",
        "0 hy I Hate San Francisco, # 29112...\n",
        "0 I need to pay Geico and a host of other bills but that is neither here nor there.\n",
        "1 As much as I love the Lakers and Kobe, I still have to state the facts...\n",
        "1 I'm biased, I love Angelina Jolie..\n",
        "0 I despise Hillary Clinton, but I don't think she's cold.\n",
        "0 i hate geico and old navy.\n"
       ]
      }
     ],
     "prompt_number": 44
    }
   ],
   "metadata": {}
  }
 ]
}