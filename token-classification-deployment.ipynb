{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Named Entity Recognition Model in Jarvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Transfer Learning Toolkit](https://developer.nvidia.com/transfer-learning-toolkit) (TLT) provides the capability to export your model in a format that can deployed using [Nvidia Jarvis](https://developer.nvidia.com/nvidia-jarvis), a highly performant application framework for multi-modal conversational AI services using GPUs. \n",
    "\n",
    "This tutorial explores taking an .ejrvs model, the result of `tlt token_classification` command, and leveraging the Jarvis ServiceMaker framework to aggregate all the necessary artifacts for Jarvis deployment to a target environment. Once the model is deployed in Jarvis, you can issue inference requests to the server. We will demonstrate how quick and straightforward this whole process is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "In this notebook, you will learn how to:  \n",
    "- Use Jarvis ServiceMaker to take a TLT exported .ejrvs and convert it to .jmir\n",
    "- Deploy the model(s) locally  on the Jarvis Server\n",
    "- Send inference requests from a demo client using Jarvis API bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along, please make sure:\n",
    "- You have access to NVIDIA NGC, and are able to download the Jarvis Quickstart [resources](https://ngc.nvidia.com/resources/ea-jarvis-stage:jarvis_quickstart/)\n",
    "- Have an .ejrvs model file that you wish to deploy. You can obtain this from `tlt <task> export` (with `export_format=JARVIS`). Please refer the tutorial on *Named entity recognition using Transfer Learning Toolkit* for more details on training and exporting an .ejrvs model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Jarvis ServiceMaker\n",
    "Servicemaker is the set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Jarvis deployment to a target environment. It has two main components as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jarvis-build\n",
    "\n",
    "This step helps build a Jarvis-ready version of the model. Itâ€™s only output is an intermediate format (called a JMIR) of an end to end pipeline for the supported services within Jarvis. We are taking a NER BERT Model in consideration<br>\n",
    "\n",
    "`jarvis-build` is responsible for the combination of one or more exported models (.ejrvs files) into a single file containing an intermediate format called Jarvis Model Intermediate Representation (.jmir). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. Please checkout the [documentation](http://docs.jarvis-ai.nvidia.com/release-1-0/service-nlp.html#pipeline-configuration) to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE THESE PATHS \n",
    "\n",
    "# ServiceMaker Docker\n",
    "JARVIS_SM_CONTAINER = \"<add container name>\"\n",
    "\n",
    "# Directory where the .ejrvs model is stored $MODEL_LOC/*.ejrvs\n",
    "MODEL_LOC = \"<add path to model location>\"\n",
    "\n",
    "# Name of the .erjvs file\n",
    "MODEL_NAME = \"<add model name>\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TLT\n",
    "KEY = \"<add encryption key used for trained model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ServiceMaker docker\n",
    "!docker pull $JARVIS_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: jarvis-build <task-name> output-dir-for-jmir/model.jmir:key dir-for-ejrvs/model.ejrvs:key\n",
    "!docker run --rm --gpus 0 -v $MODEL_LOC:/data $JARVIS_SM_CONTAINER -- \\\n",
    "        jarvis-build token_classification /data/token-classification.jmir:$KEY /data/$MODEL_NAME:$KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Jarvis-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Jarvis Model Intermediate Representation (JMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: jarvis-deploy -f dir-for-jmir/model.jmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $JARVIS_SM_CONTAINER -- \\\n",
    "            jarvis-deploy -f  /data/token-classification.jmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start Jarvis Server\n",
    "Once the model repository is generated, we are ready to start the Jarvis server. From this step onwards you need to download the Jarvis QuickStart Resource from NGC. \n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Jarvis QuickStart directory\n",
    "JARVIS_DIR = \"<Path to the uncompressed folder downloaded from quickstart(include the folder name)>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify config.sh to enable relevant Jarvis services (nlp for token classification), provide the encryption key, and path to the model repository (`jarvis_model_loc`) generated in the previous step among other configurations. \n",
    "\n",
    "For instance, if above the model repository is generated at `$MODEL_LOC/models`, then you can specify `jarvis_model_loc` as the same directory as `MODEL_LOC` <br>\n",
    "\n",
    "Pretrained versions of models specified in models_asr/nlp/tts are fetched from NGC. Since we are using our custom model, we can comment it in models_asr (and any others that are not relevant to your use case). <br>\n",
    "\n",
    "`NOTE:` Please perform the step of editing config.sh outside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Jarvis Services \n",
    "service_enabled_asr=false                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=true                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                                     ## MAKE CHANGES HERE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"                                                  ## MAKE CHANGES HERE\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# jarvis_init.sh will create a `jmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# JMIR ($jarvis_model_loc/jmir)\n",
    "# Jarvis uses an intermediate representation (JMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $jarvis_model_loc/jmir by `jarvis_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TLT and prepared using jarvis-build\n",
    "# may also be copied manually to this location $(jarvis_model_loc/jmir).\n",
    "#\n",
    "# Models ($jarvis_model_loc/models)\n",
    "# During the jarvis_init process, the JMIR files in $jarvis_model_loc/jmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $jarvis_model_loc/models. The jarvis server exclusively uses these\n",
    "# optimized versions.\n",
    "jarvis_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with MODEL_LOC)                      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $JARVIS_DIR && chmod +x ./jarvis_init.sh && chmod +x ./jarvis_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Jarvis Init. This will fetch the containers/models\n",
    "# YOU CAN SKIP THIS STEP IF YOU DID JARVIS DEPLOY\n",
    "! cd $JARVIS_DIR && ./jarvis_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Jarvis Start. This will deploy your model(s).\n",
    "! cd $JARVIS_DIR && ./jarvis_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Jarvis server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send GRPC requests, you can install Jarvis Python API bindings for client. This is available as a pip .whl with the QuickStart.\n",
    "\n",
    "Otherwise, you can use the jarvis-client docker container which comes pre-installed with all the inference dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install client API bindings\n",
    "! cd $JARVIS_DIR && pip install <add .whl file>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Jarvis server and run inference\n",
    "Now we actually query the Jarvis server, let's get started. The following cell queries the jarvis server(using grpc) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $JARVIS_DIR/ner_client.py\n",
    "\n",
    "import grpc\n",
    "import os\n",
    "import argparse\n",
    "import jarvis_api.jarvis_nlp_pb2 as jnlp\n",
    "import jarvis_api.jarvis_nlp_pb2_grpc as jnlp_srv\n",
    "\n",
    "# use the NER network to return top-1 classes for entities\n",
    "def postprocess_labels_server(tokens_response):\n",
    "    results = []\n",
    "    for i in range(0, len(tokens_response.results)):\n",
    "        slots = []\n",
    "        slot_scores = []\n",
    "        tokens = []\n",
    "        for j in range(0, len(tokens_response.results[i].results)):\n",
    "          entity = tokens_response.results[i].results[j]\n",
    "          tokens.append(entity.token)\n",
    "          slots.append(entity.label[0].class_name)\n",
    "          slot_scores.append(entity.label[0].score)\n",
    "        results.append((slots, tokens, slot_scores))\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_ner(grpc_server, query):\n",
    "    channel = grpc.insecure_channel(grpc_server)\n",
    "    jarvis_nlp = jnlp_srv.JarvisNLPStub(channel)\n",
    "    req = jnlp.AnalyzeEntitiesRequest()\n",
    "    req.query = query\n",
    "    resp = jarvis_nlp.AnalyzeEntities(req)\n",
    "    print(\"Query:\", query)\n",
    "    print(postprocess_labels_server(resp))\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Client app to test named entity recognition on Jarvis\")\n",
    "    parser.add_argument(\"--server\", default=\"localhost:50051\", type=str, help=\"URI to GRPC server endpoint\")\n",
    "    parser.add_argument(\"--query\", default=\"NVIDIA is located at Santa Clara\", type=str, help=\"Input Query\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def run_ner_client():\n",
    "    args = get_args()\n",
    "    run_ner(args.server, query=args.query)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_ner_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've installed the Jarvis Python API bindings for client using the pip whl, you can execute the above saved script with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3 $JARVIS_DIR/ner_client.py --query \"NVIDIA is located at Santa Clara\" --server localhost:50051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, to use the Jarvis client docker container, you can execute the below docker run command with the right `JARVIS_CLIENT_CONTAINER` name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JARVIS_CLIENT_CONTAINER = \"<add container name>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d --privileged \\\n",
    "    -v $JARVIS_DIR:/jarvis \\\n",
    "    --net=host --rm \\\n",
    "    --name jarvis-client \\\n",
    "    $JARVIS_CLIENT_CONTAINER sleep 1h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, executing the client python script in the above client container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec jarvis-client python3 /jarvis/ner_client.py --query \"NVIDIA is located at Santa Clara\" --server localhost:50051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to close the jarvis-client docker container before shutting down the jupyter kernel. In addition, do close the jarvis services container with the `jarvis_stop.sh` script in the quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker stop jarvis-client\n",
    "! cd $JARVIS_DIR && ./jarvis_stops.sh config.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
