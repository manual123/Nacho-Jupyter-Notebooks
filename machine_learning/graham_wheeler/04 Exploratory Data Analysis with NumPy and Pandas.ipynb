{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with NumPy and Pandas\n",
    "\n",
    "This is an introduction to the NumPy and Pandas libraries that form the foundation of data science in Python. These libraries, especially Pandas, have a large API surface and many powerful features. There is no way in a short amount of time to cover every topic; in many cases we will just scratch the surface. But after this you should understand the basics, have an idea of the overall scope, and have some pointers for extending your learning as you need more functionality.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We'll start by importing the numpy and pandas packages. Note the \"as\" aliases; it is conventional to use \"np\" for numpy and \"pd\" for pandas. If you are using Anaconda Python distribution, as recommended for data science, these packages should already be available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do some plotting with the matplotlib and Seaborn packages. We want the plots to appear as cell outputs inline in Jupyter.\n",
    "To do that we need to run this next line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the Seaborn library for better styled charts, and it may not yet be installed. To install it, if you are running at the command line and using Anaconda, use:\n",
    "\n",
    "    conda config --add channels conda-forge\n",
    "    conda install seaborn\n",
    "\n",
    "Else use `pip`:\n",
    "\n",
    "    pip install seaborn\n",
    "\n",
    "If you are running this in Jupyter from an Anaconda installation, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.executable is the path to the Python executable; e.g. /usr/bin/python\n",
    "import sys\n",
    "!conda config --add channels conda-forge\n",
    "!conda install --yes --prefix {sys.prefix} seaborn\n",
    "!conda install --yes --prefix {sys.prefix} xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import the plotting packages. We're also going to change the default style for matplotlib plots to use Seaborn's styling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Call sns.set() to change the default styles for matplotlib to use Seaborn styles.\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy - the Foundation of Data Science in Python\n",
    "\n",
    "Data science is largely about the manipulation of (often large) collections of numbers. To support effective data science a language needs a way to do this efficiently. Python lists are suboptimal because they are heterogeneous collections of object references; the objects in turn have reference counts for garbage collection, type info, size info, and the actual data. Thus storing (say) a list of a four 32-bit integers, rather than requiring just 16 bytes requires much more. Furthermore there is typically poor locality of the items referenced from the list, leading to cache misses and other performance problems. Python does offer an `array` type which is homogeneous and improves on lists as far as storage goes, but it offers limited operations on that data.\n",
    "\n",
    "NumPy bridges the gap, offering both efficient storage of homogeneous data in single or multi-dimensional arrays, and a rich set of computationally -efficient operations on that data.\n",
    "\n",
    "In this section we will cover some of the basics of NumPy. We won't go into too much detail as our main focus will be Pandas, a library built on top of NumPy that is particularly well-suited to manipulating tabular data. You can get a deeper intro to NumPy here: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-dimensional NumPy array from a range\n",
    "# arange returns evenly spaced values within a given interval, given a starting point (included)\n",
    "# and stopping point (excluded), and an optional step size and data type.\n",
    "\n",
    "a = np.arange(1, 11)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-dimensional NumPy array from a range with a specified increment\n",
    "a = np.arange(0.5, 10.5, 0.5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the array into a 4x5 matrix\n",
    "a = a.reshape(4, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape and # of elements. shape() gives the number of rows (axis 0)\n",
    "# and the number of columns (axis 1). These axis indices are important later.\n",
    "print(np.shape(a))\n",
    "print(np.size(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one dimensional NumPy array from a list\n",
    "a = np.array([1, 2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a value\n",
    "b = a\n",
    "a = np.append(a, 4)  # Note that this makes a copy; the original array is not affected\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index and slice\n",
    "print(f'Second element of a is {a[1]}')\n",
    "print(f'Last element of a is {a[-1]}')\n",
    "print(f'Middle two elements of a are {a[1:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of zeros of length n\n",
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of 1s\n",
    "np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of 10 random integers between 1 and 100\n",
    "np.random.randint(1,100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linearly spaced array of 5 values from 0 to 100\n",
    "np.linspace(0, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2-D array from a list of lists\n",
    "b = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape, # of elements, and # of dimensions\n",
    "print(np.shape(b))\n",
    "print(np.size(b))\n",
    "print(np.ndim(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row of b; these are equivalent\n",
    "print(b[0]) \n",
    "print(b[0,:])  # First row, \"all columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first column of b\n",
    "print(b[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subsection of b, from 1,1 through 2,2 (i.e. before 3,3)\n",
    "print(b[1:3,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy supports Boolean operations on arrays and using arrays of Boolean values to select elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an array of Booleans based on whether entries are odd or even numbers\n",
    "b%2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Boolean indexing to set all even values to -1\n",
    "b[b%2 == 0] = -1\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UFuncs\n",
    "\n",
    "NumPy supports highly efficient low-level operations on arrays called UFuncs (Universal Functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(b)  # Get the mean of all the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(b, 2)  # Raise every element to second power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the details on UFuncs here: https://docs.scipy.org/doc/numpy-1.13.0/reference/ufuncs.html\n",
    "\n",
    "### Dates and Times in NumPy\n",
    "\n",
    "NumPy uses 64-bit integers to represent datetimes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array('2015-12-25', dtype=np.datetime64)  # We use an array just so Jupyter will show us the type details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the \"[D]\" after the type. NumPy is flexible in how the 64-bits are allocated between date and time components. Because we specified a date only, it assumes the granularity is days, which is what the \"D\" means. There are a number of other possible units; the most useful are:\n",
    "\n",
    "| Y\t| Years |\n",
    "|---|-------|\n",
    "| M\t| Months |\n",
    "| W\t| Weeks |\n",
    "| D\t| Days |\n",
    "| h\t| Hours |\n",
    "| m\t| Minutes |\n",
    "| s\t| Seconds |\n",
    "| ms | Milliseconds |\n",
    "| us | Microsecond |\n",
    "\n",
    "Obviously the finer the granularity the more bits are assigned to fractional seconds leaving less for years so the range dates we can represent shrinks. The values are signed integers; in most cases 0 would be 0AD but for some very fine granularity units 0 is Jan 1, 1970 (e.g. \"as\" is attoseconds and the range here is less than 10 seconds either side of the start of 1970!). \n",
    "\n",
    "There is also a default \"ns\" format suitable for most uses.\n",
    "\n",
    "When constructing a NumPy datetime the units can be specified explicitly or inferred based on the initialization value's format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(np.datetime64('2015-12-25 12:00:00.00'))  # default to ms as that's the granularity in the datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(np.datetime64('2015-12-25 12:00:00.00', 'us'))  # use microseconds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy's date parsing is very limited and for the most part we will use Pandas datetime types that we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "NumPy is primarily aimed at scientific computation e.g. linear algebra. As such, 2D data is in the form of arrays of arrays. In data science applications, we are more often dealing with tabular data; that is, collections of records (samples, observations) where each record may be heterogeneous but the schema is consistent from record to record. The Pandas library is built on top of NumPy to provide this type of representation of data, along with the types of operations more typical in data science applications, like indexing, filtering and aggregation. There are two primary classes it provides for this, Series and DataFrame. \n",
    "\n",
    "### Pandas Series\n",
    "\n",
    "A Pandas Series is a one-dimensional array of indexed data. It wraps a sequence of values (a NumPy array) and a sequence of indices (a `pd.Index` object), along with a name. Pandas indexes can be thought of as immutable dictionaries mapping keys to locations/offsets in the value array; the dictionary implementation is very efficient and there are specialized versions for each type of index (int, float, etc).\n",
    "\n",
    "> For those interested, the underlying implementation used for indexes in Pandas is klib: https://github.com/attractivechaos/klib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = pd.Series([1, 4, 9, 16, 25])\n",
    "print(squares.name)\n",
    "squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above you can see that by default, a series will have numeric indices assigned, as a sequential list starting from 0, much like a typical Python list or array. The default name for the series is `None`, and the type of the data is `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can show the first few lines with `.head()`. The argument, if omitted, defaults to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data need not be numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([\"quick\", \"brown\", \"fox\"], name=\"Fox\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have assigned a name to the series, and note that the data type is now `object`. Think of Pandas `object` as being strings/text and/or `None` rather than generic Python objects; this is the predominant usage.\n",
    "\n",
    "What if we combine integers and strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, \"quick\", \"brown\", \"fox\"], name=\"Fox\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have \"missing\" values using None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([\"quick\", None, \"fox\"], name=\"Fox\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a series of type `object`, `None` can simply be included, but what if the series is numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, None, 3])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the special float value *NaN* (`np.nan`, for 'not a number') is used in this case. This is also why the series has been changed to have type float64 and not int64; floating point numbers have special reserved values to represent NaN while ints don't.\n",
    "\n",
    "Be careful with NaN; it will fail equality tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead you can use `is` or `np.isnan()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nan is np.nan)\n",
    "print(np.isnan(np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal indexing and slicing operations are available, much like Python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays have implicit integer sequence indices, but Pandas indices are explicit and need not be integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = pd.Series([1, 4, 9, 16, 25], \n",
    "                    index=['square of 1', 'square of 2', 'square of 3', 'square of 4', 'square of 5'])\n",
    "squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares['square of 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a Series is a lot like a Python dict (with additional slicing like a list). In fact, we can construct one from a Python dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({'square of 1':1, 'square of 2':4, 'square of 3':9, 'square of 4':16, 'square of 5':25})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use both a dictionary and an explicit index but be careful if the index and dictionary keys don't align completely; the explicit index takes precedence. Look at what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({\"one\": 1, \"three\": 3}, index=[\"one\", \"two\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Given the list below, create a Series that has the list as both the index and the values, and then display the first 3 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Put your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sample solution, uncomment %load and execute.\n",
    "# %load ex04-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of dict-style operations work on a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'square of 5' in squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.items()  # Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(squares.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, unlike with a Python dict, `.values` is an array attribute, not a function returning an iterable, so we use `.values`, not `.values()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add new entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares['square of 6'] = 36\n",
    "squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change existing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares['square of 6'] = -1\n",
    "squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and delete entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del squares['square of 6']\n",
    "squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration (`.__iter__`) iterates over the values in a Series, while membership testing (`.__contains__`) checks the indices. `.iteritems()` will iterate over (index, value) tuples, similar to list's `.enumerate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in squares:  # calls .__iter__()\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(16 in squares)\n",
    "print('square of 4' in squares)  # calls .__contains__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(16 in squares.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in squares.iteritems():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Operations\n",
    "\n",
    "You can iterate over a Series or Dataframe, but in many cases there are much more efficient vectorized UFuncs available; these are implemented in native code exploiting parallel processor operations and are much faster. Some examples are `.sum()`, `.median()`, `.mode()`, and `.mean()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series also behaves a lot like a list. We saw some indexing and slicing earlier. This can be done on non-numeric indexes too, but be careful: it _includes_ the final value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares['square of 2': 'square of 4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one or both of the keys are invalid, the results will be empty: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares['square of 2': 'cube of 4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Delete the row 'k' from the earlier series you created in exercise 1, then display the rows from 'f' through 'l'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run next line for a sample solution\n",
    "# %load ex04-02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to be aware of, is that the index need not be unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.Series(['alice', 'bob', 'carol'], index=['teacher', 'teacher', 'plumber'])\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we dereference a Series by a non-unique index we will get a Series, not a scalar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['plumber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['teacher']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to be very careful with non-unique indices. For example, assignment will change all the values for that index without collapsing to a single entry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['teacher'] = 'dave'\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent this you could use positional indexing, but my advice is to try to avoid using non-unique indices if at all possible. You can use the `.is_unique` property on the index to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "A DataFrame is like a dictionary where the keys are column names and the values are Series that share the same index and hold the column values. The first \"column\" is actually the shared Series index (there are some exceptions to this where the index can be multi-level and span more than one column but in most cases it is flat).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.Series(['Alice', 'Bob', 'Carol'])\n",
    "phones = pd.Series(['555-123-4567', '555-987-6543', '555-245-6789'])\n",
    "dept = pd.Series(['Marketing', 'Accounts', 'HR'])\n",
    "\n",
    "staff = pd.DataFrame({'Name': names, 'Phone': phones, 'Department': dept})  # 'Name', 'Phone', 'Department' are the column names\n",
    "staff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that the first column with values 0, 1, 2 is actually the shared index, and there are three series keyed off the three names \"Department\", \"Name\" and \"Phone\".\n",
    "\n",
    "Like `Series`, `DataFrame` has an index for rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame` also has an index for columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index operator actually selects a column in the DataFrame, while the .iloc and .loc attributes still select rows (actually, we will see in the next section that they can select a subset of the DataFrame with a row selector and column selector, but the row selector comes first so if you supply a single argument to .loc or .iloc you will select rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff['Name']  # Acts similar to dictionary; returns a column's Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.loc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a transpose of the DataFrame with the .T attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access columns like this, with dot-notation. Occasionally this breaks if there is a conflict with a UFunc name, like 'count':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.Name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add new columns. Later we'll see how to do this as a function of existing columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff['Fulltime'] = True\n",
    "staff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.describe()` to get summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.quantile()` to get quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([2, 3, 1, 4, 3, 5, 2, 6, 3])\n",
    "df.quantile(q=[0.25, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.drop()` to remove rows. This will return a copy with the modifications and leave the original untouched unless you include the argument `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.drop([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that because we didn't say inplace=True,\n",
    "# the original is unchanged\n",
    "staff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to construct a DataFrame. For example, from a Series or dictionary of Series, from a list of Python dicts, or from a 2-D NumPy array. There are also utility functions to read data from disk into a DataFrame, e.g. from a .csv file or an Excel spreadsheet. We'll cover some of these later.\n",
    "\n",
    "Many DataFrame operations take an `axis` argument which defaults to zero. This specifies whether we want to apply the operation by rows (axis=0) or by columns (axis=1).\n",
    "\n",
    "You can drop columns if you specify `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff.drop([\"Fulltime\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to remove a column in-place is to use `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del staff[\"Department\"]\n",
    "staff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the index to be some other column. If you want to save the existing index, then first add it as a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff['Number'] = staff.index\n",
    "staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can set the new index. This is a destructive\n",
    "# operation that discards the old index, which is\n",
    "# why we saved it as a new column first.\n",
    "staff = staff.set_index('Name')\n",
    "staff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can promote the index to a column and go back to a numeric index with `reset_index()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff = staff.reset_index()\n",
    "staff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Create a DataFrame from the dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n",
    "           'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n",
    "           'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n",
    "           'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code to create the DataFrame here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of all visits (the total number of visits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sample solution, uncomment %load and execute.\n",
    "# %load ex04-03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Indexing\n",
    "\n",
    "The Pandas Index type can be thought of as an immutable ordered multiset (multiset as indices need not be unique). The immutability makes it safe to share an index between multiple columns of a DataFrame. The set-like properties are useful for things like joins (a join is like an intersection between Indexes). There are dict-like properties (index by label) and list-like properties too (index by location). \n",
    "\n",
    "Indexes are complicated but understanding them is key to leveraging the power of pandas. Let's look at some example operations to get more familiar with how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create two Indexes for experimentation\n",
    "\n",
    "i1 = pd.Index([1, 3, 5, 7, 9])\n",
    "i2 = pd.Index([2, 3, 5, 7, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can index like a list with `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also slice like a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal Python bitwise operators have set-like behavior on indices; this is very useful when comparing two dataframes that have similar indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 & i2  # Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 | i2  # Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 ^ i2  # Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series and DataFrames have an explicit Index but they also have an implicit index like a list. When using the `[]` operator, the type of the argument will determine which index is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1, 2], index=[\"1\", \"2\"])\n",
    "print(s[\"1\"])  # matches index type; use explicit\n",
    "print(s[1])  # integer doesn't match index type; use implicit positional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the explicit Index uses integer values things can get confusing. In such cases it is good to make your intent explicit; there are attributes for this:\n",
    "\n",
    "- `.loc` references the explicit Index\n",
    "- `.iloc` references the implicit Index; i.e. a positional index 0, 1, 2,...\n",
    "\n",
    "The Python way is \"explicit is better than implicit\" so when indexing/slicing it is better to use these. The example below illustrates the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: explicit index starts at 1; implicit index starts at 0\n",
    "nums = pd.Series(['first', 'second', 'third', 'fourth'], index=[1, 2, 3, 4]) \n",
    "\n",
    "print(f'Item at explicit index 1 is {nums.loc[1]}')\n",
    "print(f'Item at implicit index 1 is {nums.iloc[1]}')\n",
    "print(nums.loc[1:3])\n",
    "print(nums.iloc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `.iloc`, the expression in `[]` can be:\n",
    "\n",
    "* an integer, a list of integers, or a slice object (e.g. `1:7`)\n",
    "* a Boolean array (see Filtering section below for why this is very useful)\n",
    "* a function with one argument (the calling object) that returns one of the above\n",
    "\n",
    "Selecting outside of the bounds of the object will raise an IndexError except when using slicing.\n",
    "\n",
    "When using `.loc`, the expression in `[]` can be:\n",
    "\n",
    "* an label, a list of labels, or a slice object with labels (e.g. `'a':'f'`; unlike normal slices the stop label is included in the slice)\n",
    "* a Boolean array\n",
    "* a function with one argument (the calling object) that returns one of the above\n",
    "\n",
    "You can use one or two dimensions in `[]` after `.loc` or `.iloc` depending on whether you want to select a subset of rows, columns, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `set_index` method to change the index of a DataFrame.\n",
    "\n",
    "If you want to change entries in a DataFrame selectively to some other value, you can use assignment with indexing, such as:\n",
    "\n",
    "```python\n",
    "df.loc[row_indexer, column_indexer] = value\n",
    "```\n",
    "\n",
    "_Don't_ use:\n",
    "\n",
    "```python\n",
    "df[row_indexer][column_indexer] = value\n",
    "```\n",
    "\n",
    "That _chained indexing_ can result in copies being made which will not have the effect you expect. You want to do all your indexing in one operation. See the details at https://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "    \n",
    " ### Exercise 4\n",
    " \n",
    " Use the same DataFrame from Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select just the 'animal' and 'age' columns from the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data in rows [3, 5, 7] and in columns ['animal', 'age'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sample solution, uncomment %load and execute.\n",
    "# %load ex04-04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading/Saving CSV, JSON and Excel Files\n",
    "\n",
    "Use `Pandas.read_csv` to read a CSV file into a dataframe. There are many optional argumemts that you can provide, for example to set or override column headers, skip initial rows, treat first row as containing column headers, specify the type of columns (Pandas will try to infer these otherwise), skip columns, and so on. The `parse_dates` argument is especially useful for specifying which columns have date fields as Pandas doesn't infer these.\n",
    "\n",
    "Full docs are at https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_csv('http://samplecsvs.s3.amazonaws.com/SacramentocrimeJanuary2006.csv',\n",
    "                    parse_dates=['cdatetime'])\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to do some preprocessing of a field during loading you can use the `converters` argument which takes a dictionary mapping the field names to functions that transform the field. E.g. if you had a string field `zip` and you wanted to take just the first 3 digits, you could use:\n",
    "\n",
    "```python\n",
    "..., converters={'zip': lambda x: x[:3]}, ...\n",
    "```\n",
    "\n",
    "If you know what types to expect for the columns, you can (and, IMO, you should) pass a dictionary in with the `types` argument that maps field names to NumPy types, to override the type inference. You can see details of NumPy scalar types here: https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html. Omit any fields that you may have already included in the `parse_dates` argument.\n",
    "\n",
    "By default the first line is expected to contain the column headers. If it doesn't you can specify them yourself, using arguments such as:\n",
    "\n",
    "```python\n",
    "..., header=None, names=['column1name','column2name'], ...\n",
    "```\n",
    "\n",
    "If the separator is not a comma, use the `sep` argument; e.g. for a TAB-separated file:\n",
    "\n",
    "```python\n",
    "..., sep='\\t', ...\n",
    "```\n",
    "\n",
    "Use `Pandas.read_excel` to load spreadsheet data. Full details here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `DataFrame.to_csv` method to save a DataFrame to a file or `DataFrame.to_excel` to save as a spreadsheet.\n",
    "\n",
    "It's also possible to read JSON data into a DataFrame. The complexity here is that JSON data is typically hierarchical; in order to turn it into a DataFrame the data typically needs to be flattened in some way. This is controlled by an `orient` parameter. For details see https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "\n",
    "You can sort a DataFrame using the `sort_values` method:\n",
    "\n",
    "```python\n",
    "DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, na_position='last')\n",
    "```\n",
    "    \n",
    "The `by` argument should be a column name or list of column names in priority order (if axis=0, i.e. we are sorting the rows, which is typically the case).\n",
    "\n",
    "See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html for the details.\n",
    "    \n",
    "\n",
    "### Filtering\n",
    "\n",
    "A Boolean expression on a Series will return a Series of Booleans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.survived == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you index a Series with a Boolean Series, you will select the items where the index is True. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic.survived == 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine these with `&` (and) and `|` (or). Pandas uses these bitwise operators because Python allows them to be overloaded while 'and' and 'or' cannot be, and in any event they arguably make sense as they are operating on Boolean series which are similar to bit vectors.\n",
    "\n",
    "As `&` and `|` have higher operator precedence than relational operators like `>` and `==`, the subexpressions we use with them need to be enclosed in parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic.survived & (titanic.sex == 'female') & (titanic.age > 50)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy itself also supports such Boolean filtering; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([3, 2, 4, 1, 5])\n",
    "s[s > np.mean(s)]  # Get the values above the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "To see if there are missing values, we can use isnull() to get a DataFrame where there are null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will show us the first few rows that had null values. If we want to know which columns may have nulls, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.any()` returns True if any are true; `.all()` returns True if all are true.\n",
    "  \n",
    "To drop rows that have missing values, use dropna(); add `inplace=True` to do it in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there are none - no-one could both be on a boat and be a recovered body, so at least one of these fields is always NaN.\n",
    "\n",
    "It may be more useful to be selective. For example, if we want to get the rows in which ticket and cabin are not null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = titanic.notnull()\n",
    "filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[filter.ticket & filter.cabin].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.count()` to get the number of entries in each column that are not null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace missing values with values of our choosing, we use `.fillna()`. With a single scalar argument it will replace all null entries in the DataFrame with that value. Usually we will want to be more granular and control which columns are affected in what ways. Let's see if there are rows with no fare specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[~filter.fare]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the fare to zero by passing a dictionary as the argument rather than a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.fillna({'fare': 0}, inplace=True)\n",
    "titanic[~filter.fare]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use a `method=\"ffill\"` argument for a forward fill or `method=\"bfill\"` argument for a backward fill; these are most useful for time series data. Yet another option is to use the `.interpolate()` method to use interpolation for the missing values; that is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Using the previous DataFrame from exercise 3, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the rows where the number of visits is greater than or equal to 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows where the age is missing, i.e. is NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows where the animal is a cat and the age is less than 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows the age is between 2 and 4 (inclusive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index to use this list:\n",
    "# idx = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the age in row 'f' to 1.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a new row 'k' to df with your choice of values for each column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then delete that row to return the original DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sample solution, uncomment %load and execute.\n",
    "# %load ex04-05.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "`pandas.concat` can be used to concatenate Series and DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(['A', 'B', 'C'])\n",
    "s2 = pd.Series(['D', 'E', 'F'])\n",
    "df = pd.concat([s1, s2])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Indexes are concatenated too, so if you are using a simple row number index you can end up with duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you don't want this behavior use the `ignore_index` argument; a new index will be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use `verify_integrity=True` to cause an exception to be raised if the result would have duplicate indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame([['A1', 'B1'],['A2', 'B2']], columns=['A', 'B'])\n",
    "d2 = pd.DataFrame([['C3', 'D3'],['C4', 'D4']], columns=['A', 'B'])\n",
    "d3 = pd.DataFrame([['B1', 'C1'],['B2', 'C2']], columns=['B', 'C'])\n",
    "pd.concat([d1, d2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join on other axis too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the columns are not completely shared, additional NaN entries will be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force concat to only include the columns that are shared with an inner join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d3], join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html for more options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and Joining\n",
    "\n",
    "We have already seen how we can add a new column to a DataFrame when it is a fixed scalar value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(['Fred', 'Alice', 'Joe'], columns=['Name'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Married'] = False\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also give an array of values provided it has the same length, or we can use a Series keyed on the index if it is not the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Phone'] = ['555-123-4567', '555-321-0000', '555-999-8765']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Department'] = pd.Series({0: 'HR', 2: 'Marketing'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to join two DataFrames instead. Pandas has a `merge` function that supports one-to-one, many-to-one and many-to-many joins. merge will look for matching column names between the inputs and use this as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame({'city': ['Seattle', 'Boston', 'New York'], 'population': [704352, 673184, 8537673]})\n",
    "d2 = pd.DataFrame({'city': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]})\n",
    "pd.merge(d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explicitly specify the column to join on; this is equivalent to the above example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(d1, d2, on='city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is more than one column in common, only items where the column values match in all cases will be included. Let's add a common column `x` and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d10 = pd.DataFrame({'city': ['Seattle', 'Boston', 'New York'], \n",
    "                    'x': ['a', 'b', 'c'], \n",
    "                    'population': [704352, 673184, 8537673]})\n",
    "d11 = pd.DataFrame({'city': ['Boston', 'New York', 'Seattle'], \n",
    "                    'x': ['a', 'c', 'b'], \n",
    "                    'area': [48.42, 468.48, 142.5]})\n",
    "pd.merge(d10, d11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Pandas avoided ambiguous cases by just dropping them.\n",
    "\n",
    "However, if we specify the column for the join, Pandas will just treat the other common columns (if any) as distinct, and add suffixes to disambiguate the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(d10, d11, on='city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the column names to join on don't match you can specify the names to use explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.DataFrame({'place': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]})\n",
    "pd.merge(d1, d3, left_on='city', right_on='place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to drop the redundant column:\n",
    "pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` joins on arbitrary columns; if you want to join on the index you can use `left_index` and `right_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(list('ABC'), columns=['c1'])\n",
    "df2 = pd.DataFrame(list('DEF'), columns=['c2'])\n",
    "pd.merge(df1, df2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a utility method on DataFrame, `join`, to do the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` can take a `how` argument that can be `inner` (intersection), `outer` (union), `left` (first augmented by second) or `right` (second augmented by first)  to control the type of join. `inner` joins are the default.\n",
    "\n",
    "If there are other columns with the same name between the two DataFrames, Pandas will give them unique names by appending `_x` to the columns from the first argument and `_y` to the columns from the second argument.\n",
    "\n",
    "It's also possible to use lists of column names for the `left_on` and `right_on` arguments to join on multiple columns.\n",
    "\n",
    "For more info on merging see https://pandas.pydata.org/pandas-docs/stable/merging.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data\n",
    "\n",
    "There are some more useful ways to explore the data in our DataFrame. Let's return to the Titanic data set, but this time we will use the sample dataset that comes with Seaborn, which is a bit different to the one we loaded before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `.unique()` to see the full set of distinct values in a series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.deck.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.value_counts()` will get the counts of the unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.deck.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.describe()` will give summary statistics on a DataFrame. We first drop rows with NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating, Pivot Tables, and Multi-indexes\n",
    "\n",
    "There is a common set of operations known as the *split-apply-combine* pattern:\n",
    "\n",
    "- _split_ the data into groups based on some criteria (this is a `GROUP BY` in SQL, or `groupby` in Pandas)\n",
    "- _apply_ some aggregate function on the groups, such as finding the mean for of some column for each group\n",
    "- _combining_ the results into a new table (Dataframe)\n",
    "\n",
    "Let's look at some examples. We can see the survival rates by gender by grouping by gender, and aggegating the survival feature using `.mean()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('sex')['survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly it's interesting to see the survival rate by passenger class; we'll still group by gender as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['sex', 'class'])['survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we grouped by two columns, the DataFrame result this time is a hierarchical table; an example of a *multi-indexed* DataFrame (indexed by both 'sex' and 'class'). We're mostly going to ignore those in this notebook - you can read about them [here](https://jakevdp.github.io/PythonDataScienceHandbook/03.05-hierarchical-indexing.html) - but it is worth noting that Pandas has an `unstack` method that can turn a mutiply-indexed DataFrame back into a conventionally-indexed one. Each call to `unstack` will flatten out one level of a multi-index hierarchy (starting at the innermost, by default, although you can control this). There is also a `stack` method that does the opposite. Let's repeat the above but unstack the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['sex', 'class'])['survived'].mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recognize the result as a *pivot* of the hierachical table. Pandas has a convenience method `pivot_table` to do all of the above in one go. It can take an `aggfunc` argument to specify how to aggregate the results; the default is to find the mean which is just what we want so we can omit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index='sex', columns='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have pivoted the other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index='class', columns='sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted counts instead, we could use Numpy's `sum` function to aggregate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index='sex', columns='class', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see more about what aggregation functions are available [here](https://jakevdp.github.io/PythonDataScienceHandbook/02.04-computation-on-arrays-aggregates.html). Let's break things down further by age group (under 18 or over 18). To do this we will create a new series with the age range of each observation, using the `cut` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.cut(titanic['age'], [0, 18, 100])  # Assume no-one is over 100\n",
    "age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our pivot table using the age series as one of the indices! Pretty cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index=['sex', age], columns='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Functions\n",
    "\n",
    "We saw earlier that we can add new columns to a DataFrame easily. The new column can be a function of an existing column. For example, we could add an 'is_adult' field to the Titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['is_adult'] = titanic.age >= 18\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a simple case; we can do more complex row-by-row applications of arbitrary functions; here's the same change done differently (this would be much less efficient but may be the only option if the function is complex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['is_adult'] = titanic.apply(lambda row: row['age'] >= 18, axis=1)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Use the same DataFrame from exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean age for each different type of animal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each type of animal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data first by the values in the 'age' column in decending order,\n",
    "# then by the value in the 'visits' column in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the 'animal' column, change the 'snake' entries to 'python'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'priority' column contains the values 'yes' and 'no'. Replace this column with a column of boolean values: \n",
    "#'yes' should be True and 'no' should be False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sample solution, uncomment %load and execute.\n",
    "# %load ex04-06.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Operations\n",
    "\n",
    "Pandas has [vectorized string operations](https://pandas.pydata.org/pandas-docs/stable/text.html) that will skip over missing values. Looks look at some examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the more detailed Titanic data set\n",
    "titanic3 = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "titanic3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper-case the home.dest field\n",
    "titanic3['home.dest'].str.upper().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the field up into two\n",
    "place_df = titanic3['home.dest'].str.split('/', expand=True)  # Expands the split list into DF columns\n",
    "place_df.columns = ['home', 'dest', '']  # For some reason there is a third column\n",
    "titanic3['home'] = place_df['home']\n",
    "titanic3['dest'] = place_df['dest']\n",
    "titanic3 = titanic3.drop(['home.dest'], axis=1)\n",
    "titanic3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal and Categorical Data\n",
    "\n",
    "So far we have mostly seen numeric, date-related, and \"object\" or string data. When loading data, Pandas will try to infer if it is numeric, but fall back to string/object. Loading functions like `read_csv` take arguments that can let us explicitly tell Pandas what the type of a column is, or whether it should try to parse the values in a column as dates. However, there are other types that are common where Pandas will need more help.\n",
    "\n",
    "*Categorical* data is data where the values fall into a finite set of non-numeric values. Examples could be month names, department names, or occupations. It's possible to represent these as strings but generally much more space and time efficient to map the values to some more compact underlying representation. *Ordinal* data is categorical data where the values are also ordered; for example, exam grades like 'A', 'B', 'C', etc, or statements of preference ('Dislike', 'Neutral', 'Like'). In terms of use, the main difference is that it is valid to compare categorical data for equality only, while for ordinal values sorting, or comparing with relational operators like '>', is meaningful (of course in practice we often sort categorical values alphabetically, but that is mostly a convenience and doesn't usually imply relative importance or weight). It's useful to think of ordinal and categorical data as being similar to enumerations in programming languages that support these. \n",
    "\n",
    "Let's look at some examples. We will use a dataset with automobile data from the [UCI Machine Learning Repository](http://mlr.cs.umass.edu/ml/index.html). This data has '?' for missing values so we need to specify that to get the right conversion. It's also missing a header line so we need to supply names for the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos = pd.read_csv(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/autos/imports-85.data\", na_values='?', \n",
    "                    header=None, names=[\n",
    "                        \"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "                        \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
    "                        \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "                        \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "                        \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
    "                        \"city_mpg\", \"highway_mpg\", \"price\"\n",
    "                    ])\n",
    "autos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some obvious examples here for categorical types; for example `make`, `body_style`, `drive_wheels`, and `engine_location`. There are also some numeric columns that have been represented as words. Let's fix those first. First we should see what possible values they can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos['num_cylinders'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos['num_doors'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the `nan` values for `num_doors`; four seems a reasonable default for the number of doors of a car:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos = autos.fillna({\"num_doors\": \"four\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert these to numbers we need to way to map from the number name to its value. We can use a dictionary for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = {\"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6, \"eight\": 8, \"twelve\": 12}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `replace` method to transform the values using the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos = autos.replace({\"num_doors\": numbers, \"num_cylinders\": numbers})\n",
    "autos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's return to the categorical columns. We can use `astype` to convert the type, and we want to use the type `category`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos[\"make\"] = autos[\"make\"].astype('category')\n",
    "autos[\"fuel_type\"] = autos[\"fuel_type\"].astype('category')\n",
    "autos[\"aspiration\"] = autos[\"aspiration\"].astype('category')\n",
    "autos[\"body_style\"] = autos[\"body_style\"].astype('category')\n",
    "autos[\"drive_wheels\"] = autos[\"drive_wheels\"].astype('category')\n",
    "autos[\"engine_location\"] = autos[\"engine_location\"].astype('category')\n",
    "autos[\"engine_type\"] = autos[\"engine_type\"].astype('category')\n",
    "autos[\"fuel_system\"] = autos[\"fuel_system\"].astype('category')\n",
    "autos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood now each of these columns has been turned into a type similar to an enumeration. We can use the `.cat` attribute to access some of the details. For example, to see the numeric value's now associated with each row for the `make` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos['make'].cat.codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos['make'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos['fuel_type'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to change the categories, assign new categories, remove category values, order or re-order the category values, and more; you can see more info at http://pandas.pydata.org/pandas-docs/stable/categorical.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having an underlying numerical representation is important; most machine learning algorithms require numeric features and can't deal with strings or categorical symbolic values directly. For ordinal types we can usually just use the numeric encoding we have generated above, but with non-ordinal data we need to be careful; we shouldn't be attributing weight to the underlying numeric values. Instead, for non-ordinal values, the typical approach is to use *one-hot encoding* - create a new column for each distinct value, and just use 0 or 1 in each of these columns to indicate if the observation is in that category. Let's take a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheels = autos[['make', 'drive_wheels']]\n",
    "wheels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_dummies` method will 1-hot encode a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = pd.get_dummies(wheels['drive_wheels']).head()\n",
    "onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge this into a dataframe with the `make`, we can merge with the `wheels` dataframe on the implicit index field, and then drop the original categorical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheels.merge(onehot, left_index=True, right_index=True).drop('drive_wheels', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligned Operations\n",
    "\n",
    "Pandas will align DataFrames on indexes when performing operations. Consider for example two DataFrames, one with number of transactions by day of week, and one with number of customers by day of week, and say we want to know average transactions per customer by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.DataFrame([2, 4, 5],\n",
    "                           index=['Mon', 'Wed', 'Thu'])\n",
    "customers = pd.DataFrame([2, 2, 3, 2], \n",
    "                        index=['Sat', 'Mon', 'Tue', 'Thu'])\n",
    "\n",
    "transactions / customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how pandas aligned on index to produce the result, and used NaN for mismatched entries. We could specify the value to use as operands by using the `div` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.div(customers, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Methods and .pipe()\n",
    "\n",
    "Many operations on Series and DataFrames return modified copies of the Series or Dataframe, unless the `inplace=True` argument is included. Even in that case there is usually a copy made and then the reference is just replaced at the end, so using inplace operations generally isn't faster. Because a Series or Dataframe reference is returned, you can chain multiple operations, for example:\n",
    "\n",
    "```python\n",
    "df = (pd.read_csv('data.csv')\n",
    "        .rename(columns=str.lower)\n",
    "        .drop('id', axis=1))\n",
    "```\n",
    "\n",
    "This is great for built-in operations, but what about custom operations? The good news is these are possible too, with `.pipe()`, which will allow you to specify your own functions to call as part of the operation chain:\n",
    "\n",
    "```python\n",
    "def my_operation(df, *args, **kwargs):\n",
    "    # Do something to the df\n",
    "    ...\n",
    "    # Return the modified dataframe\n",
    "    return df\n",
    "   \n",
    "# Now we can call this in our chain.\n",
    "df = (pd.read_csv('data.csv')\n",
    "        .rename(columns=str.lower)\n",
    "        .drop('id', axis=1)\n",
    "        .pipe(my_operation, 'foo', bar=True))    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance and Hypothesis Testing\n",
    "\n",
    "In exploring the data, we may come up with hypotheses about relationships between different values. We can get an indication of whether our hypothesis is correct or the relationship is coincidental using [tests of statistical significance](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing).\n",
    "\n",
    "We may have a simple hypothesis, like \"All X are Y\". For phenomena in the real world, we usually we can't explore all possible X, and so we can't usually prove all X are Y. To prove the opposite, on the other hand, only requires a single counter-example. The well-known illustration is the *black swan*: to prove that all swans are white you would have to find every swan that exists (and possibly that has ever existed and may ever exist) and check its color, but to prove not all swans are white you need to find just a single swan that is not white and you can stop there. For these kinds of hypotheses we can often just look at our historical data and try to find a counterexample.\n",
    "\n",
    "For most of the phenomena we deal with, however, things are not so simple; we are usually dealing with complex combinations of observable variables rather than just absolute classifications. For example, we may be A/B testing some change on a website, and we want to know if the change we are testing causes an improvement in some metric like conversion. Conversion happens in both cases, so this is a a more nuanced problem than the black swan problem. \n",
    "\n",
    "Let's say that the conversion is better in the test set. How do we know that the change caused the improvement, and it wasn't just by chance? One way is to combine the observations from both the test and control set, then take random samples, and see what the probability is of a sample showing a similar improvement in conversion. If the probability of a similar improvement from a random sample is very low, then we can conclude that the improvement from the change is statistically significant.\n",
    "\n",
    "In practice we may have a large number of observations in the test set and a large number of observations in the control set, and the approach outlined above may be computationally too costly. There are various tests that can give us similar measures at a much lower cost, such as the [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) (when comparing means of populations) or the [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test) (when comparing categorical data). The details of how these tests work tests and which ones to choose are beyond the scope of this notebook, but to begin with you can use this guide:\n",
    "\n",
    "| Comparison Type | Numeric Data | Categorical Data |\n",
    "|-----------------|--------------|------------------|\n",
    "| Sample vs Known Value | 1-Sample t-test | Binomial test |\n",
    "| Sample vs Sample | 2-Sample t-test | Chi-test |\n",
    "| 3 or more Sample comparison | ANOVA or Tukey | Chi-test |\n",
    "\n",
    "The usual approach is to assume the opposite of what we want to prove; this is called the [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis) or $H_0$. For our example, the null hypothesis states there is no relationship between our change and conversion on the website. In many cases the null hypothesis is similar; i.e. it is the hypothesis that any change between the samples is just the result of chance. \n",
    "\n",
    "We then calculate the probability that the data supports the null hypothesis rather than just being the result of unrelated variance: this is called the *p-value*. In general, a p-value of less than 0.05 (5%) is taken to mean that the hypothesis is valid, although this has recently become a [contentious point](http://www.sciencemag.org/news/2017/07/it-will-be-much-harder-call-new-findings-significant-if-team-gets-its-way). We'll set aside that debate for now and stick with 0.05.\n",
    "\n",
    "Let's revisit the Titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how gender affected survival rates, one way is with *cross-tabulation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(titanic['survived'],titanic['sex'])\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a lot more men than women on the ship, and it certainly looks like the survival rate for women was better than for men, but is the difference statistically significant? Our  hypothesis is that gender affects survivability, and so the null hypothesis is that it doesn't. Let's measure this with a chi-squared test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "chi2, p, dof, expected = stats.chi2_contingency(ct.values)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a very small p-value! So it looks like gender was an issue.\n",
    "\n",
    "Something we have ignored here is sample size. Generally when running an experiment we need to know how long to run it for. To do that, we need to know the size of the difference we are looking for, and the p-value we will accept; from this we can get an idea of how many samples we need, and from that we should be able to compute the duration we need for the experiment. There are online calculators to help with this; see for example [here](https://www.optimizely.com/sample-size-calculator/).\n",
    "\n",
    "When running an online experiment, check the p-value periodically and plot the trend. You want to see the p-value gradually converging. If instead it is erratic and showing no sign of conversion, that suggests the experiment is not going to be conclusive.\n",
    "\n",
    "One last comment: it is often said \"correlation does not imply causation\". We should be more precise and less flippant than this! If X and Y are correlated, there are only four possibilities:\n",
    "\n",
    "- this was a chance event. We can determine the probability of that and assess if its a reasonable explanation.\n",
    "- X causes Y or Y causes X\n",
    "- X and Y are both caused by some unknown factor Z. This is usually what we are referring to when saying \"correlation does not imply causation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Pandas includes the ability to do simple plots. For a Series, this typically means plotting the values in the series as the Y values, and then index as the X values; for a DataFrame this would be a multiplot. You can use `x` and `y` named arguments to select specific columns to plot, and you can use a `kind` argument to specify the type of plot.\n",
    "\n",
    "See https://pandas.pydata.org/pandas-docs/stable/visualization.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([2, 3, 1, 5, 3], index=['a', 'b', 'c', 'd', 'e'])\n",
    "s.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [2, 1],\n",
    "        [4, 4],\n",
    "        [1, 2],\n",
    "        [3, 6]\n",
    "    ],\n",
    "    index=['a', 'b', 'c', 'd'],\n",
    "    columns=['s1', 's2']\n",
    ")\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='s1', y='s2', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting with Seaborn\n",
    "\n",
    "See the [Python Graph Gallery](https://python-graph-gallery.com/) for many examples of different types of charts including the code used to create them. As you learn to use the plotting libraries in many cases the fastest way to get results is just find an example from there and copy/paste/edit it.\n",
    "\n",
    "There are a number of plotting libraries for Python; the most well known are matplotlib, Seaborn, Bokeh, and Plotly. Some offer more interactivity than others. Matplotlib is the most commonly used; it is very flexible but requires a fair amount of boilerplate code. There is a good tutorial on matplotlib [here](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python). We will instead use [Seaborn](https://seaborn.pydata.org/), which is built on top of matplotlib and simplifies its usage so that many plots just take one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the more detailed Titanic data set\n",
    "titanic3 = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')\n",
    "titanic3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a factorplot to count categorical data\n",
    "import seaborn as sns\n",
    "sns.factorplot('sex', data=titanic3, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's bring class in too:\n",
    "sns.factorplot('pclass', data=titanic3, hue='sex', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course we can aggregate the other way too\n",
    "sns.factorplot('sex', data=titanic3, hue='pclass', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many people were on each deck\n",
    "deck = pd.DataFrame(titanic3['cabin'].dropna().str[0])\n",
    "deck.columns = ['deck']  # Get just the deck column\n",
    "sns.factorplot('deck', data=deck, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What class passenger was on each deck?\n",
    "df = titanic3[['cabin', 'pclass']].dropna()\n",
    "df['deck'] = df.apply(lambda row: ord(row.cabin[0]) -64, axis=1)\n",
    "\n",
    "sns.regplot(x=df[\"pclass\"], y=df[\"deck\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates and Time Series\n",
    "\n",
    "Pandas provides several classes for dealing with datetimes: `Timestamp`, `Period`, and `Timedelta`, and corresponding index types based off these, namely `DatetimeIndex`, `PeriodIndex` and `TimedeltaIndex`.\n",
    "\n",
    "For parsing dates we can use `pd.to_datetime` which can parse dates in many formats, or `pd.to_timedelta` to get a time delta. For formatting dates as strings the `Timestamp.strftime` method can be used.\n",
    "\n",
    "For example, to get a four-week-long range of dates starting from Christmas 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = pd.to_datetime(\"December 25, 2017\") + pd.to_timedelta(np.arange(4*7), 'D')\n",
    "di"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to pass a list of dates to `to_datetime` to create a DatetimeIndex. A DatetimeIndex can be converted to a TimedeltaIndex by subtracting a start date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di - di[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course the converse is possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(di - di[0]) + di[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating the indices is to specify range start and ends plus optionally the granularity, via the `periods` and `freq` arguments, using the APIs `pd.date_range`, `pd.timedelta_range`, and `pd.interval_range`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-30', '2017-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-30', '2017-12-31', freq='h')  # Hourly frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-30', periods=4)  # 4 values using the default frequency of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-30', periods=4, freq='h')  # 4 values using hourly frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periods represent time intervals locked to timestamps. Consider the difference below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-01', '2017-12', freq='M')  # This gives us 12 dates, one for each month, on the last day of each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.period_range('2017-01', '2017-12', freq='M')  # This gives us 12 month long periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why the dates above were on the last day of each month. Pandas uses frequency codes, as follows:\n",
    "\n",
    "| Code | Meaning |\n",
    "|------|---------|\n",
    "| D | Calendar day |\n",
    "| B | Business day |\n",
    "| W | Weekly |\n",
    "| MS | Month start |\n",
    "| BMS | Business month start |\n",
    "| M | Month end |\n",
    "| BM | Business month end |\n",
    "| QS | Quarter start |\n",
    "| BQS | Business quarter start |\n",
    "| Q | Quarter end |\n",
    "| BQ | Business quarter end |\n",
    "| AS | Year start |\n",
    "| A | Year end |\n",
    "| BAS | Business year start |\n",
    "| BS | Business year end |\n",
    "| T\t| Minutes |\n",
    "| S\t| Seconds |\n",
    "| L\t| Milliseonds |\n",
    "| U\t| Microseconds |\n",
    "\n",
    "These can also be combined in some cases; e.g. \"!H30T\" or \"90T\" each represent 90 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-01', periods=16, freq='1H30T') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add month offsets to annual or quarterly frequencies or day of week constraints to weekly frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017', periods=4, freq='QS-FEB')  # 4 quarters starting from beginning of February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-01', periods=4, freq='W-MON')  # First 4 Mondays in Jan 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what use are all these? To understand that we need some time-series data. Let's get the eBay daily stock closing price for 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "\n",
    "ebay = data.DataReader('EBAY', start='2017', end='2018', data_source='iex')['close']\n",
    "ebay.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our index is not timestamp-based, so let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay.index = pd.to_datetime(ebay.index)\n",
    "ebay.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot just January prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay[\"2017-01\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot weekly closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay[pd.date_range('2017-01', periods=52, freq='W-FRI')].plot()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a small sample of what Pandas can do with time series; Pandas came out of financial computation and has very rich capabilities in this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Data with pandas_profiling and facets\n",
    "\n",
    "`pandas_profiling` is a Python package that can produce much more detailed summaries of data than the `.describe()` method. In this case we must install with `pip` and the right way to do this from the notebook is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas_profiling\n",
    "import seaborn as sns;\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "pandas_profiling.ProfileReport(titanic)  # You may need to run cell twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facets is a library from Google that looks very good. It has similar functionality to pandas_profiling as well as some powerful visualization. Installation is more complex so we won't use it now but it is worth considering.\n",
    "\n",
    "https://github.com/pair-code/facets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Data that Exceeds Your System's RAM\n",
    "\n",
    "Pandas is an in-memory system. The use of NumPy means it uses memory very efficiently but you are still limited by the RAM you have available. If your data is too large, there are several options available, including:\n",
    "\n",
    "- process the data sequentially (may not be possible but see [here](https://hackernoon.com/analysing-1-4-billion-rows-with-python-6cec86ca9d73) for an interesting approach)\n",
    "- partition the data into chunks and process those separately\n",
    "- partition the data into chunks and use multiple computers configured as a cluster with `ipyparallel` (https://ipyparallel.readthedocs.io/en/latest/)\n",
    "- use a DataFrame-like library that handles larger datasets, like Dask DataFrames (http://dask.pydata.org/en/latest/dataframe.html) \n",
    "- use a tool like [Apache Drill](https://drill.apache.org/) which can SQL queries against files on disk in formats like CSV \n",
    "- putting the data in a database and operating on a subset in Pandas using a SELECT statement.\n",
    "\n",
    "These are all out of scope of this document but we will briefly elaborate on the last two. Python comes standard with an implementation of Sqlite, in the package `sqlite3`. Pandas supports reading a DataFrame from the result of running a query against a Sqlite database. Here's a very simple example of how that may look:\n",
    "\n",
    "```python\n",
    "import sqlite3 as lite\n",
    "\n",
    "with lite.connect('mydata.db') as con:\n",
    "    query = 'select * from sales limit 100'\n",
    "    df = pd.read_sql(query, con)\n",
    "```\n",
    "\n",
    "You can read more about Sqlite here: https://sqlite.org/quickstart.html.\n",
    "\n",
    "Dask supports chunked dataframes that support most of the functionality of Pandas. The key additional parameter is `blocksize` which specifies the maximum size of a chunk of data to read into memory at one time. In addition, Dask methods are lazily evaluated; you must explicitly call a `.compute()` method to kick off the calculation. Here is a simple example: assume we have multiple CSV files containing temperature measurements. We could compute the mean temperature with something like:\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('temp*.csv', blocksize=25e6)  # Use 25MB chunks\n",
    "df.temperature.mean().compute()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Interactivity with ipywidgets\n",
    "\n",
    "`ipywidgets` is an extension package for Jupyter that allows output cells to include interactive HTML elements. To install, you will need to run a command to enable the extension from a terminal and then restart Jupyter. First, install the package; the code below shows the right way to do this from within the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge --prefix {sys.prefix} --yes ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to run this command from your terminal, kill and restart JupyterLab, then return here.\n",
    "\n",
    "    jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "(You can run it from within JupyterLab but you will still need a restart before the widgets will work).\n",
    "\n",
    "We will look at a simple example using the `interact` function from `ipywidgets`. You call this giving it a function as the first argument, followed by zero or more additional arguments that can be tuples, lists or dictionaries. These arguments will each become interactive controls like sliders and drop-downs, and any change in their values will cause the function to be called again with the new values as arguments.\n",
    "\n",
    "See http://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html for more info on creating other types of controls when using `interact`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([[2, 1], [4, 4], [1, 2], [3, 6]], index=['a', 'b', 'c', 'd'], columns=['s1', 's2'])\n",
    "\n",
    "\n",
    "def plot_graph(kind, col):\n",
    "    what = df if col == 'all' else df[col]\n",
    "    what.plot(kind=kind)\n",
    "\n",
    "\n",
    "interact(plot_graph, kind=['line', 'bar'], col=['all', 's1', 's2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Packages and Resources\n",
    "\n",
    "- `openpyxl` allows you to create and work directly with Excel spreadsheets\n",
    "- `faker` can create fake data like names, addresses, credit card numbers, and social security numbers\n",
    "- `numba` includes a `@jit` decorator that can [speed up the execution of many functions](http://nbviewer.jupyter.org/github/akittas/presentations/blob/master/pythess/numba/numba.ipynb); useful when crunching data outside of Pandas (it won't speed up Pandas code)\n",
    "- `moviepy` allows you to edit video frame-by-frame (or even create video)\n",
    "- [ray](https://rise.cs.berkeley.edu/blog/pandas-on-ray/) is a new package that lets you leverage your GPU to speed up pandas code\n",
    "- [qgrid](https://github.com/quantopian/qgrid) is a Jupyter extension that adds interactive sorting, filtering and editing of DataFrames\n",
    "\n",
    "Video tutorials on Pandas: http://www.dataschool.io/easier-data-analysis-with-pandas/\n",
    "\n",
    "Jake VanderPlas' excellent Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "\n",
    "Tom Augspurger has a great [multi-part series](http://tomaugspurger.github.io/modern-1-intro) on Pandas aimed at intermediate to advanced users. \n",
    "\n",
    "## Example: Loading JSON into a DataFrame and Expanding Complex Fields\n",
    "\n",
    "In this example we'll see how we can load some structured data and process it into a flat table form better suited to machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some data; top stories from lobste.rs; populate a DataFrame with the JSON\n",
    "stories = pd.read_json('https://lobste.rs/hottest.json')\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"short_id' field as the index\n",
    "stories = stories.set_index('short_id')\n",
    "\n",
    "# Show the first few rows\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the submitter_user field; it is a dictionary itself.\n",
    "stories.submitter_user[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to expand these fields into our dataframe. First expand into its own dataframe.\n",
    "user_df = stories.submitter_user.apply(pd.Series)\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should make sure there are no collisions in column names.\n",
    "set(user_df.columns).intersection(stories.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can rename the column to avoid the clash\n",
    "user_df = user_df.rename(columns={'created_at': 'user_created_at'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine them, dropping the original compound column that we are expanding.\n",
    "stories = pd.concat([stories.drop(['submitter_user'], axis=1), user_df], axis=1)\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tags field is another compound field.\n",
    "stories.tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe with the tag lists expanded into columns of Series.\n",
    "tag_df = stories.tags.apply(pd.Series)\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame\n",
    "tag_df = tag_df.stack()\n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand into a 1-hot encoding\n",
    "tag_df = pd.get_dummies(tag_df)\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge multiple rows\n",
    "tag_df = tag_df.sum(level=0)\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And add back to the original dataframe\n",
    "stories = pd.concat([stories.drop('tags', axis=1), tag_df], axis=1)\n",
    "stories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Baby Names\n",
    "\n",
    "The data comes from US census and is the count of names of children born in years from 1880 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babynames = pd.read_csv('NationalNames.csv.zip', compression='zip')  # Pandas can unzip the data for you\n",
    "babynames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: show the baby names from 1918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: get the counts per year for the name 'John'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: do the same but restrict to boys now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: plot popularity of John as a boy's name per year\n",
    "# (hint: look at help for Seaborn barplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
