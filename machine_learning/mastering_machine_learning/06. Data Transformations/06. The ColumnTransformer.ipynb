{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ColumnTransformer\n",
    "\n",
    "All scikit-learn transformers apply their transformation to all of the columns. If you desire to apply different transformations to different columns, you'll need to use the `ColumnTransformer` meta-estimator. Let's begin by selecting continuous, nominal, and ordinal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "housing = pd.read_csv('../data/housing_sample.csv')\n",
    "X = housing[['Neighborhood', 'Exterior1st', 'GrLivArea', 'GarageArea', 'HeatingQC']]\n",
    "y = housing['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of data needing different transformations\n",
    "\n",
    "Our input data contains continuous, nominal, and ordinal data, each needing their own transformation. As we saw in the last chapter, passing in the above five-column dataset to a transformer such as an instance of `OneHotEncoder`, transformed each column, including those that we did not want to get transformed. In order to apply different transformations to different columns of data, you'll need to use the `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of 3-item tuples - name, transformer, columns\n",
    "\n",
    "\n",
    "The `ColumnTransformer` requires that you instantiate it with a list of 3-item tuples. The first value of the tuple is a string called the **name**. This will be used if you refer to the transformer during a grid search. The second value of the tuple is the actual **transformer**. In this example, we will be doing one-hot encoding. The last value in the tuple is the list of **columns** to apply the transformation to. A separate three-item tuple will be created for each group of columns needing to be transformed.\n",
    "\n",
    "Let's begin by only transforming the nominal features `Neighborhood` and `Exterior1st`. Since we just have one transformation group, we'll create a list containing a single three-item tuple. The **name** of this transformation group is 'nom' (short for nominal). The **transformer** is the `OneHotEncoder` instance `ohe`.  The **columns** are a list of the two column names to be transformed (`nom_cols`). Our three-item tuple for this transformation group is `('nom', ohe, nom_cols)`.\n",
    "\n",
    "After the `OneHotEncoder` transformer is instantiated  and the list of columns declared, the list of three-item tuples needed for the `ColumnTransformer` is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "nom_cols = ['Neighborhood', 'Exterior1st']\n",
    "transformers = [('nom', ohe, nom_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the `ColumnTransformer`\n",
    "\n",
    "After creating the list of three-item tuples, we can instantiate the `ColumnTransformer`, which is located in the `compose` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit and transform our input data and output the first row. Notice that we have a numpy array now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = ct.fit_transform(X)\n",
    "X_t[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens to the other columns?\n",
    "\n",
    "Only the columns provided in the three-item tuple get transformed. The other columns are dropped by default. Let's output the shape and verify that the number of columns are equal to the number of unique values in those two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['Neighborhood', 'Exterior1st']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new feature names\n",
    "\n",
    "Use the `get_feature_names` to get the column names of the transformed array. The first 10 features are outputted below. Notice that the name of the transformer, 'nom', is prepended to the beginning and separated from the remainder of the name by two underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the remaining columns\n",
    "\n",
    "Alternatively, we can choose to keep the remaining columns unchanged in the result by setting the `remainder` parameter to 'passthrough'. We reinstantiate our `ColumnTransformer` and output the first rows of the transformed data with the non-transformed columns kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers, remainder='passthrough')\n",
    "X_t = ct.fit_transform(X)\n",
    "X_t[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three more columns than there were when we encoded just the two string columns. We can look at the last three columns to verify that they have been passed through the transformer without any transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t[:5, -3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new column names - NotImplementedError\n",
    "\n",
    "In the future, we will be able to use the `get_feature_names` method to get the column names of the transformed array when using 'passthrough'. Unfortunately, this has not been implemented yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add transformation group to scale the continuous features\n",
    "\n",
    "We can add a new transformation group that scales the continuous features. Let's extend our list of transformers by adding a new three-item tuple to it. We give it the **name** 'con' (short for continuous), use the `StandardScaler` instance `ss` **transformer**, and create a list of the continuous **columns** `GrLivArea` and `GarageArea`. There is one feature that does not appear in either of the transformations, `HeatingQC`, that we  continue to drop. We output the first row and the shape after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "nom_cols = ['Neighborhood', 'Exterior1st']\n",
    "con_cols = ['GrLivArea', 'GarageArea']\n",
    "transformers = [('nom', ohe, nom_cols), ('con', ss, con_cols)]\n",
    "ct = ColumnTransformer(transformers)\n",
    "X_t = ct.fit_transform(X)\n",
    "X_t[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transformation group for ordinal encoding\n",
    "\n",
    "Let's add one more column group for the ordinal columns. In this dataset, only `HeatingQC` is ordinal. We transform these three column groupings and output the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "order = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]\n",
    "oe = OrdinalEncoder(order)\n",
    "ord_cols = ['HeatingQC']\n",
    "transformers = [('nom', ohe, nom_cols),\n",
    "                ('con', ss, con_cols),\n",
    "                ('ord', oe, ord_cols)]\n",
    "ct = ColumnTransformer(transformers)\n",
    "X_t = ct.fit_transform(X)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ColumnTransformer` splits each of the column groupings into its own dataset, applies the particular transformation to each grouping and then combines all the columns back together again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![][0]\n",
    "\n",
    "[0]: images/columntransformer_basic.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning after transforming\n",
    "\n",
    "Now that we successfully transformed each set of columns, we can perform machine learning. To do this, we build a short pipeline where the first completes the column transformations and the second does the machine learning. In this case, we use a decision tree as our machine learning model. Remember that you must create a list of two item tuples (name, estimator) to instantiate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "dtr = DecisionTreeRegressor()\n",
    "steps = [('ct', ct), ('dtr', dtr)]\n",
    "pipe = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the last step is a machine learning model, this pipeline must be trained using the `fit` method. Calling the `fit` method passes the data into the `ColumnTransformer` which independently learns how to transform each of the three column groupings. It then transforms each column grouping returning a single array which is used for training the `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this pipeline using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "cross_val_score(pipe, X, y, cv=kf).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline within the `ColumnTransformer`\n",
    "\n",
    "Our first ColumnTransformer applied a single transformation to each set of columns. We can take this further and apply any number of transformations to a distinct set of columns by using a pipeline within the ColumnTransformer. The diagram below shows this process.\n",
    "![][1]\n",
    "\n",
    "Let's begin by creating a pipeline containing two steps (imputation and standardization) for the continuous features.\n",
    "\n",
    "[1]: images/columntransformer_pipeline.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "con_si = SimpleImputer(strategy='mean')\n",
    "con_ss = StandardScaler()\n",
    "con_steps = [('si', con_si),('ss', con_ss)]\n",
    "con_pipe = Pipeline(con_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a two-step pipeline (imputation and encoding) for the nominal and ordinal categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_si = SimpleImputer(strategy='most_frequent')\n",
    "nom_ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "nom_steps = [('si', nom_si), ('ohe', nom_ohe)]\n",
    "nom_pipe = Pipeline(nom_steps)\n",
    "\n",
    "ord_si = SimpleImputer(strategy='most_frequent')\n",
    "ord_oe = OrdinalEncoder(order)\n",
    "ord_steps = [('si', ord_si), ('oe', ord_oe)]\n",
    "ord_pipe = Pipeline(ord_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we prepare a column transformer to pass the two continuous features through the continuous pipeline, the two nominal features through the nominal pipeline, and the ordinal feature through the ordinal pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [('con', con_pipe, con_cols),\n",
    "                ('nom', nom_pipe, nom_cols),\n",
    "                ('ord', ord_pipe, ord_cols)]\n",
    "ct = ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transform all of the columns in our dataset at once. The first rows is output to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = ct.fit_transform(X)\n",
    "X_t[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a final pipeline to do machine learning\n",
    "\n",
    "Now that we can apply separate transformations to separate groups of columns, we can pass this result to a machine learning model. To connect the ColumnTransformer to the machine learning estimator, we use a pipeline. We use a random forest with 100 trees and a max depth of 5 as our model.\n",
    "\n",
    "![][1]\n",
    "\n",
    "[1]: images/columntransformer_pipeline_ml.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "steps = [('ct', ct), ('rfr', rfr)]\n",
    "pipe_final = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can get cross validated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe_final, X, y, cv=kf).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All in one cell\n",
    "\n",
    "It might be helpful to see all the steps used to setup the final pipeline absent of the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous pipeline\n",
    "con_si = SimpleImputer(strategy='mean')\n",
    "con_ss = StandardScaler()\n",
    "con_steps = [('si', con_si), ('ss', con_ss)]\n",
    "con_pipe = Pipeline(con_steps)\n",
    "\n",
    "# nominal pipeline\n",
    "nom_si = SimpleImputer(strategy='most_frequent')\n",
    "nom_ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "nom_steps = [('si', nom_si), ('ohe', nom_ohe)]\n",
    "nom_pipe = Pipeline(nom_steps)\n",
    "\n",
    "# ordinal pipeline\n",
    "ord_si = SimpleImputer(strategy='most_frequent')\n",
    "ord_oe = OrdinalEncoder(order)\n",
    "ord_steps = [('si', ord_si), ('oe', ord_oe)]\n",
    "ord_pipe = Pipeline(ord_steps)\n",
    "\n",
    "# ColumnTransformer setup\n",
    "nom_cols = ['Neighborhood', 'Exterior1st']\n",
    "con_cols = ['GrLivArea', 'GarageArea']\n",
    "ord_cols = ['HeatingQC']\n",
    "\n",
    "transformers = [('con', con_pipe, con_cols),\n",
    "                ('nom', nom_pipe, nom_cols),\n",
    "                ('ord', ord_pipe, ord_cols)]\n",
    "ct = ColumnTransformer(transformers)\n",
    "\n",
    "# Final pipeline\n",
    "rfr = RandomForestRegressor(n_estimators=100)\n",
    "steps = [('ct', ct), ('rfr', rfr)]\n",
    "pipe_final = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid searching the final pipeline\n",
    "\n",
    "Tuning the random forest within this pipeline requires us to refer to the hyperparameter by preceding it with the name of the step ('rfr') followed by two underscores.\n",
    "\n",
    "Tuning the hyperparameters of the transformers takes more work. For instance, if we want to tune the `strategy` of the `SimpleImputer` for the continuous columns, we have to traverse back through the `ColumnTransformer` ('ct'), continuous pipeline ('con'), before finally reaching the `SimpleImputer` ('si'). Below, we search three random forest hyperparameters and the strategy of continuous imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'rfr__max_depth': range(4, 10),\n",
    "        'rfr__min_samples_leaf': [10, 20],\n",
    "        'rfr__max_features': [.5, .7],\n",
    "        'ct__con__si__strategy': ['mean', 'median']}\n",
    "gs = GridSearchCV(pipe_final, grid, cv=kf, n_jobs=-1)\n",
    "gs.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Create different pipelines and grid search them to find the best parameters.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
