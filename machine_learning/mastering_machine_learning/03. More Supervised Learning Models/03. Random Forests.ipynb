{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Random forests are a very successful machine learning algorithm developed in the late 90's and early 2000's. You can read the [original notes online][1] from the creator Leo Brieman himself. A random forest is an extension of a decision tree model.\n",
    "\n",
    "## Random forests are a collection of decision trees\n",
    "\n",
    "A random forest is simply a collection of decision trees built from the same training data. The final model will be a number of decision trees that are determined by the user. To make a prediction, the random forest averages the predictions made by each individual tree.\n",
    "\n",
    "### Isn't a decision tree deterministic?\n",
    "\n",
    "Yes, all the nodes and branches created from a decision tree are deterministic, meaning that the tree will always be built the same given the same input and output data (There actually is a slight exception here. When there are multiple splits of the data that are tied for the best split, then scikit-learn randomly selects which split to take.). \n",
    "\n",
    "### A random forest adds two sources of randomness to the decision tree\n",
    "\n",
    "Each decision tree created in a random forest will be different. There are **two sources of randomness**. The first source of randomness comes from choosing different random sets of data to build the trees from. This means all the trees will be built from different observations. The datasets for each tree are created through a procedure called **bootstrapping**. Rows are sampled from the original dataset **with replacement**. Typically, the same number of rows are chosen for the random dataset. Because the data is sampled with replacement, many rows will be repeated and others will be absent.\n",
    "\n",
    "The other source of randomness comes when building the tree itself. In a normal decision tree, every single feature is searched for the best binary split. With a random forest, you can opt to select a subset of the features to search at each node for the best binary split. By default, scikit-learn searches all the features, but provides the `max_features` parameter to limit the number of features to search. Note, that this random procedure is done for each node, so a different set of features will be searched at each node.\n",
    "\n",
    "### Bagging = Bootstrap Aggregating\n",
    "\n",
    "A random forest uses bootstrapping to create new random datasets for each tree. The predictions of all the trees are aggregated to produce a final decision. This combination of bootstrapping and aggregating is called **bagging**. This procedure is not specific to random forests and may be applied to many different models.\n",
    "\n",
    "## Random Forest in Scikit-Learn\n",
    "\n",
    "Let's build a random forest on the housing dataset with three decision trees. Choose the number of trees by setting the parameter `n_estimators` during instantiation. We will keep the depth of the trees small to make visualization possible in the notebook. We can also set a limit to the number of features searched at each node with the `max_features` parameter.\n",
    "\n",
    "### Import from the `ensemble` module\n",
    "\n",
    "Random forests are considered an **ensemble** model which is a term to refer to any model that combines many other models together. Let's begin b reading in our data selecting a few of the columns to learn from.\n",
    "\n",
    "[1]: https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv('../data/housing_sample.csv')\n",
    "cols = ['GrLivArea', 'GarageArea', 'BedroomAbvGr', 'FullBath']\n",
    "X = housing[cols]\n",
    "y = housing['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `RandomForestRegressor` from the `ensemble` module and then instantiate it by setting the number of trees to 3, the max depth of 2, and search 2 features at each node for the best split. Finally, we train the model with the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=3, max_depth=2, max_features=2)\n",
    "rfr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each tree is stored in the `estimators_` attribute\n",
    "\n",
    "We can get access to each individual tree in the `estimators_` attribute. It is a list of all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize each tree\n",
    "\n",
    "Let's visualize each tree so that we can verify that each one is indeed different. A function is created to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=3, max_depth=2, max_features=2)\n",
    "rfr.fit(X, y)\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.suptitle('Visualizing each tree from a random forest', fontsize=30)\n",
    "kwargs = dict(feature_names=cols, precision=1, filled=True, rounded=True)\n",
    "for i in range(3):\n",
    "    plot_tree(rfr.estimators_[i], ax=axes[i], **kwargs)\n",
    "    axes[i].set_title(f'Tree {i + 1}', fontsize=15, pad=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "We have four features in our model. Let's predict the sale price of a house with above ground living area of 2,300, garage area of 1,000, 4 bedrooms and 2 baths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [[2300, 1000, 4, 2]]\n",
    "rfr.predict(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify prediction is average of individual trees\n",
    "\n",
    "Let's iterate over each tree to get a prediction for each one. Then let's take the average and verify that it matches the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [tree.predict(val) for tree in rfr.estimators_]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the random forest\n",
    "\n",
    "Again, we can evaluate our model with the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests build weak learners, why are they good?\n",
    "\n",
    "A random forest purposefully withholds information from each tree. Not all the observations are available to each tree and not all the features are available for each node at each decision. Each individual tree is considered a **weak learner** because of this. As it turns out, many weak learners acting independently can together make good decisions. This is sometimes referred to as **the wisdom of the crowd**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "<span  style=\"color:green; font-size:16px\">Build many different random forests with different combinations of features and different values for the number of trees and the depth of each tree.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
