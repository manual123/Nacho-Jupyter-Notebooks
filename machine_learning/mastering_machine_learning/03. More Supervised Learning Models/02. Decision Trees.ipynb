{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are regarded as some of the most interpretable models, in that people with little training can make sense of them and put them to use to make decisions. The following decision tree can be used to estimate the annual income of someone.\n",
    "\n",
    "![][1]\n",
    "\n",
    "### Terms\n",
    "\n",
    "A decision tree is composed of **nodes**, **branches**, and **leaves**. The nodes are represented by the rectangles in the diagram. At each node, a decision is made. In scikit-learn, all decision trees are composed of **binary** decisions. The condition evaluated at the node is either true or false producing two branches at each node. In general, decision trees can have any number of branches stemming from each node.\n",
    "\n",
    "A branch is simply the path that one takes down the decision tree to reach the next node. All paths in a decision tree end at a leaf. The leaves contain the predicted value. In this example, the leaves hold an estimated annual salary.\n",
    "\n",
    "### Regression vs Classification Decision trees\n",
    "\n",
    "Decision trees can be used for both regression and classification learning problems. Regression problems have leaves that contain some numeric prediction. Classification problems have leaves with discrete classes. This chapter only focuses on regression decision trees. Together, these are formally referred to as **Classification and Regression Trees** or **CART**.\n",
    "\n",
    "## How a decision tree is created\n",
    "\n",
    "The creation of a decision tree is a computationally expensive process that involves a series of repeated steps. A summary of these steps is listed below.\n",
    "\n",
    "1. At each node, create every single possible binary split of the data\n",
    "1. Calculate the standard deviation of the target variable for each of the two groups\n",
    "1. Calculate the weighted sum of these two standard deviations where the weight is proportional to the size of the group\n",
    "1. Choose the binary split with minimum weighted standard deviation\n",
    "1. Use this split to create two new nodes\n",
    "1. Repeat the above steps for each node until some stopping criterion is met\n",
    "\n",
    "[1]: images/tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually build a short tree\n",
    "\n",
    "We'll now manually build a short decision tree to help better understand the steps it takes to create them. Let's begin by reading in the data selecting just `BedroomAbvGr` and `FullBath` as our features. The reason we use these two features is that they each contain a small number of unique values making the total number of binary splits much smaller than if we used other features like `GrLivArea` which contains hundreds of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv('../data/housing_sample.csv')\n",
    "df = housing[['BedroomAbvGr', 'FullBath', 'SalePrice']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Create every single possible binary split\n",
    "\n",
    "At each node, we need to create every single possible binary split of the data. Let's begin with a concrete example of a single binary split of the data. We can divide the data into houses that have 2 or less bedrooms and those that have 3 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_or_less_br = df.query('BedroomAbvGr <= 2')\n",
    "df_three_or_more_br = df.query('BedroomAbvGr >= 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now split our data into two distinct DataFrames. Let's verify this by outputting the shape of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_or_less_br.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_three_or_more_br.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that the number of bedrooms in each DataFrame corresponds to how we split the data by counting the frequency of each unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_or_less_br['BedroomAbvGr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_three_or_more_br['BedroomAbvGr'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one example of a binary split of the data. We will be attempting every single possible binary split. To do this, we need to know the unique values of each feature. The function below, returns a dictionary mapping the feature name to the unique values. The Series `unique` method returns a numpy array which we sort it in-place with the `sort` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(df):\n",
    "    unique_values = {}\n",
    "    features = df.columns[:-1]\n",
    "    for feature in features:\n",
    "        unique = df[feature].unique()\n",
    "        unique.sort()\n",
    "        unique_values[feature] = unique\n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function assumes that the target variable is the last column in the DataFrame. Let's verify that the function returns the sorted unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all the binary splits\n",
    "\n",
    "Because there are relatively few unique values, we can list them out in their entirety. \n",
    "\n",
    "* 0 or less bedrooms <-> 1 or more\n",
    "* 1 or less bedrooms <-> 2 or more\n",
    "* 2 or less bedrooms <-> 3 or more\n",
    "* 3 or less bedrooms <-> 4 or more\n",
    "* 4 or less bedrooms <-> 5 or more\n",
    "* 5 or less bedrooms <-> 6 or more\n",
    "* 6 or less bedrooms <-> 7 or more\n",
    "* 0 or less bathrooms <-> 1 or more\n",
    "* 1 or less bathrooms <-> 2 or more\n",
    "* 2 or less bathrooms <-> 3 or more\n",
    "\n",
    "For each feature, there will be one less possible split than the number of unique values. For instance, there are eight unique values for bedrooms and seven possible binary splits. A total of 10 possible binary splits exist for the two features. Below, a function is defined to split the data given the feature name and the split value. It returns two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, feature, val):\n",
    "    df_left = df.query(f'{feature} <= {val}')\n",
    "    df_right = df.query(f'{feature} > {val}')\n",
    "    return df_left, df_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to verify that it produces the same DataFrame shapes for a split value of 2 for bedrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left, df_right = split(df, 'BedroomAbvGr', 2)\n",
    "df_left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Calculate the standard deviation of the target variable for each of the two groups\n",
    "\n",
    "After making the split, you'll have two DataFrames. We now calculate the standard deviation of the target variable for each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_groups(df_left, df_right):\n",
    "    return df_left['SalePrice'].std(ddof=0), df_right['SalePrice'].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_groups(df_left, df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Calculate the weighted sum of these two standard deviations\n",
    "\n",
    "You might be wondering why we calculated the standard deviation (square root of the variance). Ideally, we want each group to have as little variance as possible. The standard deviation is our objective measure of performance for rating the quality of the split. The lower the standard deviation, the better the split.\n",
    "\n",
    "We have two standard deviations, but need a single number as our scoring metric. Each standard deviation is weighted in proportion to the number of observations of the DataFrame that it was calculated from. The sum of these weighted standard deviations is reported as the scoring metric for that particular split. The function below returns this metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_std(df_left, df_right):\n",
    "    std_left, std_right = std_groups(df_left, df_right)\n",
    "    num_rows_left = len(df_left)\n",
    "    num_rows_right = len(df_right)\n",
    "    N = num_rows_left + num_rows_right\n",
    "    return std_left * num_rows_left / N + std_right * num_rows_right / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more observations in `df_right` (1,046) than `df_left` (461), so the weighted standard deviation should be close to the standard deviation of `df_right`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_std(df_left, df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Choose the binary split with minimum weighted standard deviation\n",
    "\n",
    "We just calculated the weighted standard deviation for one split. We need to calculate this score for all of the possible splits. The function below returns a DataFrame of the scores of all the possible binary splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_split_scores(df):\n",
    "    unique_values = get_unique_values(df)\n",
    "    scores = {'feature': [], 'split value': [], 'weighted std': []}\n",
    "    for feature, values in unique_values.items():\n",
    "        for val in values[:-1]:\n",
    "            df_left, df_right = split(df, feature, val)\n",
    "            score = weighted_std(df_left, df_right).round(0)\n",
    "            scores['feature'].append(feature)\n",
    "            scores['split value'].append(val)\n",
    "            scores['weighted std'].append(score)\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the function and return the scores for all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = calculate_all_split_scores(df)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to find the split with the minimum weighted standard deviation. The following function returns takes the above DataFrame of scores and returns the feature name and split value for the minimum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_split(scores):\n",
    "    best_split = scores.nsmallest(1, 'weighted std')\n",
    "    best_feature = best_split['feature'].values[0]\n",
    "    best_split_value = best_split['split value'].values[0]\n",
    "    return best_feature, best_split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the function informs us that splitting the houses into those that have one or fewer bathrooms and two or more minimizes the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_best_split(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Use this split to create two new nodes\n",
    "\n",
    "Let's use our previously created `split` function to make our two new nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature, best_split_value = choose_best_split(scores)\n",
    "df_left, df_right = split(df, best_feature, best_split_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the shape to view how many observations are in each new node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Sale Price\n",
    "\n",
    "We now have a very simple decision tree and can actually use it to estimate the sale price. Decision trees use the mean value of the observations in the leaves as the predicted value. Let's calculate the mean sale price of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left['SalePrice'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right['SalePrice'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View current state of tree\n",
    "\n",
    "We can visualize this simple tree with the following diagram:\n",
    "\n",
    "![][1]\n",
    "\n",
    "[1]: images/stump.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Repeat the above steps for each node until some stopping criterion is met\n",
    "\n",
    "The above five steps complete the process of finding the best binary split of the data with two new nodes produced. These same steps are repeated for each new node until a stopping criteria is met. A common stopping criterion is the **depth** of the tree. Currently, our tree has a depth of 1. There are many other stopping criterion, which will be discussed later when we use scikit-learn to build our tree.\n",
    "\n",
    "The function `make_decision` below completes all the above steps, printing out the decision and returning the two new DataFrames. It also prints the mean target value for each new node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision(df):\n",
    "    scores = calculate_all_split_scores(df)\n",
    "    best_feature, best_split_value = choose_best_split(scores)\n",
    "    df_left, df_right = split(df, best_feature, best_split_value)\n",
    "    print(f'Decision: {best_feature} <= {best_split_value}')\n",
    "    mean_left = df_left.iloc[:, -1].mean()\n",
    "    print(f'mean left = {mean_left:,.0f}')\n",
    "    mean_right = df_right.iloc[:, -1].mean()\n",
    "    print(f'mean right = {mean_right:,.0f}')\n",
    "    return df_left, df_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that this function produces the same result for our first decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left, df_right = make_decision(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function to continue splitting our nodes. Let's find out what the decision is for our left node and again for our right node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left_left, df_left_right = make_decision(df_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right_left, df_right_right = make_decision(df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this function can be repeatedly called manually like this, we can improve upon it. The function `make_tree` below recursively builds the decision tree, storing the result as a dictionary. The tree stops growing whenever it reaches its maximum depth, which is set at two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(df, tree=None, max_depth=2):\n",
    "    if tree is None:\n",
    "        tree = {'depth': 0}\n",
    "    tree['mean'] = df.iloc[:, -1].mean()\n",
    "    tree['N'] = len(df)\n",
    "    if tree['depth'] < max_depth:\n",
    "        scores = calculate_all_split_scores(df)\n",
    "        best_feature, best_split_value = choose_best_split(scores)\n",
    "        df_left, df_right = split(df, best_feature, best_split_value)\n",
    "        tree['decision'] = f'{best_feature} <= {best_split_value}'\n",
    "        tree['left'] = make_tree(df_left, {'depth': tree['depth'] + 1}, max_depth)\n",
    "        tree['right'] = make_tree(df_right, {'depth': tree['depth'] + 1}, max_depth)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the entire tree with one command and output the dictionary containing all of the tree info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = make_tree(df)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision at each node is stored in the 'decision' key. Let's verify that the decision for the first split is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree['decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left and right nodes extending from each node are stored in the 'left' and 'right' keys. Let's select the left node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree['left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth, mean, N (number of observations), decision, and new nodes are output. Let's display just the decision for this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree['left']['decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the left node extending from this current node. This is actually a leaf as it there are no other 'left' or 'right' keys in this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree['left']['left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `print_tree` below prints out the decision, number of observations, and mean at each node. It iterates through the tree using a breadth-first (horizontal) search as opposed to depth-first (vertical) which was how the tree was built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(queue, tree_string=None, count=0):\n",
    "    cur_tree = queue[0]\n",
    "    if 'decision' in cur_tree:\n",
    "        info = cur_tree['decision']\n",
    "    else:\n",
    "        info = f\"leaf {cur_tree['mean']:,.0f}\"\n",
    "    depth = cur_tree['depth']\n",
    "    decision = cur_tree.get('decision', 'leaf')\n",
    "    N = cur_tree['N']\n",
    "    mean = cur_tree['mean']\n",
    "    space = {0: 0, 1: 20, 2:2}[depth] * ' '\n",
    "    if count == 0:\n",
    "        tree_string = ['', '', '']\n",
    "        space = ''\n",
    "    tree_string[0] += space + f'{decision:^15}'\n",
    "    tree_string[1] += space + f'{f\"N = {N}\":^15}'\n",
    "    tree_string[2] += space + f'{f\"mean = {mean:,.0f}\":^15}'\n",
    "    count += 1\n",
    "    if count == 2 ** depth:\n",
    "        count = 0\n",
    "        tree_string = [f'{string:^100}' for string in tree_string]\n",
    "        print('\\n'.join(tree_string))\n",
    "        if decision != 'leaf':\n",
    "            for i in range(10, 16, 2):\n",
    "                s = f'/ {\" \":{i}}  \\\\'\n",
    "                s = [s] * (2 ** depth)\n",
    "                s = space[:30 - i].join(s)\n",
    "                print(f'{s:^100}')\n",
    "    if 'left' in cur_tree:\n",
    "        queue.append(cur_tree['left'])\n",
    "        queue.append(cur_tree['right'])\n",
    "    if len(queue) > 1:\n",
    "        print_tree(queue[1:], tree_string, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the function a one-item list of our tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree([tree])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clearer visual of the just the decisions and predictions is shown in the following diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![][1]\n",
    "\n",
    "[1]: images/full_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use scikit-learn to create a decision tree\n",
    "\n",
    "scikit-learn has its decision tree models available in the `tree` module. Let's first get our data assigned to the variables `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['BedroomAbvGr', 'FullBath']\n",
    "X = housing[cols]\n",
    "y = housing['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the `DecisionTreeRegressor` (and NOT the `DecisionTreeClassifier`). When we instantiate it, we are given several choices to control how the tree is built. Once of these choices is `max_depth` which controls the maximum depth of tree. We set this to two upon instantiation. Finally, we build the decision tree with a call to the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(max_depth=2)\n",
    "dtr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize this tree, scikit-learn provides the `plot_tree` helper function. Below, we plot the tree and verify that our decision tree created above matches the one found with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plot_tree(dtr, fontsize=15, feature_names=cols, rounded=True, precision=1, filled=True, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "Our decision tree above has four possible outcomes. If we have a one bedroom, one bathroom house, our model predicts 168,000. Let's see if the scikit-learn model agrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Like all regression models, $R^2$ is returned from the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the tree\n",
    "\n",
    "A tree object is available after training and is accessible as the `tree_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = dtr.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of nodes is found in the `node_count` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.node_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of leaves is found in the `n_leaves` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.n_leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate visualization with graphviz\n",
    "\n",
    "The `plot_tree` function from above renders the image using matplotlib. You can produce a sharper image with the help of the graphviz third-party library. You will need to install it before proceeding by executing the following command in your Anaconda Prompt/Terminal program. \n",
    "\n",
    " `conda install python-graphviz`\n",
    " \n",
    "You will probably need to install the [software GraphViz][1] as well and then run the following commands.\n",
    "\n",
    "[1]: https://www.graphviz.org/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "dot_data = export_graphviz(dtr, feature_names=cols, precision=1, filled=True, rounded=True)  \n",
    "graphviz.Source(dot_data, format='png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<span  style=\"color:green; font-size:16px\">Make more decision trees using a different combination of features and depths. Be careful when visualizing very deep trees as there can be many thousands of nodes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
